{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq --no-deps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6YJ5jLEDmek",
        "outputId": "ee4a3a23-31af-45b4-87cb-70c4821f8e0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11288625 sha256=dc867a198293c442afc2fde1d5315fe26c862ea5cea1270643589a2b4a9f143e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "Successfully built fairseq\n",
            "Installing collected packages: fairseq\n",
            "Successfully installed fairseq-0.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip==23.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JDq2QqcDuQl",
        "outputId": "dc2e4482-ebd8-4aa4-ca8e-9c3af6f0ac46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip==23.1.2 in /usr/local/lib/python3.10/dist-packages (23.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FjUJgL7EwPf",
        "outputId": "7ada24ca-a4f6-4443-fe9d-c9393f989f02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.11)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.9.11)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.5)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.26.4)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.12.2)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=65b569ac7d483b0b002e3c4abf38aa8ba34c728cc27d39efd963033846b430c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-3.0.0 colorama-0.4.6 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.10.1 sacrebleu-2.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BazY5-zQ1hB1",
        "outputId": "7168fcd0-9eea-4250-c0c0-87d4e416118d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch==1.12.1+cu113 in /usr/local/lib/python3.10/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision==0.13.1+cu113 in /usr/local/lib/python3.10/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torchaudio==0.12.1 in /usr/local/lib/python3.10/dist-packages (0.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (10.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b96B5zZYYIt8",
        "outputId": "d18ad19e-d4d7-4da9-c4ce-b08abc08fabe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQEagZKOYLqa",
        "outputId": "26f79b31-85a5-47f1-a0d5-431ea4e2acdf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_src = '/content/drive/MyDrive/Code_transfer/train.src'\n",
        "train_tgt = '/content/drive/MyDrive/Code_transfer/train.tgt'\n",
        "valid_src = '/content/drive/MyDrive/Code_transfer/valid.src'\n",
        "valid_tgt = '/content/drive/MyDrive/Code_transfer/valid.tgt'\n",
        "test_src = '/content/drive/MyDrive/Code_transfer/test.src'\n",
        "test_tgt = '/content/drive/MyDrive/Code_transfer/test.tgt'"
      ],
      "metadata": {
        "id": "WPXAqNFnf8Bs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and process the dataset\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    # Split lines into tokens\n",
        "    tokenized_lines = [line.split() for line in lines]\n",
        "    return tokenized_lines\n",
        "\n",
        "# Function to calculate vocabulary size and max length\n",
        "def calculate_vocab_size_and_max_length(tokenized_lines):\n",
        "    vocab = set()  # To store unique words\n",
        "    max_length = 0  # To track maximum sequence length\n",
        "\n",
        "    for tokens in tokenized_lines:\n",
        "        vocab.update(tokens)  # Add tokens to vocab set\n",
        "        max_length = max(max_length, len(tokens))  # Update max length\n",
        "\n",
        "    return len(vocab), max_length\n",
        "\n",
        "# Process each dataset and calculate vocabulary size and max sequence length\n",
        "datasets = {\n",
        "    'train_src': train_src,\n",
        "    'train_tgt': train_tgt,\n",
        "    'valid_src': valid_src,\n",
        "    'valid_tgt': valid_tgt,\n",
        "    'test_src': test_src,\n",
        "    'test_tgt': test_tgt\n",
        "}\n",
        "\n",
        "for dataset_name, dataset_path in datasets.items():\n",
        "    tokenized_lines = read_file(dataset_path)\n",
        "    vocab_size, max_length = calculate_vocab_size_and_max_length(tokenized_lines)\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"  Vocabulary Size: {vocab_size}\")\n",
        "    print(f\"  Max Sequence Length: {max_length}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6x1X-iKhPgI",
        "outputId": "955de02f-2ed8-4d44-9e16-22c6681213fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: train_src\n",
            "  Vocabulary Size: 63251\n",
            "  Max Sequence Length: 42105\n",
            "\n",
            "Dataset: train_tgt\n",
            "  Vocabulary Size: 1533\n",
            "  Max Sequence Length: 573\n",
            "\n",
            "Dataset: valid_src\n",
            "  Vocabulary Size: 39242\n",
            "  Max Sequence Length: 42105\n",
            "\n",
            "Dataset: valid_tgt\n",
            "  Vocabulary Size: 744\n",
            "  Max Sequence Length: 564\n",
            "\n",
            "Dataset: test_src\n",
            "  Vocabulary Size: 497\n",
            "  Max Sequence Length: 1432\n",
            "\n",
            "Dataset: test_tgt\n",
            "  Vocabulary Size: 62\n",
            "  Max Sequence Length: 138\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the Dataset"
      ],
      "metadata": {
        "id": "TSpVXgGQVWun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Function to count the number of unique tokens in a dataset\n",
        "def get_vocab_size(input_file):\n",
        "    vocab = set()\n",
        "    with open(input_file, 'r') as f:\n",
        "        for line in f:\n",
        "            tokens = line.split()\n",
        "            vocab.update(tokens)\n",
        "    return len(vocab)\n",
        "\n",
        "# Get vocab size for train_src and train_tgt\n",
        "train_src_vocab_size = get_vocab_size(train_src)\n",
        "train_tgt_vocab_size = get_vocab_size(train_tgt)\n",
        "\n",
        "# Train BPE models for source and target datasets\n",
        "spm.SentencePieceTrainer.train(input=train_src, model_prefix='bpe_src', vocab_size=470)\n",
        "spm.SentencePieceTrainer.train(input=train_tgt, model_prefix='bpe_tgt', vocab_size=470)\n",
        "\n",
        "# Apply BPE encoding to the datasets\n",
        "def apply_bpe(input_file, output_file, model_file):\n",
        "    sp = spm.SentencePieceProcessor(model_file=model_file)\n",
        "    with open(output_file, 'w') as out_file:\n",
        "        with open(input_file) as in_file:\n",
        "            for line in in_file:\n",
        "                out_file.write(' '.join(sp.encode(line, out_type=str)) + '\\n')\n",
        "\n",
        "# Apply BPE to source and target files\n",
        "apply_bpe(train_src, 'train_bpe.src', 'bpe_src.model')\n",
        "apply_bpe(train_tgt, 'train_bpe.tgt', 'bpe_tgt.model')\n",
        "apply_bpe(valid_src, 'valid_bpe.src', 'bpe_src.model')\n",
        "apply_bpe(valid_tgt, 'valid_bpe.tgt', 'bpe_tgt.model')\n",
        "apply_bpe(test_src, 'test_bpe.src', 'bpe_src.model')\n",
        "apply_bpe(test_tgt, 'test_bpe.tgt', 'bpe_tgt.model')\n"
      ],
      "metadata": {
        "id": "OSUGsVAEgRwq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordwise_encoding(input_file, output_file):\n",
        "    with open(input_file, 'r') as in_file, open(output_file, 'w') as out_file:\n",
        "        for line in in_file:\n",
        "            out_file.write(' '.join(line.strip().split()) + '\\n')\n",
        "\n",
        "# Apply word-wise encoding to source and target files\n",
        "wordwise_encoding(train_src, 'train_wordwise.src')\n",
        "wordwise_encoding(train_tgt, 'train_wordwise.tgt')\n",
        "wordwise_encoding(valid_src, 'valid_wordwise.src')\n",
        "wordwise_encoding(valid_tgt, 'valid_wordwise.tgt')\n",
        "wordwise_encoding(test_src, 'test_wordwise.src')\n",
        "wordwise_encoding(test_tgt, 'test_wordwise.tgt')\n"
      ],
      "metadata": {
        "id": "i9z_3mPmlD4n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data with Fairseq"
      ],
      "metadata": {
        "id": "57u-ds32VatD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BPE data preprocessing\n",
        "!fairseq-preprocess --source-lang src --target-lang tgt \\\n",
        "    --trainpref train_bpe --validpref valid_bpe --testpref test_bpe \\\n",
        "    --destdir data-bin/bpe --workers 20\n",
        "\n",
        "# Word-wise data preprocessing\n",
        "!fairseq-preprocess --source-lang src --target-lang tgt \\\n",
        "    --trainpref train_wordwise --validpref valid_wordwise --testpref test_wordwise \\\n",
        "    --destdir data-bin/wordwise --workers 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxL6pJmVldDr",
        "outputId": "367a2fd1-d975-441f-e59b-8ab9c9f9baa9",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-23 15:54:22 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-10-23 15:54:23 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='src', target_lang='tgt', trainpref='train_bpe', validpref='valid_bpe', testpref='test_bpe', align_suffix=None, destdir='data-bin/bpe', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
            "2024-10-23 15:54:38 | INFO | fairseq_cli.preprocess | [src] Dictionary: 520 types\n",
            "2024-10-23 15:55:22 | INFO | fairseq_cli.preprocess | [src] train_bpe.src: 1100 sents, 10336364 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:55:22 | INFO | fairseq_cli.preprocess | [src] Dictionary: 520 types\n",
            "2024-10-23 15:55:37 | INFO | fairseq_cli.preprocess | [src] valid_bpe.src: 275 sents, 3591261 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:55:37 | INFO | fairseq_cli.preprocess | [src] Dictionary: 520 types\n",
            "2024-10-23 15:55:37 | INFO | fairseq_cli.preprocess | [src] test_bpe.src: 5 sents, 12940 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:55:37 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 488 types\n",
            "2024-10-23 15:55:38 | INFO | fairseq_cli.preprocess | [tgt] train_bpe.tgt: 1100 sents, 261498 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:55:38 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 488 types\n",
            "2024-10-23 15:55:39 | INFO | fairseq_cli.preprocess | [tgt] valid_bpe.tgt: 275 sents, 69143 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:55:39 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 488 types\n",
            "2024-10-23 15:55:39 | INFO | fairseq_cli.preprocess | [tgt] test_bpe.tgt: 5 sents, 1006 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:55:39 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/bpe\n",
            "2024-10-23 15:55:41 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-10-23 15:55:41 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='src', target_lang='tgt', trainpref='train_wordwise', validpref='valid_wordwise', testpref='test_wordwise', align_suffix=None, destdir='data-bin/wordwise', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
            "2024-10-23 15:55:45 | INFO | fairseq_cli.preprocess | [src] Dictionary: 63256 types\n",
            "2024-10-23 15:55:56 | INFO | fairseq_cli.preprocess | [src] train_wordwise.src: 1100 sents, 2092651 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:55:56 | INFO | fairseq_cli.preprocess | [src] Dictionary: 63256 types\n",
            "2024-10-23 15:56:00 | INFO | fairseq_cli.preprocess | [src] valid_wordwise.src: 275 sents, 697514 tokens, 2.05% replaced (by <unk>)\n",
            "2024-10-23 15:56:00 | INFO | fairseq_cli.preprocess | [src] Dictionary: 63256 types\n",
            "2024-10-23 15:56:01 | INFO | fairseq_cli.preprocess | [src] test_wordwise.src: 5 sents, 3275 tokens, 2.08% replaced (by <unk>)\n",
            "2024-10-23 15:56:01 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 1544 types\n",
            "2024-10-23 15:56:01 | INFO | fairseq_cli.preprocess | [tgt] train_wordwise.tgt: 1100 sents, 127900 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:56:01 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 1544 types\n",
            "2024-10-23 15:56:02 | INFO | fairseq_cli.preprocess | [tgt] valid_wordwise.tgt: 275 sents, 33485 tokens, 1.42% replaced (by <unk>)\n",
            "2024-10-23 15:56:02 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 1544 types\n",
            "2024-10-23 15:56:02 | INFO | fairseq_cli.preprocess | [tgt] test_wordwise.tgt: 5 sents, 488 tokens, 2.46% replaced (by <unk>)\n",
            "2024-10-23 15:56:02 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wordwise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# positional data preprocessing\n",
        "\n",
        "!fairseq-preprocess --source-lang src --target-lang tgt \\\n",
        "    --trainpref train_wordwise --validpref valid_wordwise --testpref test_wordwise \\\n",
        "    --destdir data-bin/positional --workers 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TB7_yr1F9HQ6",
        "outputId": "b6de92d8-e97c-463b-e731-732ab64e9ee1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-23 15:56:14 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-10-23 15:56:14 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='src', target_lang='tgt', trainpref='train_wordwise', validpref='valid_wordwise', testpref='test_wordwise', align_suffix=None, destdir='data-bin/positional', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
            "2024-10-23 15:56:18 | INFO | fairseq_cli.preprocess | [src] Dictionary: 63256 types\n",
            "2024-10-23 15:56:27 | INFO | fairseq_cli.preprocess | [src] train_wordwise.src: 1100 sents, 2092651 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:56:27 | INFO | fairseq_cli.preprocess | [src] Dictionary: 63256 types\n",
            "2024-10-23 15:56:32 | INFO | fairseq_cli.preprocess | [src] valid_wordwise.src: 275 sents, 697514 tokens, 2.05% replaced (by <unk>)\n",
            "2024-10-23 15:56:32 | INFO | fairseq_cli.preprocess | [src] Dictionary: 63256 types\n",
            "2024-10-23 15:56:34 | INFO | fairseq_cli.preprocess | [src] test_wordwise.src: 5 sents, 3275 tokens, 2.08% replaced (by <unk>)\n",
            "2024-10-23 15:56:34 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 1544 types\n",
            "2024-10-23 15:56:35 | INFO | fairseq_cli.preprocess | [tgt] train_wordwise.tgt: 1100 sents, 127900 tokens, 0.0% replaced (by <unk>)\n",
            "2024-10-23 15:56:35 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 1544 types\n",
            "2024-10-23 15:56:35 | INFO | fairseq_cli.preprocess | [tgt] valid_wordwise.tgt: 275 sents, 33485 tokens, 1.42% replaced (by <unk>)\n",
            "2024-10-23 15:56:35 | INFO | fairseq_cli.preprocess | [tgt] Dictionary: 1544 types\n",
            "2024-10-23 15:56:35 | INFO | fairseq_cli.preprocess | [tgt] test_wordwise.tgt: 5 sents, 488 tokens, 2.46% replaced (by <unk>)\n",
            "2024-10-23 15:56:35 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/positional\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Models\n",
        " BPE model training"
      ],
      "metadata": {
        "id": "xBHcK7w5U-c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train data-bin/bpe \\\n",
        "    --arch transformer --share-decoder-input-output-embed \\\n",
        "    --encoder-layers 6 --decoder-layers 6 \\\n",
        "    --max-tokens 50000 --lr 0.01 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.1 --dropout 0.1 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 50 --save-dir checkpoints/bpe_transformer \\\n",
        "    --max-source-positions 42105 --max-target-positions 573 \\\n",
        "    --skip-invalid-size-inputs-valid-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMHi-DdCHKt0",
        "outputId": "f68fa559-10ac-4cf8-9188-64196a823156",
        "collapsed": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      56    |      75    |     139    |      83    |\n",
            "|       from large pool |      15    |      33    |      84    |      69    |\n",
            "|       from small pool |      41    |      43    |      55    |      14    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      22    |     998    |     988    |\n",
            "|       from large pool |       8    |      19    |     604    |     596    |\n",
            "|       from small pool |       2    |       5    |     394    |     392    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 17.48 GiB (GPU 0; 14.75 GiB total capacity; 1.06 GiB already allocated; 12.53 GiB free; 1.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:10 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 44           |        cudaMalloc retries: 49        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1090 MB |   13575 MB |  280971 MB |  279881 MB |\n",
            "|       from large pool |    1009 MB |   13490 MB |  280749 MB |  279739 MB |\n",
            "|       from small pool |      80 MB |      84 MB |     222 MB |     141 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1090 MB |   13575 MB |  280971 MB |  279881 MB |\n",
            "|       from large pool |    1009 MB |   13490 MB |  280749 MB |  279739 MB |\n",
            "|       from small pool |      80 MB |      84 MB |     222 MB |     141 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1478 MB |   14286 MB |  160504 MB |  159026 MB |\n",
            "|       from large pool |    1396 MB |   14200 MB |  160394 MB |  158998 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     110 MB |      28 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  397078 KB |   11315 MB |  234406 MB |  234018 MB |\n",
            "|       from large pool |  395344 KB |   11315 MB |  234058 MB |  233672 MB |\n",
            "|       from small pool |    1734 KB |       2 MB |     347 MB |     345 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     328    |     409    |    2347    |    2019    |\n",
            "|       from large pool |      55    |     116    |    1125    |    1070    |\n",
            "|       from small pool |     273    |     293    |    1222    |     949    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     328    |     409    |    2347    |    2019    |\n",
            "|       from large pool |      55    |     116    |    1125    |    1070    |\n",
            "|       from small pool |     273    |     293    |    1222    |     949    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |      75    |     139    |      86    |\n",
            "|       from large pool |      12    |      33    |      84    |      72    |\n",
            "|       from small pool |      41    |      43    |      55    |      14    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       5    |      22    |    1016    |    1011    |\n",
            "|       from large pool |       3    |      19    |     614    |     611    |\n",
            "|       from small pool |       2    |       5    |     402    |     400    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  30% 44/147 [00:11<00:23,  4.42it/s]2024-10-23 16:02:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 370.00 MiB (GPU 0; 14.75 GiB total capacity; 13.62 GiB already allocated; 13.06 MiB free; 13.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:10 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 45           |        cudaMalloc retries: 50        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   13950 MB |   14052 MB |  309133 MB |  295182 MB |\n",
            "|       from large pool |   13864 MB |   13966 MB |  308894 MB |  295030 MB |\n",
            "|       from small pool |      85 MB |      85 MB |     238 MB |     152 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   13950 MB |   14052 MB |  309133 MB |  295182 MB |\n",
            "|       from large pool |   13864 MB |   13966 MB |  308894 MB |  295030 MB |\n",
            "|       from small pool |      85 MB |      85 MB |     238 MB |     152 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14292 MB |   14292 MB |  173318 MB |  159026 MB |\n",
            "|       from large pool |   14206 MB |   14206 MB |  173204 MB |  158998 MB |\n",
            "|       from small pool |      86 MB |      86 MB |     114 MB |      28 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  349890 KB |   11315 MB |  248395 MB |  248053 MB |\n",
            "|       from large pool |  349452 KB |   11315 MB |  248027 MB |  247686 MB |\n",
            "|       from small pool |     438 KB |       2 MB |     367 MB |     366 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     499    |     499    |    2680    |    2181    |\n",
            "|       from large pool |     187    |     187    |    1363    |    1176    |\n",
            "|       from small pool |     312    |     312    |    1317    |    1005    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     499    |     499    |    2680    |    2181    |\n",
            "|       from large pool |     187    |     187    |    1363    |    1176    |\n",
            "|       from small pool |     312    |     312    |    1317    |    1005    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     101    |     101    |     187    |      86    |\n",
            "|       from large pool |      58    |      58    |     130    |      72    |\n",
            "|       from small pool |      43    |      43    |      57    |      14    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      35    |      36    |    1137    |    1102    |\n",
            "|       from large pool |      30    |      31    |     714    |     684    |\n",
            "|       from small pool |       5    |       5    |     423    |     418    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  31% 45/147 [00:12<00:25,  4.08it/s]2024-10-23 16:02:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.61 GiB (GPU 0; 14.75 GiB total capacity; 8.83 GiB already allocated; 1.94 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:11 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 46           |        cudaMalloc retries: 52        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9045 MB |   14052 MB |  322454 MB |  313408 MB |\n",
            "|       from large pool |    8964 MB |   13966 MB |  322211 MB |  313247 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     242 MB |     161 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9045 MB |   14052 MB |  322454 MB |  313408 MB |\n",
            "|       from large pool |    8964 MB |   13966 MB |  322211 MB |  313247 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     242 MB |     161 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12318 MB |   14292 MB |  184406 MB |  172088 MB |\n",
            "|       from large pool |   12236 MB |   14206 MB |  184292 MB |  172056 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     114 MB |      32 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3272 MB |   11315 MB |  259336 MB |  256063 MB |\n",
            "|       from large pool |    3271 MB |   11315 MB |  258954 MB |  255683 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     381 MB |     380 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    2743    |    2396    |\n",
            "|       from large pool |      69    |     187    |    1399    |    1330    |\n",
            "|       from small pool |     278    |     312    |    1344    |    1066    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    2743    |    2396    |\n",
            "|       from large pool |      69    |     187    |    1399    |    1330    |\n",
            "|       from small pool |     278    |     312    |    1344    |    1066    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      64    |     101    |     190    |     126    |\n",
            "|       from large pool |      23    |      58    |     133    |     110    |\n",
            "|       from small pool |      41    |      43    |      57    |      16    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      17    |      39    |    1226    |    1209    |\n",
            "|       from large pool |      15    |      33    |     774    |     759    |\n",
            "|       from small pool |       2    |       6    |     452    |     450    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  31% 46/147 [00:13<00:35,  2.85it/s]2024-10-23 16:02:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.96 GiB (GPU 0; 14.75 GiB total capacity; 6.80 GiB already allocated; 2.85 GiB free; 11.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 47           |        cudaMalloc retries: 54        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6964 MB |   14052 MB |  335237 MB |  328273 MB |\n",
            "|       from large pool |    6883 MB |   13966 MB |  334991 MB |  328107 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     246 MB |     165 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6964 MB |   14052 MB |  335237 MB |  328273 MB |\n",
            "|       from large pool |    6883 MB |   13966 MB |  334991 MB |  328107 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     246 MB |     165 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11390 MB |   14292 MB |  194566 MB |  183176 MB |\n",
            "|       from large pool |   11308 MB |   14206 MB |  194452 MB |  183144 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     114 MB |      32 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4425 MB |   11315 MB |  266067 MB |  261642 MB |\n",
            "|       from large pool |    4424 MB |   11315 MB |  265679 MB |  261254 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     388 MB |     387 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     346    |     499    |    2805    |    2459    |\n",
            "|       from large pool |      69    |     187    |    1436    |    1367    |\n",
            "|       from small pool |     277    |     312    |    1369    |    1092    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     346    |     499    |    2805    |    2459    |\n",
            "|       from large pool |      69    |     187    |    1436    |    1367    |\n",
            "|       from small pool |     277    |     312    |    1369    |    1092    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     101    |     192    |     129    |\n",
            "|       from large pool |      22    |      58    |     135    |     113    |\n",
            "|       from small pool |      41    |      43    |      57    |      16    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      39    |    1265    |    1250    |\n",
            "|       from large pool |      13    |      33    |     803    |     790    |\n",
            "|       from small pool |       2    |       6    |     462    |     460    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  32% 47/147 [00:13<00:38,  2.58it/s]2024-10-23 16:02:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 18.71 GiB (GPU 0; 14.75 GiB total capacity; 638.91 MiB already allocated; 13.05 GiB free; 946.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 48           |        cudaMalloc retries: 55        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  654245 KB |   14052 MB |  335691 MB |  335052 MB |\n",
            "|       from large pool |  572671 KB |   13966 MB |  335443 MB |  334884 MB |\n",
            "|       from small pool |   81574 KB |      85 MB |     247 MB |     167 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  654245 KB |   14052 MB |  335691 MB |  335052 MB |\n",
            "|       from large pool |  572671 KB |   13966 MB |  335443 MB |  334884 MB |\n",
            "|       from small pool |   81574 KB |      85 MB |     247 MB |     167 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     946 MB |   14292 MB |  194566 MB |  193620 MB |\n",
            "|       from large pool |     866 MB |   14206 MB |  194452 MB |  193586 MB |\n",
            "|       from small pool |      80 MB |      86 MB |     114 MB |      34 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  314459 KB |   11315 MB |  267392 MB |  267085 MB |\n",
            "|       from large pool |  314113 KB |   11315 MB |  266997 MB |  266690 MB |\n",
            "|       from small pool |     346 KB |       2 MB |     395 MB |     395 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    2832    |    2508    |\n",
            "|       from large pool |      53    |     187    |    1446    |    1393    |\n",
            "|       from small pool |     271    |     312    |    1386    |    1115    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    2832    |    2508    |\n",
            "|       from large pool |      53    |     187    |    1446    |    1393    |\n",
            "|       from small pool |     271    |     312    |    1386    |    1115    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     192    |     135    |\n",
            "|       from large pool |      17    |      58    |     135    |     118    |\n",
            "|       from small pool |      40    |      43    |      57    |      17    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    1287    |    1277    |\n",
            "|       from large pool |       9    |      33    |     817    |     808    |\n",
            "|       from small pool |       1    |       6    |     470    |     469    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  33% 48/147 [00:14<00:42,  2.33it/s]2024-10-23 16:02:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.75 GiB (GPU 0; 14.75 GiB total capacity; 10.35 GiB already allocated; 1.89 GiB free; 12.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 49           |        cudaMalloc retries: 56        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10603 MB |   14052 MB |  352923 MB |  342320 MB |\n",
            "|       from large pool |   10521 MB |   13966 MB |  352670 MB |  342149 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     253 MB |     171 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10603 MB |   14052 MB |  352923 MB |  342320 MB |\n",
            "|       from large pool |   10521 MB |   13966 MB |  352670 MB |  342149 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     253 MB |     171 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12374 MB |   14292 MB |  205994 MB |  193620 MB |\n",
            "|       from large pool |   12292 MB |   14206 MB |  205878 MB |  193586 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     116 MB |      34 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1770 MB |   11315 MB |  271675 MB |  269904 MB |\n",
            "|       from large pool |    1770 MB |   11315 MB |  271270 MB |  269499 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     405 MB |     405 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     363    |     499    |    2922    |    2559    |\n",
            "|       from large pool |      81    |     187    |    1502    |    1421    |\n",
            "|       from small pool |     282    |     312    |    1420    |    1138    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     363    |     499    |    2922    |    2559    |\n",
            "|       from large pool |      81    |     187    |    1502    |    1421    |\n",
            "|       from small pool |     282    |     312    |    1420    |    1138    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     101    |     200    |     135    |\n",
            "|       from large pool |      24    |      58    |     142    |     118    |\n",
            "|       from small pool |      41    |      43    |      58    |      17    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1324    |    1312    |\n",
            "|       from large pool |      10    |      33    |     843    |     833    |\n",
            "|       from small pool |       2    |       6    |     481    |     479    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 21.38 GiB (GPU 0; 14.75 GiB total capacity; 660.03 MiB already allocated; 13.16 GiB free; 826.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 50           |        cudaMalloc retries: 57        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  675873 KB |   14052 MB |  353409 MB |  352749 MB |\n",
            "|       from large pool |  594283 KB |   13966 MB |  353154 MB |  352574 MB |\n",
            "|       from small pool |   81589 KB |      85 MB |     254 MB |     174 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  675873 KB |   14052 MB |  353409 MB |  352749 MB |\n",
            "|       from large pool |  594283 KB |   13966 MB |  353154 MB |  352574 MB |\n",
            "|       from small pool |   81589 KB |      85 MB |     254 MB |     174 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     826 MB |   14292 MB |  205994 MB |  205168 MB |\n",
            "|       from large pool |     746 MB |   14206 MB |  205878 MB |  205132 MB |\n",
            "|       from small pool |      80 MB |      86 MB |     116 MB |      36 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  169951 KB |   11315 MB |  273195 MB |  273029 MB |\n",
            "|       from large pool |  169620 KB |   11315 MB |  272783 MB |  272618 MB |\n",
            "|       from small pool |     330 KB |       2 MB |     411 MB |     411 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    2949    |    2625    |\n",
            "|       from large pool |      53    |     187    |    1512    |    1459    |\n",
            "|       from small pool |     271    |     312    |    1437    |    1166    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    2949    |    2625    |\n",
            "|       from large pool |      53    |     187    |    1512    |    1459    |\n",
            "|       from small pool |     271    |     312    |    1437    |    1166    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     200    |     143    |\n",
            "|       from large pool |      17    |      58    |     142    |     125    |\n",
            "|       from small pool |      40    |      43    |      58    |      18    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    1351    |    1341    |\n",
            "|       from large pool |       9    |      33    |     859    |     850    |\n",
            "|       from small pool |       1    |       6    |     492    |     491    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  34% 50/147 [00:14<00:39,  2.45it/s]2024-10-23 16:02:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 22.78 GiB (GPU 0; 14.75 GiB total capacity; 1.02 GiB already allocated; 12.86 GiB free; 1.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 51           |        cudaMalloc retries: 58        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1039 MB |   14052 MB |  354346 MB |  353307 MB |\n",
            "|       from large pool |     959 MB |   13966 MB |  354090 MB |  353130 MB |\n",
            "|       from small pool |      79 MB |      85 MB |     256 MB |     176 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1039 MB |   14052 MB |  354346 MB |  353307 MB |\n",
            "|       from large pool |     959 MB |   13966 MB |  354090 MB |  353130 MB |\n",
            "|       from small pool |      79 MB |      85 MB |     256 MB |     176 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1132 MB |   14292 MB |  206464 MB |  205332 MB |\n",
            "|       from large pool |    1052 MB |   14206 MB |  206346 MB |  205294 MB |\n",
            "|       from small pool |      80 MB |      86 MB |     118 MB |      38 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   94668 KB |   11315 MB |  273365 MB |  273273 MB |\n",
            "|       from large pool |   94452 KB |   11315 MB |  272950 MB |  272858 MB |\n",
            "|       from small pool |     216 KB |       2 MB |     415 MB |     415 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     327    |     499    |    2978    |    2651    |\n",
            "|       from large pool |      56    |     187    |    1525    |    1469    |\n",
            "|       from small pool |     271    |     312    |    1453    |    1182    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     327    |     499    |    2978    |    2651    |\n",
            "|       from large pool |      56    |     187    |    1525    |    1469    |\n",
            "|       from small pool |     271    |     312    |    1453    |    1182    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      60    |     101    |     207    |     147    |\n",
            "|       from large pool |      20    |      58    |     148    |     128    |\n",
            "|       from small pool |      40    |      43    |      59    |      19    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      13    |      39    |    1369    |    1356    |\n",
            "|       from large pool |      12    |      33    |     871    |     859    |\n",
            "|       from small pool |       1    |       6    |     498    |     497    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.75 GiB (GPU 0; 14.75 GiB total capacity; 655.17 MiB already allocated; 13.04 GiB free; 954.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 52           |        cudaMalloc retries: 59        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  670894 KB |   14052 MB |  354824 MB |  354169 MB |\n",
            "|       from large pool |  589308 KB |   13966 MB |  354566 MB |  353991 MB |\n",
            "|       from small pool |   81586 KB |      85 MB |     257 MB |     178 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  670894 KB |   14052 MB |  354824 MB |  354169 MB |\n",
            "|       from large pool |  589308 KB |   13966 MB |  354566 MB |  353991 MB |\n",
            "|       from small pool |   81586 KB |      85 MB |     257 MB |     178 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     954 MB |   14292 MB |  206466 MB |  205512 MB |\n",
            "|       from large pool |     874 MB |   14206 MB |  206346 MB |  205472 MB |\n",
            "|       from small pool |      80 MB |      86 MB |     120 MB |      40 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  306001 KB |   11315 MB |  273730 MB |  273432 MB |\n",
            "|       from large pool |  305667 KB |   11315 MB |  273309 MB |  273010 MB |\n",
            "|       from small pool |     334 KB |       2 MB |     421 MB |     421 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    3005    |    2681    |\n",
            "|       from large pool |      53    |     187    |    1535    |    1482    |\n",
            "|       from small pool |     271    |     312    |    1470    |    1199    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    3005    |    2681    |\n",
            "|       from large pool |      53    |     187    |    1535    |    1482    |\n",
            "|       from small pool |     271    |     312    |    1470    |    1199    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |     101    |     208    |     150    |\n",
            "|       from large pool |      18    |      58    |     148    |     130    |\n",
            "|       from small pool |      40    |      43    |      60    |      20    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      11    |      39    |    1384    |    1373    |\n",
            "|       from large pool |      10    |      33    |     880    |     870    |\n",
            "|       from small pool |       1    |       6    |     504    |     503    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  35% 52/147 [00:15<00:26,  3.57it/s]2024-10-23 16:02:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.16 GiB (GPU 0; 14.75 GiB total capacity; 1.12 GiB already allocated; 12.81 GiB free; 1.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 53           |        cudaMalloc retries: 60        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1149 MB |   14052 MB |  355995 MB |  354846 MB |\n",
            "|       from large pool |    1069 MB |   13966 MB |  355734 MB |  354665 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     260 MB |     180 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1149 MB |   14052 MB |  355995 MB |  354846 MB |\n",
            "|       from large pool |    1069 MB |   13966 MB |  355734 MB |  354665 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     260 MB |     180 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1192 MB |   14292 MB |  207172 MB |  205980 MB |\n",
            "|       from large pool |    1110 MB |   14206 MB |  207050 MB |  205940 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     122 MB |      40 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   43549 KB |   11315 MB |  273876 MB |  273833 MB |\n",
            "|       from large pool |   41868 KB |   11315 MB |  273448 MB |  273407 MB |\n",
            "|       from small pool |    1680 KB |       2 MB |     427 MB |     425 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3039    |    2709    |\n",
            "|       from large pool |      57    |     187    |    1550    |    1493    |\n",
            "|       from small pool |     273    |     312    |    1489    |    1216    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3039    |    2709    |\n",
            "|       from large pool |      57    |     187    |    1550    |    1493    |\n",
            "|       from small pool |     273    |     312    |    1489    |    1216    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |     217    |     156    |\n",
            "|       from large pool |      20    |      58    |     156    |     136    |\n",
            "|       from small pool |      41    |      43    |      61    |      20    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    1392    |    1386    |\n",
            "|       from large pool |       3    |      33    |     881    |     878    |\n",
            "|       from small pool |       3    |       6    |     511    |     508    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.30 GiB (GPU 0; 14.75 GiB total capacity; 6.63 GiB already allocated; 3.97 GiB free; 10.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 54           |        cudaMalloc retries: 61        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6785 MB |   14052 MB |  368267 MB |  361482 MB |\n",
            "|       from large pool |    6704 MB |   13966 MB |  368002 MB |  361298 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     265 MB |     184 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6785 MB |   14052 MB |  368267 MB |  361482 MB |\n",
            "|       from large pool |    6704 MB |   13966 MB |  368002 MB |  361298 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     265 MB |     184 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10238 MB |   14292 MB |  216834 MB |  206596 MB |\n",
            "|       from large pool |   10156 MB |   14206 MB |  216712 MB |  206556 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     122 MB |      40 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3452 MB |   11315 MB |  279035 MB |  275582 MB |\n",
            "|       from large pool |    3451 MB |   11315 MB |  278601 MB |  275149 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     434 MB |     433 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     346    |     499    |    3101    |    2755    |\n",
            "|       from large pool |      69    |     187    |    1587    |    1518    |\n",
            "|       from small pool |     277    |     312    |    1514    |    1237    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     346    |     499    |    3101    |    2755    |\n",
            "|       from large pool |      69    |     187    |    1587    |    1518    |\n",
            "|       from small pool |     277    |     312    |    1514    |    1237    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     101    |     228    |     163    |\n",
            "|       from large pool |      24    |      58    |     167    |     143    |\n",
            "|       from small pool |      41    |      43    |      61    |      20    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    1409    |    1401    |\n",
            "|       from large pool |       6    |      33    |     891    |     885    |\n",
            "|       from small pool |       2    |       6    |     518    |     516    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  37% 54/147 [00:15<00:23,  3.92it/s]2024-10-23 16:02:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.93 GiB (GPU 0; 14.75 GiB total capacity; 10.31 GiB already allocated; 41.06 MiB free; 13.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 55           |        cudaMalloc retries: 62        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10552 MB |   14052 MB |  388121 MB |  377569 MB |\n",
            "|       from large pool |   10470 MB |   13966 MB |  387849 MB |  377379 MB |\n",
            "|       from small pool |      82 MB |      85 MB |     272 MB |     189 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10552 MB |   14052 MB |  388121 MB |  377569 MB |\n",
            "|       from large pool |   10470 MB |   13966 MB |  387849 MB |  377379 MB |\n",
            "|       from small pool |      82 MB |      85 MB |     272 MB |     189 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14264 MB |   14292 MB |  220860 MB |  206596 MB |\n",
            "|       from large pool |   14180 MB |   14206 MB |  220736 MB |  206556 MB |\n",
            "|       from small pool |      84 MB |      86 MB |     124 MB |      40 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3711 MB |   11315 MB |  290601 MB |  286889 MB |\n",
            "|       from large pool |    3709 MB |   11315 MB |  290156 MB |  286446 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     444 MB |     442 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     362    |     499    |    3190    |    2828    |\n",
            "|       from large pool |      80    |     187    |    1642    |    1562    |\n",
            "|       from small pool |     282    |     312    |    1548    |    1266    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     362    |     499    |    3190    |    2828    |\n",
            "|       from large pool |      80    |     187    |    1642    |    1562    |\n",
            "|       from small pool |     282    |     312    |    1548    |    1266    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     101    |     230    |     163    |\n",
            "|       from large pool |      25    |      58    |     168    |     143    |\n",
            "|       from small pool |      42    |      43    |      62    |      20    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      39    |    1459    |    1439    |\n",
            "|       from large pool |      17    |      33    |     927    |     910    |\n",
            "|       from small pool |       3    |       6    |     532    |     529    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 16.79 GiB (GPU 0; 14.75 GiB total capacity; 941.70 MiB already allocated; 12.76 GiB free; 1.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 56           |        cudaMalloc retries: 63        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |     941 MB |   14052 MB |  388994 MB |  388052 MB |\n",
            "|       from large pool |     861 MB |   13966 MB |  388720 MB |  387858 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     274 MB |     194 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |     941 MB |   14052 MB |  388994 MB |  388052 MB |\n",
            "|       from large pool |     861 MB |   13966 MB |  388720 MB |  387858 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     274 MB |     194 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1234 MB |   14292 MB |  220860 MB |  219626 MB |\n",
            "|       from large pool |    1152 MB |   14206 MB |  220736 MB |  219584 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     124 MB |      42 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  299314 KB |   11315 MB |  292502 MB |  292209 MB |\n",
            "|       from large pool |  297392 KB |   11315 MB |  292050 MB |  291760 MB |\n",
            "|       from small pool |    1922 KB |       2 MB |     451 MB |     449 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3224    |    2894    |\n",
            "|       from large pool |      57    |     187    |    1657    |    1600    |\n",
            "|       from small pool |     273    |     312    |    1567    |    1294    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3224    |    2894    |\n",
            "|       from large pool |      57    |     187    |    1657    |    1600    |\n",
            "|       from small pool |     273    |     312    |    1567    |    1294    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |     230    |     169    |\n",
            "|       from large pool |      20    |      58    |     168    |     148    |\n",
            "|       from small pool |      41    |      43    |      62    |      21    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      14    |      39    |    1490    |    1476    |\n",
            "|       from large pool |      11    |      33    |     946    |     935    |\n",
            "|       from small pool |       3    |       6    |     544    |     541    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  38% 56/147 [00:16<00:28,  3.19it/s]2024-10-23 16:02:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 23.77 GiB (GPU 0; 14.75 GiB total capacity; 1.19 GiB already allocated; 12.65 GiB free; 1.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 57           |        cudaMalloc retries: 64        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1223 MB |   14052 MB |  390271 MB |  389047 MB |\n",
            "|       from large pool |    1143 MB |   13966 MB |  389993 MB |  388850 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     277 MB |     197 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1223 MB |   14052 MB |  390271 MB |  389047 MB |\n",
            "|       from large pool |    1143 MB |   13966 MB |  389993 MB |  388850 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     277 MB |     197 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1352 MB |   14292 MB |  221724 MB |  220372 MB |\n",
            "|       from large pool |    1270 MB |   14206 MB |  221600 MB |  220330 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     124 MB |      42 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  131632 KB |   11315 MB |  292591 MB |  292463 MB |\n",
            "|       from large pool |  130040 KB |   11315 MB |  292134 MB |  292007 MB |\n",
            "|       from small pool |    1592 KB |       2 MB |     457 MB |     455 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3258    |    2928    |\n",
            "|       from large pool |      57    |     187    |    1672    |    1615    |\n",
            "|       from small pool |     273    |     312    |    1586    |    1313    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3258    |    2928    |\n",
            "|       from large pool |      57    |     187    |    1672    |    1615    |\n",
            "|       from small pool |     273    |     312    |    1586    |    1313    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     101    |     239    |     177    |\n",
            "|       from large pool |      21    |      58    |     177    |     156    |\n",
            "|       from small pool |      41    |      43    |      62    |      21    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    1498    |    1492    |\n",
            "|       from large pool |       3    |      33    |     947    |     944    |\n",
            "|       from small pool |       3    |       6    |     551    |     548    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.89 GiB (GPU 0; 14.75 GiB total capacity; 656.21 MiB already allocated; 13.03 GiB free; 966.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 58           |        cudaMalloc retries: 65        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  671957 KB |   14052 MB |  390751 MB |  390094 MB |\n",
            "|       from large pool |  590371 KB |   13966 MB |  390472 MB |  389895 MB |\n",
            "|       from small pool |   81586 KB |      85 MB |     278 MB |     199 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  671957 KB |   14052 MB |  390751 MB |  390094 MB |\n",
            "|       from large pool |  590371 KB |   13966 MB |  390472 MB |  389895 MB |\n",
            "|       from small pool |   81586 KB |      85 MB |     278 MB |     199 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     966 MB |   14292 MB |  221724 MB |  220758 MB |\n",
            "|       from large pool |     886 MB |   14206 MB |  221600 MB |  220714 MB |\n",
            "|       from small pool |      80 MB |      86 MB |     124 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  317226 KB |   11315 MB |  292986 MB |  292676 MB |\n",
            "|       from large pool |  316893 KB |   11315 MB |  292522 MB |  292212 MB |\n",
            "|       from small pool |     333 KB |       2 MB |     464 MB |     463 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    3285    |    2961    |\n",
            "|       from large pool |      53    |     187    |    1682    |    1629    |\n",
            "|       from small pool |     271    |     312    |    1603    |    1332    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    3285    |    2961    |\n",
            "|       from large pool |      53    |     187    |    1682    |    1629    |\n",
            "|       from small pool |     271    |     312    |    1603    |    1332    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     239    |     182    |\n",
            "|       from large pool |      17    |      58    |     177    |     160    |\n",
            "|       from small pool |      40    |      43    |      62    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    1513    |    1503    |\n",
            "|       from large pool |       9    |      33    |     956    |     947    |\n",
            "|       from small pool |       1    |       6    |     557    |     556    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  39% 58/147 [00:16<00:20,  4.29it/s]2024-10-23 16:02:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 19.28 GiB (GPU 0; 14.75 GiB total capacity; 1.10 GiB already allocated; 12.77 GiB free; 1.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 59           |        cudaMalloc retries: 66        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1131 MB |   14052 MB |  391895 MB |  390764 MB |\n",
            "|       from large pool |    1050 MB |   13966 MB |  391613 MB |  390562 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     281 MB |     201 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1131 MB |   14052 MB |  391895 MB |  390764 MB |\n",
            "|       from large pool |    1050 MB |   13966 MB |  391613 MB |  390562 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     281 MB |     201 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1232 MB |   14292 MB |  221990 MB |  220758 MB |\n",
            "|       from large pool |    1150 MB |   14206 MB |  221864 MB |  220714 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     126 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  103142 KB |   11315 MB |  293228 MB |  293127 MB |\n",
            "|       from large pool |  101445 KB |   11315 MB |  292758 MB |  292659 MB |\n",
            "|       from small pool |    1696 KB |       2 MB |     469 MB |     468 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3319    |    2989    |\n",
            "|       from large pool |      57    |     187    |    1697    |    1640    |\n",
            "|       from small pool |     273    |     312    |    1622    |    1349    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3319    |    2989    |\n",
            "|       from large pool |      57    |     187    |    1697    |    1640    |\n",
            "|       from small pool |     273    |     312    |    1622    |    1349    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |     243    |     182    |\n",
            "|       from large pool |      20    |      58    |     180    |     160    |\n",
            "|       from small pool |      41    |      43    |      63    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      14    |      39    |    1532    |    1518    |\n",
            "|       from large pool |      11    |      33    |     968    |     957    |\n",
            "|       from small pool |       3    |       6    |     564    |     561    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB (GPU 0; 14.75 GiB total capacity; 11.05 GiB already allocated; 2.35 GiB free; 11.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 60           |        cudaMalloc retries: 67        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11314 MB |   14052 MB |  410393 MB |  399078 MB |\n",
            "|       from large pool |   11232 MB |   13966 MB |  410106 MB |  398873 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     287 MB |     205 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11314 MB |   14052 MB |  410393 MB |  399078 MB |\n",
            "|       from large pool |   11232 MB |   13966 MB |  410106 MB |  398873 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     287 MB |     205 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11894 MB |   14292 MB |  232828 MB |  220934 MB |\n",
            "|       from large pool |   11812 MB |   14206 MB |  232702 MB |  220890 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     126 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  593234 KB |   11315 MB |  297488 MB |  296909 MB |\n",
            "|       from large pool |  593036 KB |   11315 MB |  297011 MB |  296432 MB |\n",
            "|       from small pool |     198 KB |       2 MB |     476 MB |     476 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     363    |     499    |    3409    |    3046    |\n",
            "|       from large pool |      82    |     187    |    1756    |    1674    |\n",
            "|       from small pool |     281    |     312    |    1653    |    1372    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     363    |     499    |    3409    |    3046    |\n",
            "|       from large pool |      82    |     187    |    1756    |    1674    |\n",
            "|       from small pool |     281    |     312    |    1653    |    1372    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      66    |     101    |     250    |     184    |\n",
            "|       from large pool |      25    |      58    |     187    |     162    |\n",
            "|       from small pool |      41    |      43    |      63    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1567    |    1555    |\n",
            "|       from large pool |      10    |      33    |     996    |     986    |\n",
            "|       from small pool |       2    |       6    |     571    |     569    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  41% 60/147 [00:17<00:21,  4.10it/s]2024-10-23 16:02:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 23.24 GiB (GPU 0; 14.75 GiB total capacity; 1.18 GiB already allocated; 10.44 GiB free; 3.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 61           |        cudaMalloc retries: 68        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1209 MB |   14052 MB |  411649 MB |  410440 MB |\n",
            "|       from large pool |    1128 MB |   13966 MB |  411359 MB |  410230 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     290 MB |     209 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1209 MB |   14052 MB |  411649 MB |  410440 MB |\n",
            "|       from large pool |    1128 MB |   13966 MB |  411359 MB |  410230 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     290 MB |     209 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3610 MB |   14292 MB |  232828 MB |  229218 MB |\n",
            "|       from large pool |    3528 MB |   14206 MB |  232702 MB |  229174 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     126 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2400 MB |   11315 MB |  302284 MB |  299883 MB |\n",
            "|       from large pool |    2399 MB |   11315 MB |  301800 MB |  299401 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     484 MB |     482 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3443    |    3113    |\n",
            "|       from large pool |      57    |     187    |    1771    |    1714    |\n",
            "|       from small pool |     273    |     312    |    1672    |    1399    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3443    |    3113    |\n",
            "|       from large pool |      57    |     187    |    1771    |    1714    |\n",
            "|       from small pool |     273    |     312    |    1672    |    1399    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     250    |     191    |\n",
            "|       from large pool |      18    |      58    |     187    |     169    |\n",
            "|       from small pool |      41    |      43    |      63    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1595    |    1583    |\n",
            "|       from large pool |       9    |      33    |    1013    |    1004    |\n",
            "|       from small pool |       3    |       6    |     582    |     579    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  41% 61/147 [00:17<00:21,  4.02it/s]2024-10-23 16:02:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 31.38 GiB (GPU 0; 14.75 GiB total capacity; 1.14 GiB already allocated; 10.44 GiB free; 3.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 62           |        cudaMalloc retries: 69        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1164 MB |   14052 MB |  412842 MB |  411677 MB |\n",
            "|       from large pool |    1084 MB |   13966 MB |  412548 MB |  411464 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     293 MB |     213 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1164 MB |   14052 MB |  412842 MB |  411677 MB |\n",
            "|       from large pool |    1084 MB |   13966 MB |  412548 MB |  411464 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     293 MB |     213 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3610 MB |   14292 MB |  232828 MB |  229218 MB |\n",
            "|       from large pool |    3528 MB |   14206 MB |  232702 MB |  229174 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     126 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2445 MB |   11315 MB |  305292 MB |  302847 MB |\n",
            "|       from large pool |    2443 MB |   11315 MB |  304801 MB |  302358 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     490 MB |     488 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3477    |    3147    |\n",
            "|       from large pool |      57    |     187    |    1786    |    1729    |\n",
            "|       from small pool |     273    |     312    |    1691    |    1418    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3477    |    3147    |\n",
            "|       from large pool |      57    |     187    |    1786    |    1729    |\n",
            "|       from small pool |     273    |     312    |    1691    |    1418    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     250    |     191    |\n",
            "|       from large pool |      18    |      58    |     187    |     169    |\n",
            "|       from small pool |      41    |      43    |      63    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1613    |    1601    |\n",
            "|       from large pool |       9    |      33    |    1024    |    1015    |\n",
            "|       from small pool |       3    |       6    |     589    |     586    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.87 GiB (GPU 0; 14.75 GiB total capacity; 9.40 GiB already allocated; 2.71 GiB free; 11.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 63           |        cudaMalloc retries: 70        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9627 MB |   14052 MB |  427046 MB |  417418 MB |\n",
            "|       from large pool |    9546 MB |   13966 MB |  426747 MB |  417201 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     298 MB |     216 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9627 MB |   14052 MB |  427046 MB |  417418 MB |\n",
            "|       from large pool |    9546 MB |   13966 MB |  426747 MB |  417201 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     298 MB |     216 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11534 MB |   14292 MB |  240752 MB |  229218 MB |\n",
            "|       from large pool |   11452 MB |   14206 MB |  240626 MB |  229174 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     126 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1906 MB |   11315 MB |  309072 MB |  307166 MB |\n",
            "|       from large pool |    1905 MB |   11315 MB |  308574 MB |  306668 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     498 MB |     497 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    3540    |    3193    |\n",
            "|       from large pool |      69    |     187    |    1822    |    1753    |\n",
            "|       from small pool |     278    |     312    |    1718    |    1440    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    3540    |    3193    |\n",
            "|       from large pool |      69    |     187    |    1822    |    1753    |\n",
            "|       from small pool |     278    |     312    |    1718    |    1440    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |     252    |     191    |\n",
            "|       from large pool |      20    |      58    |     189    |     169    |\n",
            "|       from small pool |      41    |      43    |      63    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1643    |    1631    |\n",
            "|       from large pool |      10    |      33    |    1044    |    1034    |\n",
            "|       from small pool |       2    |       6    |     599    |     597    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 15.38 GiB (GPU 0; 14.75 GiB total capacity; 1.13 GiB already allocated; 10.44 GiB free; 3.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 64           |        cudaMalloc retries: 71        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1156 MB |   14052 MB |  428226 MB |  427070 MB |\n",
            "|       from large pool |    1075 MB |   13966 MB |  427925 MB |  426849 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     301 MB |     221 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1156 MB |   14052 MB |  428226 MB |  427070 MB |\n",
            "|       from large pool |    1075 MB |   13966 MB |  427925 MB |  426849 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     301 MB |     221 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3610 MB |   14292 MB |  240752 MB |  237142 MB |\n",
            "|       from large pool |    3528 MB |   14206 MB |  240626 MB |  237098 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     126 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2453 MB |   11315 MB |  312818 MB |  310364 MB |\n",
            "|       from large pool |    2452 MB |   11315 MB |  312312 MB |  309860 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     505 MB |     504 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3574    |    3244    |\n",
            "|       from large pool |      57    |     187    |    1837    |    1780    |\n",
            "|       from small pool |     273    |     312    |    1737    |    1464    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3574    |    3244    |\n",
            "|       from large pool |      57    |     187    |    1837    |    1780    |\n",
            "|       from small pool |     273    |     312    |    1737    |    1464    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     252    |     193    |\n",
            "|       from large pool |      18    |      58    |     189    |     171    |\n",
            "|       from small pool |      41    |      43    |      63    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1668    |    1656    |\n",
            "|       from large pool |       9    |      33    |    1059    |    1050    |\n",
            "|       from small pool |       3    |       6    |     609    |     606    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  44% 64/147 [00:17<00:18,  4.38it/s]2024-10-23 16:02:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.89 GiB (GPU 0; 14.75 GiB total capacity; 10.82 GiB already allocated; 1.77 GiB free; 12.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 65           |        cudaMalloc retries: 72        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11079 MB |   14052 MB |  446256 MB |  435177 MB |\n",
            "|       from large pool |   10997 MB |   13966 MB |  445949 MB |  434952 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     307 MB |     225 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11079 MB |   14052 MB |  446256 MB |  435177 MB |\n",
            "|       from large pool |   10997 MB |   13966 MB |  445949 MB |  434952 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     307 MB |     225 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12496 MB |   14292 MB |  249638 MB |  237142 MB |\n",
            "|       from large pool |   12414 MB |   14206 MB |  249512 MB |  237098 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     126 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1416 MB |   11315 MB |  317169 MB |  315752 MB |\n",
            "|       from large pool |    1416 MB |   11315 MB |  316653 MB |  315236 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     515 MB |     515 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     363    |     499    |    3664    |    3301    |\n",
            "|       from large pool |      81    |     187    |    1893    |    1812    |\n",
            "|       from small pool |     282    |     312    |    1771    |    1489    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     363    |     499    |    3664    |    3301    |\n",
            "|       from large pool |      81    |     187    |    1893    |    1812    |\n",
            "|       from small pool |     282    |     312    |    1771    |    1489    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     101    |     255    |     193    |\n",
            "|       from large pool |      21    |      58    |     192    |     171    |\n",
            "|       from small pool |      41    |      43    |      63    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      14    |      39    |    1711    |    1697    |\n",
            "|       from large pool |      12    |      33    |    1091    |    1079    |\n",
            "|       from small pool |       2    |       6    |     620    |     618    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 10.46 GiB (GPU 0; 14.75 GiB total capacity; 1.14 GiB already allocated; 10.44 GiB free; 3.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 66           |        cudaMalloc retries: 73        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1164 MB |   14052 MB |  447449 MB |  446284 MB |\n",
            "|       from large pool |    1084 MB |   13966 MB |  447139 MB |  446054 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     310 MB |     229 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1164 MB |   14052 MB |  447449 MB |  446284 MB |\n",
            "|       from large pool |    1084 MB |   13966 MB |  447139 MB |  446054 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     310 MB |     229 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3610 MB |   14292 MB |  249638 MB |  246028 MB |\n",
            "|       from large pool |    3528 MB |   14206 MB |  249512 MB |  245984 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     126 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2445 MB |   11315 MB |  321448 MB |  319003 MB |\n",
            "|       from large pool |    2443 MB |   11315 MB |  320924 MB |  318480 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     523 MB |     522 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3698    |    3368    |\n",
            "|       from large pool |      57    |     187    |    1908    |    1851    |\n",
            "|       from small pool |     273    |     312    |    1790    |    1517    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3698    |    3368    |\n",
            "|       from large pool |      57    |     187    |    1908    |    1851    |\n",
            "|       from small pool |     273    |     312    |    1790    |    1517    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     255    |     196    |\n",
            "|       from large pool |      18    |      58    |     192    |     174    |\n",
            "|       from small pool |      41    |      43    |      63    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1742    |    1730    |\n",
            "|       from large pool |       9    |      33    |    1110    |    1101    |\n",
            "|       from small pool |       3    |       6    |     632    |     629    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  45% 66/147 [00:18<00:21,  3.79it/s]2024-10-23 16:02:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.53 GiB (GPU 0; 14.75 GiB total capacity; 12.75 GiB already allocated; 349.06 MiB free; 13.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 67           |        cudaMalloc retries: 74        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   13056 MB |   14052 MB |  469714 MB |  456658 MB |\n",
            "|       from large pool |   12974 MB |   13966 MB |  469397 MB |  456423 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     317 MB |     235 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   13056 MB |   14052 MB |  469714 MB |  456658 MB |\n",
            "|       from large pool |   12974 MB |   13966 MB |  469397 MB |  456423 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     317 MB |     235 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13956 MB |   14292 MB |  259984 MB |  246028 MB |\n",
            "|       from large pool |   13872 MB |   14206 MB |  259856 MB |  245984 MB |\n",
            "|       from small pool |      84 MB |      86 MB |     128 MB |      44 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     899 MB |   11315 MB |  326299 MB |  325399 MB |\n",
            "|       from large pool |     897 MB |   11315 MB |  325763 MB |  324865 MB |\n",
            "|       from small pool |       2 MB |       2 MB |     536 MB |     534 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     379    |     499    |    3815    |    3436    |\n",
            "|       from large pool |      93    |     187    |    1984    |    1891    |\n",
            "|       from small pool |     286    |     312    |    1831    |    1545    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     379    |     499    |    3815    |    3436    |\n",
            "|       from large pool |      93    |     187    |    1984    |    1891    |\n",
            "|       from small pool |     286    |     312    |    1831    |    1545    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      64    |     101    |     260    |     196    |\n",
            "|       from large pool |      22    |      58    |     196    |     174    |\n",
            "|       from small pool |      42    |      43    |      64    |      22    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      13    |      39    |    1789    |    1776    |\n",
            "|       from large pool |      10    |      33    |    1144    |    1134    |\n",
            "|       from small pool |       3    |       6    |     645    |     642    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.83 GiB (GPU 0; 14.75 GiB total capacity; 1.13 GiB already allocated; 10.50 GiB free; 3.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 68           |        cudaMalloc retries: 75        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1157 MB |   14052 MB |  470896 MB |  469739 MB |\n",
            "|       from large pool |    1076 MB |   13966 MB |  470576 MB |  469499 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     320 MB |     239 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1157 MB |   14052 MB |  470896 MB |  469739 MB |\n",
            "|       from large pool |    1076 MB |   13966 MB |  470576 MB |  469499 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     320 MB |     239 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3554 MB |   14292 MB |  259984 MB |  256430 MB |\n",
            "|       from large pool |    3472 MB |   14206 MB |  259856 MB |  256384 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     128 MB |      46 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2396 MB |   11315 MB |  331059 MB |  328662 MB |\n",
            "|       from large pool |    2395 MB |   11315 MB |  330516 MB |  328121 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     543 MB |     541 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3849    |    3519    |\n",
            "|       from large pool |      57    |     187    |    1999    |    1942    |\n",
            "|       from small pool |     273    |     312    |    1850    |    1577    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3849    |    3519    |\n",
            "|       from large pool |      57    |     187    |    1999    |    1942    |\n",
            "|       from small pool |     273    |     312    |    1850    |    1577    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     260    |     201    |\n",
            "|       from large pool |      18    |      58    |     196    |     178    |\n",
            "|       from small pool |      41    |      43    |      64    |      23    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1824    |    1812    |\n",
            "|       from large pool |       9    |      33    |    1166    |    1157    |\n",
            "|       from small pool |       3    |       6    |     658    |     655    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  46% 68/147 [00:19<00:24,  3.23it/s]2024-10-23 16:02:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 21.70 GiB (GPU 0; 14.75 GiB total capacity; 1.15 GiB already allocated; 10.50 GiB free; 3.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 69           |        cudaMalloc retries: 76        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1179 MB |   14052 MB |  472111 MB |  470931 MB |\n",
            "|       from large pool |    1099 MB |   13966 MB |  471787 MB |  470688 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     323 MB |     242 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1179 MB |   14052 MB |  472111 MB |  470931 MB |\n",
            "|       from large pool |    1099 MB |   13966 MB |  471787 MB |  470688 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     323 MB |     242 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3554 MB |   14292 MB |  259984 MB |  256430 MB |\n",
            "|       from large pool |    3472 MB |   14206 MB |  259856 MB |  256384 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     128 MB |      46 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2374 MB |   11315 MB |  333979 MB |  331605 MB |\n",
            "|       from large pool |    2372 MB |   11315 MB |  333430 MB |  331058 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     549 MB |     547 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    3883    |    3553    |\n",
            "|       from large pool |      57    |     187    |    2014    |    1957    |\n",
            "|       from small pool |     273    |     312    |    1869    |    1596    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    3883    |    3553    |\n",
            "|       from large pool |      57    |     187    |    2014    |    1957    |\n",
            "|       from small pool |     273    |     312    |    1869    |    1596    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     260    |     201    |\n",
            "|       from large pool |      18    |      58    |     196    |     178    |\n",
            "|       from small pool |      41    |      43    |      64    |      23    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1842    |    1830    |\n",
            "|       from large pool |       9    |      33    |    1177    |    1168    |\n",
            "|       from small pool |       3    |       6    |     665    |     662    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.97 GiB (GPU 0; 14.75 GiB total capacity; 656.81 MiB already allocated; 13.03 GiB free; 966.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 70           |        cudaMalloc retries: 77        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  672572 KB |   14052 MB |  472591 MB |  471935 MB |\n",
            "|       from large pool |  590983 KB |   13966 MB |  472267 MB |  471690 MB |\n",
            "|       from small pool |   81589 KB |      85 MB |     324 MB |     245 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  672572 KB |   14052 MB |  472591 MB |  471935 MB |\n",
            "|       from large pool |  590983 KB |   13966 MB |  472267 MB |  471690 MB |\n",
            "|       from small pool |   81589 KB |      85 MB |     324 MB |     245 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     966 MB |   14292 MB |  259984 MB |  259018 MB |\n",
            "|       from large pool |     886 MB |   14206 MB |  259856 MB |  258970 MB |\n",
            "|       from small pool |      80 MB |      86 MB |     128 MB |      48 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  316611 KB |   11315 MB |  334644 MB |  334335 MB |\n",
            "|       from large pool |  316280 KB |   11315 MB |  334090 MB |  333781 MB |\n",
            "|       from small pool |     331 KB |       2 MB |     554 MB |     554 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    3910    |    3586    |\n",
            "|       from large pool |      53    |     187    |    2024    |    1971    |\n",
            "|       from small pool |     271    |     312    |    1886    |    1615    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    3910    |    3586    |\n",
            "|       from large pool |      53    |     187    |    2024    |    1971    |\n",
            "|       from small pool |     271    |     312    |    1886    |    1615    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     260    |     203    |\n",
            "|       from large pool |      17    |      58    |     196    |     179    |\n",
            "|       from small pool |      40    |      43    |      64    |      24    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    1858    |    1848    |\n",
            "|       from large pool |       9    |      33    |    1187    |    1178    |\n",
            "|       from small pool |       1    |       6    |     671    |     670    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  48% 70/147 [00:19<00:18,  4.23it/s]2024-10-23 16:02:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.56 GiB (GPU 0; 14.75 GiB total capacity; 12.92 GiB already allocated; 59.06 MiB free; 13.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 71           |        cudaMalloc retries: 78        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   13228 MB |   14052 MB |  495156 MB |  481927 MB |\n",
            "|       from large pool |   13146 MB |   13966 MB |  494824 MB |  481678 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     331 MB |     249 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   13228 MB |   14052 MB |  495156 MB |  481927 MB |\n",
            "|       from large pool |   13146 MB |   13966 MB |  494824 MB |  481678 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     331 MB |     249 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14246 MB |   14292 MB |  273264 MB |  259018 MB |\n",
            "|       from large pool |   14162 MB |   14206 MB |  273132 MB |  258970 MB |\n",
            "|       from small pool |      84 MB |      86 MB |     132 MB |      48 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1017 MB |   11315 MB |  339252 MB |  338235 MB |\n",
            "|       from large pool |    1015 MB |   11315 MB |  338686 MB |  337670 MB |\n",
            "|       from small pool |       2 MB |       2 MB |     566 MB |     564 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     379    |     499    |    4027    |    3648    |\n",
            "|       from large pool |      93    |     187    |    2100    |    2007    |\n",
            "|       from small pool |     286    |     312    |    1927    |    1641    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     379    |     499    |    4027    |    3648    |\n",
            "|       from large pool |      93    |     187    |    2100    |    2007    |\n",
            "|       from small pool |     286    |     312    |    1927    |    1641    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     101    |     270    |     203    |\n",
            "|       from large pool |      25    |      58    |     204    |     179    |\n",
            "|       from small pool |      42    |      43    |      66    |      24    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      13    |      39    |    1904    |    1891    |\n",
            "|       from large pool |      10    |      33    |    1220    |    1210    |\n",
            "|       from small pool |       3    |       6    |     684    |     681    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 14.33 GiB (GPU 0; 14.75 GiB total capacity; 1021.56 MiB already allocated; 10.46 GiB free; 3.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 72           |        cudaMalloc retries: 79        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1021 MB |   14052 MB |  496143 MB |  495121 MB |\n",
            "|       from large pool |     941 MB |   13966 MB |  495809 MB |  494868 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     334 MB |     253 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1021 MB |   14052 MB |  496143 MB |  495121 MB |\n",
            "|       from large pool |     941 MB |   13966 MB |  495809 MB |  494868 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     334 MB |     253 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3592 MB |   14292 MB |  273264 MB |  269672 MB |\n",
            "|       from large pool |    3510 MB |   14206 MB |  273132 MB |  269622 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     132 MB |      50 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2570 MB |   11315 MB |  344029 MB |  341459 MB |\n",
            "|       from large pool |    2568 MB |   11315 MB |  343456 MB |  340888 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     573 MB |     571 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    4061    |    3731    |\n",
            "|       from large pool |      57    |     187    |    2115    |    2058    |\n",
            "|       from small pool |     273    |     312    |    1946    |    1673    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    4061    |    3731    |\n",
            "|       from large pool |      57    |     187    |    2115    |    2058    |\n",
            "|       from small pool |     273    |     312    |    1946    |    1673    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     270    |     211    |\n",
            "|       from large pool |      18    |      58    |     204    |     186    |\n",
            "|       from small pool |      41    |      43    |      66    |      25    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      11    |      39    |    1938    |    1927    |\n",
            "|       from large pool |       9    |      33    |    1241    |    1232    |\n",
            "|       from small pool |       2    |       6    |     697    |     695    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  49% 72/147 [00:20<00:21,  3.42it/s]2024-10-23 16:02:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 10.32 GiB (GPU 0; 14.75 GiB total capacity; 11.38 GiB already allocated; 141.06 MiB free; 13.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 73           |        cudaMalloc retries: 80        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11657 MB |   14052 MB |  507795 MB |  496137 MB |\n",
            "|       from large pool |   11577 MB |   13966 MB |  507458 MB |  495881 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     336 MB |     256 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11657 MB |   14052 MB |  507795 MB |  496137 MB |\n",
            "|       from large pool |   11577 MB |   13966 MB |  507458 MB |  495881 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     336 MB |     256 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14164 MB |   14292 MB |  283836 MB |  269672 MB |\n",
            "|       from large pool |   14082 MB |   14206 MB |  283704 MB |  269622 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     132 MB |      50 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2506 MB |   11315 MB |  347078 MB |  344571 MB |\n",
            "|       from large pool |    2504 MB |   11315 MB |  346498 MB |  343994 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     579 MB |     577 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    4096    |    3765    |\n",
            "|       from large pool |      58    |     187    |    2131    |    2073    |\n",
            "|       from small pool |     273    |     312    |    1965    |    1692    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    4096    |    3765    |\n",
            "|       from large pool |      58    |     187    |    2131    |    2073    |\n",
            "|       from small pool |     273    |     312    |    1965    |    1692    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      60    |     101    |     271    |     211    |\n",
            "|       from large pool |      19    |      58    |     205    |     186    |\n",
            "|       from small pool |      41    |      43    |      66    |      25    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      12    |      39    |    1956    |    1944    |\n",
            "|       from large pool |      10    |      33    |    1254    |    1244    |\n",
            "|       from small pool |       2    |       6    |     702    |     700    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 22.24 GiB (GPU 0; 14.75 GiB total capacity; 1.16 GiB already allocated; 10.46 GiB free; 3.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 74           |        cudaMalloc retries: 81        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1190 MB |   14052 MB |  509024 MB |  507834 MB |\n",
            "|       from large pool |    1109 MB |   13966 MB |  508684 MB |  507574 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     340 MB |     259 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1190 MB |   14052 MB |  509024 MB |  507834 MB |\n",
            "|       from large pool |    1109 MB |   13966 MB |  508684 MB |  507574 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     340 MB |     259 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3592 MB |   14292 MB |  283836 MB |  280244 MB |\n",
            "|       from large pool |    3510 MB |   14206 MB |  283704 MB |  280194 MB |\n",
            "|       from small pool |      82 MB |      86 MB |     132 MB |      50 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2401 MB |   11315 MB |  350002 MB |  347601 MB |\n",
            "|       from large pool |    2400 MB |   11315 MB |  349417 MB |  347017 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     585 MB |     583 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    4130    |    3800    |\n",
            "|       from large pool |      57    |     187    |    2146    |    2089    |\n",
            "|       from small pool |     273    |     312    |    1984    |    1711    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    4130    |    3800    |\n",
            "|       from large pool |      57    |     187    |    2146    |    2089    |\n",
            "|       from small pool |     273    |     312    |    1984    |    1711    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     271    |     212    |\n",
            "|       from large pool |      18    |      58    |     205    |     187    |\n",
            "|       from small pool |      41    |      43    |      66    |      25    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      11    |      39    |    1975    |    1964    |\n",
            "|       from large pool |       8    |      33    |    1266    |    1258    |\n",
            "|       from small pool |       3    |       6    |     709    |     706    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  50% 74/147 [00:20<00:21,  3.42it/s]2024-10-23 16:02:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 18.68 GiB (GPU 0; 14.75 GiB total capacity; 638.63 MiB already allocated; 13.03 GiB free; 966.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 75           |        cudaMalloc retries: 82        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  653957 KB |   14052 MB |  509478 MB |  508839 MB |\n",
            "|       from large pool |  572383 KB |   13966 MB |  509136 MB |  508577 MB |\n",
            "|       from small pool |   81574 KB |      85 MB |     341 MB |     261 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  653957 KB |   14052 MB |  509478 MB |  508839 MB |\n",
            "|       from large pool |  572383 KB |   13966 MB |  509136 MB |  508577 MB |\n",
            "|       from small pool |   81574 KB |      85 MB |     341 MB |     261 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     966 MB |   14292 MB |  283836 MB |  282870 MB |\n",
            "|       from large pool |     886 MB |   14206 MB |  283704 MB |  282818 MB |\n",
            "|       from small pool |      80 MB |      86 MB |     132 MB |      52 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  335226 KB |   11315 MB |  350690 MB |  350363 MB |\n",
            "|       from large pool |  334880 KB |   11315 MB |  350098 MB |  349771 MB |\n",
            "|       from small pool |     346 KB |       2 MB |     592 MB |     591 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    4157    |    3833    |\n",
            "|       from large pool |      53    |     187    |    2156    |    2103    |\n",
            "|       from small pool |     271    |     312    |    2001    |    1730    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    4157    |    3833    |\n",
            "|       from large pool |      53    |     187    |    2156    |    2103    |\n",
            "|       from small pool |     271    |     312    |    2001    |    1730    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     271    |     214    |\n",
            "|       from large pool |      17    |      58    |     205    |     188    |\n",
            "|       from small pool |      40    |      43    |      66    |      26    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    1992    |    1982    |\n",
            "|       from large pool |       9    |      33    |    1277    |    1268    |\n",
            "|       from small pool |       1    |       6    |     715    |     714    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 0; 14.75 GiB total capacity; 13.56 GiB already allocated; 213.06 MiB free; 13.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 76           |        cudaMalloc retries: 83        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   13882 MB |   14052 MB |  534028 MB |  520146 MB |\n",
            "|       from large pool |   13796 MB |   13966 MB |  533674 MB |  519877 MB |\n",
            "|       from small pool |      85 MB |      85 MB |     354 MB |     268 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   13882 MB |   14052 MB |  534028 MB |  520146 MB |\n",
            "|       from large pool |   13796 MB |   13966 MB |  533674 MB |  519877 MB |\n",
            "|       from small pool |      85 MB |      85 MB |     354 MB |     268 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14092 MB |   14292 MB |  296964 MB |  282872 MB |\n",
            "|       from large pool |   14006 MB |   14206 MB |  296824 MB |  282818 MB |\n",
            "|       from small pool |      86 MB |      88 MB |     140 MB |      54 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  215032 KB |   11315 MB |  360232 MB |  360022 MB |\n",
            "|       from large pool |  214538 KB |   11315 MB |  359621 MB |  359412 MB |\n",
            "|       from small pool |     494 KB |       2 MB |     610 MB |     609 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     433    |     499    |    4376    |    3943    |\n",
            "|       from large pool |     135    |     187    |    2306    |    2171    |\n",
            "|       from small pool |     298    |     312    |    2070    |    1772    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     433    |     499    |    4376    |    3943    |\n",
            "|       from large pool |     135    |     187    |    2306    |    2171    |\n",
            "|       from small pool |     298    |     312    |    2070    |    1772    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      91    |     101    |     306    |     215    |\n",
            "|       from large pool |      48    |      58    |     236    |     188    |\n",
            "|       from small pool |      43    |      44    |      70    |      27    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      18    |      39    |    2054    |    2036    |\n",
            "|       from large pool |      13    |      33    |    1324    |    1311    |\n",
            "|       from small pool |       5    |       6    |     730    |     725    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  52% 76/147 [00:21<00:23,  3.01it/s]2024-10-23 16:02:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 10.75 GiB (GPU 0; 14.75 GiB total capacity; 11.90 GiB already allocated; 1.99 GiB free; 11.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 77           |        cudaMalloc retries: 85        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12182 MB |   14052 MB |  546242 MB |  534060 MB |\n",
            "|       from large pool |   12101 MB |   13966 MB |  545885 MB |  533784 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     357 MB |     276 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12182 MB |   14052 MB |  546242 MB |  534060 MB |\n",
            "|       from large pool |   12101 MB |   13966 MB |  545885 MB |  533784 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     357 MB |     276 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12264 MB |   14292 MB |  307972 MB |  295708 MB |\n",
            "|       from large pool |   12182 MB |   14206 MB |  307832 MB |  295650 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     140 MB |      58 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   83879 KB |   11315 MB |  364359 MB |  364277 MB |\n",
            "|       from large pool |   82247 KB |   11315 MB |  363738 MB |  363658 MB |\n",
            "|       from small pool |    1632 KB |       2 MB |     620 MB |     619 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    4411    |    4080    |\n",
            "|       from large pool |      58    |     187    |    2322    |    2264    |\n",
            "|       from small pool |     273    |     312    |    2089    |    1816    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    4411    |    4080    |\n",
            "|       from large pool |      58    |     187    |    2322    |    2264    |\n",
            "|       from small pool |     273    |     312    |    2089    |    1816    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     101    |     307    |     245    |\n",
            "|       from large pool |      21    |      58    |     237    |     216    |\n",
            "|       from small pool |      41    |      44    |      70    |      29    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      39    |    2099    |    2084    |\n",
            "|       from large pool |      12    |      33    |    1349    |    1337    |\n",
            "|       from small pool |       3    |       6    |     750    |     747    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  52% 77/147 [00:22<00:23,  2.94it/s]2024-10-23 16:02:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB (GPU 0; 14.75 GiB total capacity; 10.06 GiB already allocated; 1.99 GiB free; 11.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 78           |        cudaMalloc retries: 86        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10303 MB |   14052 MB |  562976 MB |  552672 MB |\n",
            "|       from large pool |   10222 MB |   13966 MB |  562613 MB |  552391 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     362 MB |     281 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10303 MB |   14052 MB |  562976 MB |  552672 MB |\n",
            "|       from large pool |   10222 MB |   13966 MB |  562613 MB |  552391 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     362 MB |     281 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12264 MB |   14292 MB |  307972 MB |  295708 MB |\n",
            "|       from large pool |   12182 MB |   14206 MB |  307832 MB |  295650 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     140 MB |      58 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1960 MB |   11315 MB |  379799 MB |  377839 MB |\n",
            "|       from large pool |    1959 MB |   11315 MB |  379168 MB |  377208 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     631 MB |     630 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     363    |     499    |    4501    |    4138    |\n",
            "|       from large pool |      81    |     187    |    2378    |    2297    |\n",
            "|       from small pool |     282    |     312    |    2123    |    1841    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     363    |     499    |    4501    |    4138    |\n",
            "|       from large pool |      81    |     187    |    2378    |    2297    |\n",
            "|       from small pool |     282    |     312    |    2123    |    1841    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     101    |     307    |     245    |\n",
            "|       from large pool |      21    |      58    |     237    |     216    |\n",
            "|       from small pool |      41    |      44    |      70    |      29    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      16    |      39    |    2139    |    2123    |\n",
            "|       from large pool |      14    |      33    |    1378    |    1364    |\n",
            "|       from small pool |       2    |       6    |     761    |     759    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  53% 78/147 [00:22<00:22,  3.13it/s]2024-10-23 16:02:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 25.89 GiB (GPU 0; 14.75 GiB total capacity; 692.86 MiB already allocated; 13.03 GiB free; 966.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 79           |        cudaMalloc retries: 87        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  709485 KB |   14052 MB |  563510 MB |  562817 MB |\n",
            "|       from large pool |  627871 KB |   13966 MB |  563146 MB |  562533 MB |\n",
            "|       from small pool |   81614 KB |      85 MB |     364 MB |     284 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  709485 KB |   14052 MB |  563510 MB |  562817 MB |\n",
            "|       from large pool |  627871 KB |   13966 MB |  563146 MB |  562533 MB |\n",
            "|       from small pool |   81614 KB |      85 MB |     364 MB |     284 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     966 MB |   14292 MB |  307972 MB |  307006 MB |\n",
            "|       from large pool |     886 MB |   14206 MB |  307832 MB |  306946 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     140 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  279699 KB |   11315 MB |  389670 MB |  389397 MB |\n",
            "|       from large pool |  279393 KB |   11315 MB |  389032 MB |  388760 MB |\n",
            "|       from small pool |     306 KB |       2 MB |     637 MB |     637 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    4528    |    4204    |\n",
            "|       from large pool |      53    |     187    |    2388    |    2335    |\n",
            "|       from small pool |     271    |     312    |    2140    |    1869    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    4528    |    4204    |\n",
            "|       from large pool |      53    |     187    |    2388    |    2335    |\n",
            "|       from small pool |     271    |     312    |    2140    |    1869    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     307    |     250    |\n",
            "|       from large pool |      17    |      58    |     237    |     220    |\n",
            "|       from small pool |      40    |      44    |      70    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    2166    |    2156    |\n",
            "|       from large pool |       9    |      33    |    1394    |    1385    |\n",
            "|       from small pool |       1    |       6    |     772    |     771    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  54% 79/147 [00:23<00:27,  2.48it/s]2024-10-23 16:02:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 36.20 GiB (GPU 0; 14.75 GiB total capacity; 1.20 GiB already allocated; 12.63 GiB free; 1.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 80           |        cudaMalloc retries: 88        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1225 MB |   14052 MB |  564790 MB |  563564 MB |\n",
            "|       from large pool |    1145 MB |   13966 MB |  564423 MB |  563277 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     367 MB |     287 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1225 MB |   14052 MB |  564790 MB |  563564 MB |\n",
            "|       from large pool |    1145 MB |   13966 MB |  564423 MB |  563277 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     367 MB |     287 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1370 MB |   14292 MB |  308856 MB |  307486 MB |\n",
            "|       from large pool |    1288 MB |   14206 MB |  308714 MB |  307426 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  147525 KB |   11315 MB |  389771 MB |  389627 MB |\n",
            "|       from large pool |  145928 KB |   11315 MB |  389127 MB |  388985 MB |\n",
            "|       from small pool |    1597 KB |       2 MB |     643 MB |     642 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    4562    |    4232    |\n",
            "|       from large pool |      57    |     187    |    2403    |    2346    |\n",
            "|       from small pool |     273    |     312    |    2159    |    1886    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    4562    |    4232    |\n",
            "|       from large pool |      57    |     187    |    2403    |    2346    |\n",
            "|       from small pool |     273    |     312    |    2159    |    1886    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     101    |     317    |     255    |\n",
            "|       from large pool |      21    |      58    |     246    |     225    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      39    |    2187    |    2172    |\n",
            "|       from large pool |      12    |      33    |    1408    |    1396    |\n",
            "|       from small pool |       3    |       6    |     779    |     776    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.73 GiB (GPU 0; 14.75 GiB total capacity; 11.02 GiB already allocated; 1.70 GiB free; 12.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 81           |        cudaMalloc retries: 89        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11286 MB |   14052 MB |  583211 MB |  571925 MB |\n",
            "|       from large pool |   11204 MB |   13966 MB |  582840 MB |  571635 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     371 MB |     290 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11286 MB |   14052 MB |  583211 MB |  571925 MB |\n",
            "|       from large pool |   11204 MB |   13966 MB |  582840 MB |  571635 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     371 MB |     290 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12562 MB |   14292 MB |  320048 MB |  307486 MB |\n",
            "|       from large pool |   12480 MB |   14206 MB |  319906 MB |  307426 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1275 MB |   11315 MB |  394205 MB |  392929 MB |\n",
            "|       from large pool |    1275 MB |   11315 MB |  393555 MB |  392280 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     650 MB |     649 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     363    |     499    |    4652    |    4289    |\n",
            "|       from large pool |      82    |     187    |    2462    |    2380    |\n",
            "|       from small pool |     281    |     312    |    2190    |    1909    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     363    |     499    |    4652    |    4289    |\n",
            "|       from large pool |      82    |     187    |    2462    |    2380    |\n",
            "|       from small pool |     281    |     312    |    2190    |    1909    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      66    |     101    |     321    |     255    |\n",
            "|       from large pool |      25    |      58    |     250    |     225    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      18    |      39    |    2228    |    2210    |\n",
            "|       from large pool |      16    |      33    |    1444    |    1428    |\n",
            "|       from small pool |       2    |       6    |     784    |     782    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.24 GiB (GPU 0; 14.75 GiB total capacity; 13.44 GiB already allocated; 405.06 MiB free; 13.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 82           |        cudaMalloc retries: 91        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   13762 MB |   14052 MB |  597031 MB |  583269 MB |\n",
            "|       from large pool |   13681 MB |   13966 MB |  596656 MB |  582974 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     375 MB |     294 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   13762 MB |   14052 MB |  597031 MB |  583269 MB |\n",
            "|       from large pool |   13681 MB |   13966 MB |  596656 MB |  582974 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     375 MB |     294 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13900 MB |   14292 MB |  332578 MB |  318678 MB |\n",
            "|       from large pool |   13818 MB |   14206 MB |  332436 MB |  318618 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  141040 KB |   11315 MB |  396154 MB |  396016 MB |\n",
            "|       from large pool |  139472 KB |   11315 MB |  395496 MB |  395360 MB |\n",
            "|       from small pool |    1568 KB |       2 MB |     657 MB |     656 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    4687    |    4356    |\n",
            "|       from large pool |      58    |     187    |    2478    |    2420    |\n",
            "|       from small pool |     273    |     312    |    2209    |    1936    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    4687    |    4356    |\n",
            "|       from large pool |      58    |     187    |    2478    |    2420    |\n",
            "|       from small pool |     273    |     312    |    2209    |    1936    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     101    |     322    |     259    |\n",
            "|       from large pool |      22    |      58    |     251    |     229    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      39    |    2260    |    2245    |\n",
            "|       from large pool |      12    |      33    |    1465    |    1453    |\n",
            "|       from small pool |       3    |       6    |     795    |     792    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  56% 82/147 [00:24<00:22,  2.89it/s]2024-10-23 16:02:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.37 GiB (GPU 0; 14.75 GiB total capacity; 10.49 GiB already allocated; 405.06 MiB free; 13.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:22 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 83           |        cudaMalloc retries: 92        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10743 MB |   14052 MB |  612926 MB |  602182 MB |\n",
            "|       from large pool |   10663 MB |   13966 MB |  612547 MB |  601884 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     378 MB |     297 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10743 MB |   14052 MB |  612926 MB |  602182 MB |\n",
            "|       from large pool |   10663 MB |   13966 MB |  612547 MB |  601884 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     378 MB |     297 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13900 MB |   14292 MB |  332578 MB |  318678 MB |\n",
            "|       from large pool |   13818 MB |   14206 MB |  332436 MB |  318618 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3156 MB |   11315 MB |  409679 MB |  406523 MB |\n",
            "|       from large pool |    3154 MB |   11315 MB |  409015 MB |  405860 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     663 MB |     662 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    4750    |    4403    |\n",
            "|       from large pool |      70    |     187    |    2516    |    2446    |\n",
            "|       from small pool |     277    |     312    |    2234    |    1957    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    4750    |    4403    |\n",
            "|       from large pool |      70    |     187    |    2516    |    2446    |\n",
            "|       from small pool |     277    |     312    |    2234    |    1957    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     101    |     322    |     259    |\n",
            "|       from large pool |      22    |      58    |     251    |     229    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      39    |    2292    |    2277    |\n",
            "|       from large pool |      13    |      33    |    1490    |    1477    |\n",
            "|       from small pool |       2    |       6    |     802    |     800    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  56% 83/147 [00:24<00:21,  3.04it/s]2024-10-23 16:02:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.07 GiB (GPU 0; 14.75 GiB total capacity; 8.07 GiB already allocated; 405.06 MiB free; 13.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 84           |        cudaMalloc retries: 93        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8261 MB |   14052 MB |  628252 MB |  619991 MB |\n",
            "|       from large pool |    8180 MB |   13966 MB |  627870 MB |  619689 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     382 MB |     301 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8261 MB |   14052 MB |  628252 MB |  619991 MB |\n",
            "|       from large pool |    8180 MB |   13966 MB |  627870 MB |  619689 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     382 MB |     301 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13900 MB |   14292 MB |  332578 MB |  318678 MB |\n",
            "|       from large pool |   13818 MB |   14206 MB |  332436 MB |  318618 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    5638 MB |   12725 MB |  432826 MB |  427188 MB |\n",
            "|       from large pool |    5637 MB |   12723 MB |  432156 MB |  426519 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     670 MB |     669 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     346    |     499    |    4812    |    4466    |\n",
            "|       from large pool |      69    |     187    |    2553    |    2484    |\n",
            "|       from small pool |     277    |     312    |    2259    |    1982    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     346    |     499    |    4812    |    4466    |\n",
            "|       from large pool |      69    |     187    |    2553    |    2484    |\n",
            "|       from small pool |     277    |     312    |    2259    |    1982    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     101    |     322    |     259    |\n",
            "|       from large pool |      22    |      58    |     251    |     229    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      16    |      39    |    2330    |    2314    |\n",
            "|       from large pool |      14    |      33    |    1519    |    1505    |\n",
            "|       from small pool |       2    |       6    |     811    |     809    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  57% 84/147 [00:24<00:21,  3.00it/s]2024-10-23 16:02:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.24 GiB (GPU 0; 14.75 GiB total capacity; 8.53 GiB already allocated; 405.06 MiB free; 13.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 85           |        cudaMalloc retries: 94        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8739 MB |   14052 MB |  641200 MB |  632461 MB |\n",
            "|       from large pool |    8658 MB |   13966 MB |  640815 MB |  632156 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     385 MB |     304 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8739 MB |   14052 MB |  641200 MB |  632461 MB |\n",
            "|       from large pool |    8658 MB |   13966 MB |  640815 MB |  632156 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     385 MB |     304 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13900 MB |   14292 MB |  332578 MB |  318678 MB |\n",
            "|       from large pool |   13818 MB |   14206 MB |  332436 MB |  318618 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    5160 MB |   12725 MB |  453291 MB |  448131 MB |\n",
            "|       from large pool |    5159 MB |   12723 MB |  452614 MB |  447454 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     677 MB |     676 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    4875    |    4528    |\n",
            "|       from large pool |      70    |     187    |    2591    |    2521    |\n",
            "|       from small pool |     277    |     312    |    2284    |    2007    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    4875    |    4528    |\n",
            "|       from large pool |      70    |     187    |    2591    |    2521    |\n",
            "|       from small pool |     277    |     312    |    2284    |    2007    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     101    |     322    |     259    |\n",
            "|       from large pool |      22    |      58    |     251    |     229    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      17    |      39    |    2365    |    2348    |\n",
            "|       from large pool |      15    |      33    |    1545    |    1530    |\n",
            "|       from small pool |       2    |       6    |     820    |     818    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  58% 85/147 [00:25<00:21,  2.82it/s]2024-10-23 16:02:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 27.79 GiB (GPU 0; 14.75 GiB total capacity; 1.09 GiB already allocated; 12.73 GiB free; 1.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 86           |        cudaMalloc retries: 95        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1115 MB |   14052 MB |  642323 MB |  641207 MB |\n",
            "|       from large pool |    1035 MB |   13966 MB |  641934 MB |  640899 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     388 MB |     308 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1115 MB |   14052 MB |  642323 MB |  641207 MB |\n",
            "|       from large pool |    1035 MB |   13966 MB |  641934 MB |  640899 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     388 MB |     308 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1272 MB |   14292 MB |  332578 MB |  331306 MB |\n",
            "|       from large pool |    1190 MB |   14206 MB |  332436 MB |  331246 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  160127 KB |   12725 MB |  461123 MB |  460967 MB |\n",
            "|       from large pool |  158408 KB |   12723 MB |  460439 MB |  460284 MB |\n",
            "|       from small pool |    1719 KB |       2 MB |     683 MB |     682 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    4909    |    4579    |\n",
            "|       from large pool |      57    |     187    |    2606    |    2549    |\n",
            "|       from small pool |     273    |     312    |    2303    |    2030    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    4909    |    4579    |\n",
            "|       from large pool |      57    |     187    |    2606    |    2549    |\n",
            "|       from small pool |     273    |     312    |    2303    |    2030    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |     322    |     261    |\n",
            "|       from large pool |      20    |      58    |     251    |     231    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      13    |      39    |    2385    |    2372    |\n",
            "|       from large pool |      11    |      33    |    1558    |    1547    |\n",
            "|       from small pool |       2    |       6    |     827    |     825    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  59% 86/147 [00:25<00:26,  2.34it/s]2024-10-23 16:02:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 24.83 GiB (GPU 0; 14.75 GiB total capacity; 1.21 GiB already allocated; 12.63 GiB free; 1.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 87           |        cudaMalloc retries: 96        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1242 MB |   14052 MB |  643626 MB |  642384 MB |\n",
            "|       from large pool |    1161 MB |   13966 MB |  643234 MB |  642072 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     392 MB |     311 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1242 MB |   14052 MB |  643626 MB |  642384 MB |\n",
            "|       from large pool |    1161 MB |   13966 MB |  643234 MB |  642072 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     392 MB |     311 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1370 MB |   14292 MB |  332676 MB |  331306 MB |\n",
            "|       from large pool |    1288 MB |   14206 MB |  332534 MB |  331246 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  131067 KB |   12725 MB |  461237 MB |  461109 MB |\n",
            "|       from large pool |  129497 KB |   12723 MB |  460546 MB |  460420 MB |\n",
            "|       from small pool |    1570 KB |       2 MB |     690 MB |     688 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    4943    |    4613    |\n",
            "|       from large pool |      57    |     187    |    2621    |    2564    |\n",
            "|       from small pool |     273    |     312    |    2322    |    2049    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    4943    |    4613    |\n",
            "|       from large pool |      57    |     187    |    2621    |    2564    |\n",
            "|       from small pool |     273    |     312    |    2322    |    2049    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     101    |     323    |     261    |\n",
            "|       from large pool |      21    |      58    |     252    |     231    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2393    |    2387    |\n",
            "|       from large pool |       3    |      33    |    1559    |    1556    |\n",
            "|       from small pool |       3    |       6    |     834    |     831    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.57 GiB (GPU 0; 14.75 GiB total capacity; 8.74 GiB already allocated; 1.93 GiB free; 12.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 88           |        cudaMalloc retries: 97        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8950 MB |   14052 MB |  656804 MB |  647853 MB |\n",
            "|       from large pool |    8869 MB |   13966 MB |  656407 MB |  647538 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     396 MB |     315 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8950 MB |   14052 MB |  656804 MB |  647853 MB |\n",
            "|       from large pool |    8869 MB |   13966 MB |  656407 MB |  647538 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     396 MB |     315 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12326 MB |   14292 MB |  343632 MB |  331306 MB |\n",
            "|       from large pool |   12244 MB |   14206 MB |  343490 MB |  331246 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3375 MB |   12725 MB |  465906 MB |  462531 MB |\n",
            "|       from large pool |    3374 MB |   12723 MB |  465207 MB |  461832 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     699 MB |     699 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    5006    |    4659    |\n",
            "|       from large pool |      69    |     187    |    2657    |    2588    |\n",
            "|       from small pool |     278    |     312    |    2349    |    2071    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    5006    |    4659    |\n",
            "|       from large pool |      69    |     187    |    2657    |    2588    |\n",
            "|       from small pool |     278    |     312    |    2349    |    2071    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     101    |     326    |     261    |\n",
            "|       from large pool |      24    |      58    |     255    |     231    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      18    |      39    |    2429    |    2411    |\n",
            "|       from large pool |      16    |      33    |    1585    |    1569    |\n",
            "|       from small pool |       2    |       6    |     844    |     842    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.02 GiB (GPU 0; 14.75 GiB total capacity; 9.73 GiB already allocated; 679.06 MiB free; 13.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 89           |        cudaMalloc retries: 99        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9963 MB |   14052 MB |  671516 MB |  661553 MB |\n",
            "|       from large pool |    9882 MB |   13966 MB |  671116 MB |  661234 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     399 MB |     319 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9963 MB |   14052 MB |  671516 MB |  661553 MB |\n",
            "|       from large pool |    9882 MB |   13966 MB |  671116 MB |  661234 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     399 MB |     319 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13626 MB |   14292 MB |  355986 MB |  342360 MB |\n",
            "|       from large pool |   13544 MB |   14206 MB |  355844 MB |  342300 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     142 MB |      60 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3662 MB |   12725 MB |  471751 MB |  468088 MB |\n",
            "|       from large pool |    3661 MB |   12723 MB |  471044 MB |  467383 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     706 MB |     705 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    5069    |    4722    |\n",
            "|       from large pool |      70    |     187    |    2695    |    2625    |\n",
            "|       from small pool |     277    |     312    |    2374    |    2097    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    5069    |    4722    |\n",
            "|       from large pool |      70    |     187    |    2695    |    2625    |\n",
            "|       from small pool |     277    |     312    |    2374    |    2097    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      64    |     101    |     329    |     265    |\n",
            "|       from large pool |      23    |      58    |     258    |     235    |\n",
            "|       from small pool |      41    |      44    |      71    |      30    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      16    |      39    |    2468    |    2452    |\n",
            "|       from large pool |      14    |      33    |    1614    |    1600    |\n",
            "|       from small pool |       2    |       6    |     854    |     852    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  61% 89/147 [00:26<00:18,  3.09it/s]2024-10-23 16:02:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 32.35 GiB (GPU 0; 14.75 GiB total capacity; 735.25 MiB already allocated; 13.02 GiB free; 976.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 90           |        cudaMalloc retries: 100       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  752893 KB |   14052 MB |  672113 MB |  671378 MB |\n",
            "|       from large pool |  671246 KB |   13966 MB |  671711 MB |  671056 MB |\n",
            "|       from small pool |   81647 KB |      85 MB |     401 MB |     321 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  752893 KB |   14052 MB |  672113 MB |  671378 MB |\n",
            "|       from large pool |  671246 KB |   13966 MB |  671711 MB |  671056 MB |\n",
            "|       from small pool |   81647 KB |      85 MB |     401 MB |     321 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     976 MB |   14292 MB |  355986 MB |  355010 MB |\n",
            "|       from large pool |     896 MB |   14206 MB |  355844 MB |  354948 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     142 MB |      62 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  246530 KB |   12725 MB |  472923 MB |  472682 MB |\n",
            "|       from large pool |  246258 KB |   12723 MB |  472212 MB |  471972 MB |\n",
            "|       from small pool |     272 KB |       2 MB |     710 MB |     710 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    5096    |    4772    |\n",
            "|       from large pool |      53    |     187    |    2705    |    2652    |\n",
            "|       from small pool |     271    |     312    |    2391    |    2120    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    5096    |    4772    |\n",
            "|       from large pool |      53    |     187    |    2705    |    2652    |\n",
            "|       from small pool |     271    |     312    |    2391    |    2120    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     329    |     272    |\n",
            "|       from large pool |      17    |      58    |     258    |     241    |\n",
            "|       from small pool |      40    |      44    |      71    |      31    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    2490    |    2480    |\n",
            "|       from large pool |       9    |      33    |    1628    |    1619    |\n",
            "|       from small pool |       1    |       6    |     862    |     861    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  61% 90/147 [00:26<00:21,  2.70it/s]2024-10-23 16:02:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 33.81 GiB (GPU 0; 14.75 GiB total capacity; 1.17 GiB already allocated; 12.65 GiB free; 1.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 91           |        cudaMalloc retries: 101       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1197 MB |   14052 MB |  673259 MB |  672061 MB |\n",
            "|       from large pool |    1117 MB |   13966 MB |  672855 MB |  671737 MB |\n",
            "|       from small pool |      79 MB |      85 MB |     404 MB |     324 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1197 MB |   14052 MB |  673259 MB |  672061 MB |\n",
            "|       from large pool |    1117 MB |   13966 MB |  672855 MB |  671737 MB |\n",
            "|       from small pool |      79 MB |      85 MB |     404 MB |     324 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1352 MB |   14292 MB |  356364 MB |  355012 MB |\n",
            "|       from large pool |    1272 MB |   14206 MB |  356220 MB |  354948 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     144 MB |      64 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  158059 KB |   12725 MB |  473047 MB |  472892 MB |\n",
            "|       from large pool |  157922 KB |   12723 MB |  472332 MB |  472178 MB |\n",
            "|       from small pool |     137 KB |       2 MB |     714 MB |     714 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     327    |     499    |    5125    |    4798    |\n",
            "|       from large pool |      56    |     187    |    2718    |    2662    |\n",
            "|       from small pool |     271    |     312    |    2407    |    2136    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     327    |     499    |    5125    |    4798    |\n",
            "|       from large pool |      56    |     187    |    2718    |    2662    |\n",
            "|       from small pool |     271    |     312    |    2407    |    2136    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |     334    |     273    |\n",
            "|       from large pool |      21    |      58    |     262    |     241    |\n",
            "|       from small pool |      40    |      44    |      72    |      32    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    2504    |    2494    |\n",
            "|       from large pool |       9    |      33    |    1637    |    1628    |\n",
            "|       from small pool |       1    |       6    |     867    |     866    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 10.80 GiB (GPU 0; 14.75 GiB total capacity; 11.88 GiB already allocated; 1.95 GiB free; 12.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 92           |        cudaMalloc retries: 102       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12160 MB |   14052 MB |  685421 MB |  673261 MB |\n",
            "|       from large pool |   12080 MB |   13966 MB |  685014 MB |  672934 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     406 MB |     326 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12160 MB |   14052 MB |  685421 MB |  673261 MB |\n",
            "|       from large pool |   12080 MB |   13966 MB |  685014 MB |  672934 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     406 MB |     326 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12312 MB |   14292 MB |  367422 MB |  355110 MB |\n",
            "|       from large pool |   12230 MB |   14206 MB |  367276 MB |  355046 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     146 MB |      64 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  155282 KB |   12725 MB |  473295 MB |  473143 MB |\n",
            "|       from large pool |  153568 KB |   12723 MB |  472574 MB |  472424 MB |\n",
            "|       from small pool |    1714 KB |       2 MB |     720 MB |     718 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    5160    |    4829    |\n",
            "|       from large pool |      58    |     187    |    2734    |    2676    |\n",
            "|       from small pool |     273    |     312    |    2426    |    2153    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    5160    |    4829    |\n",
            "|       from large pool |      58    |     187    |    2734    |    2676    |\n",
            "|       from small pool |     273    |     312    |    2426    |    2153    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      62    |     101    |     336    |     274    |\n",
            "|       from large pool |      21    |      58    |     263    |     242    |\n",
            "|       from small pool |      41    |      44    |      73    |      32    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      13    |      39    |    2520    |    2507    |\n",
            "|       from large pool |      11    |      33    |    1648    |    1637    |\n",
            "|       from small pool |       2    |       6    |     872    |     870    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  63% 92/147 [00:27<00:15,  3.44it/s]2024-10-23 16:02:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 19.70 GiB (GPU 0; 14.75 GiB total capacity; 646.88 MiB already allocated; 13.03 GiB free; 960.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 93           |        cudaMalloc retries: 103       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  662401 KB |   14052 MB |  685887 MB |  685240 MB |\n",
            "|       from large pool |  580821 KB |   13966 MB |  685479 MB |  684911 MB |\n",
            "|       from small pool |   81580 KB |      85 MB |     408 MB |     328 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  662401 KB |   14052 MB |  685887 MB |  685240 MB |\n",
            "|       from large pool |  580821 KB |   13966 MB |  685479 MB |  684911 MB |\n",
            "|       from small pool |   81580 KB |      85 MB |     408 MB |     328 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     960 MB |   14292 MB |  367422 MB |  366462 MB |\n",
            "|       from large pool |     880 MB |   14206 MB |  367276 MB |  366396 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     146 MB |      66 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  320639 KB |   12725 MB |  473768 MB |  473455 MB |\n",
            "|       from large pool |  320299 KB |   12723 MB |  473041 MB |  472728 MB |\n",
            "|       from small pool |     340 KB |       2 MB |     727 MB |     726 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    5187    |    4863    |\n",
            "|       from large pool |      53    |     187    |    2744    |    2691    |\n",
            "|       from small pool |     271    |     312    |    2443    |    2172    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    5187    |    4863    |\n",
            "|       from large pool |      53    |     187    |    2744    |    2691    |\n",
            "|       from small pool |     271    |     312    |    2443    |    2172    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     336    |     279    |\n",
            "|       from large pool |      17    |      58    |     263    |     246    |\n",
            "|       from small pool |      40    |      44    |      73    |      33    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    2535    |    2525    |\n",
            "|       from large pool |       9    |      33    |    1657    |    1648    |\n",
            "|       from small pool |       1    |       6    |     878    |     877    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  63% 93/147 [00:27<00:16,  3.36it/s]2024-10-23 16:02:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 11.17 GiB (GPU 0; 14.75 GiB total capacity; 12.34 GiB already allocated; 1.49 GiB free; 12.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 94           |        cudaMalloc retries: 104       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12632 MB |   14052 MB |  698559 MB |  685927 MB |\n",
            "|       from large pool |   12551 MB |   13966 MB |  698148 MB |  685596 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     411 MB |     331 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12632 MB |   14052 MB |  698559 MB |  685927 MB |\n",
            "|       from large pool |   12551 MB |   13966 MB |  698148 MB |  685596 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     411 MB |     331 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12778 MB |   14292 MB |  379240 MB |  366462 MB |\n",
            "|       from large pool |   12696 MB |   14206 MB |  379092 MB |  366396 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     148 MB |      66 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  149167 KB |   12725 MB |  473858 MB |  473713 MB |\n",
            "|       from large pool |  147552 KB |   12723 MB |  473125 MB |  472981 MB |\n",
            "|       from small pool |    1615 KB |       2 MB |     732 MB |     731 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    5222    |    4891    |\n",
            "|       from large pool |      58    |     187    |    2760    |    2702    |\n",
            "|       from small pool |     273    |     312    |    2462    |    2189    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    5222    |    4891    |\n",
            "|       from large pool |      58    |     187    |    2760    |    2702    |\n",
            "|       from small pool |     273    |     312    |    2462    |    2189    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      63    |     101    |     342    |     279    |\n",
            "|       from large pool |      22    |      58    |     268    |     246    |\n",
            "|       from small pool |      41    |      44    |      74    |      33    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      39    |    2556    |    2541    |\n",
            "|       from large pool |      12    |      33    |    1671    |    1659    |\n",
            "|       from small pool |       3    |       6    |     885    |     882    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 14.23 GiB (GPU 0; 14.75 GiB total capacity; 1.10 GiB already allocated; 12.76 GiB free; 1.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 95           |        cudaMalloc retries: 105       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1125 MB |   14052 MB |  699695 MB |  698570 MB |\n",
            "|       from large pool |    1044 MB |   13966 MB |  699280 MB |  698236 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     414 MB |     334 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1125 MB |   14052 MB |  699695 MB |  698570 MB |\n",
            "|       from large pool |    1044 MB |   13966 MB |  699280 MB |  698236 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     414 MB |     334 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1240 MB |   14292 MB |  379240 MB |  378000 MB |\n",
            "|       from large pool |    1158 MB |   14206 MB |  379092 MB |  377934 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     148 MB |      66 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  117634 KB |   12725 MB |  474070 MB |  473955 MB |\n",
            "|       from large pool |  115946 KB |   12723 MB |  473331 MB |  473217 MB |\n",
            "|       from small pool |    1688 KB |       2 MB |     739 MB |     737 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    5256    |    4926    |\n",
            "|       from large pool |      57    |     187    |    2775    |    2718    |\n",
            "|       from small pool |     273    |     312    |    2481    |    2208    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    5256    |    4926    |\n",
            "|       from large pool |      57    |     187    |    2775    |    2718    |\n",
            "|       from small pool |     273    |     312    |    2481    |    2208    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |     342    |     281    |\n",
            "|       from large pool |      20    |      58    |     268    |     248    |\n",
            "|       from small pool |      41    |      44    |      74    |      33    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      14    |      39    |    2575    |    2561    |\n",
            "|       from large pool |      11    |      33    |    1683    |    1672    |\n",
            "|       from small pool |       3    |       6    |     892    |     889    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  65% 95/147 [00:28<00:15,  3.31it/s]2024-10-23 16:02:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 33.75 GiB (GPU 0; 14.75 GiB total capacity; 743.91 MiB already allocated; 13.04 GiB free; 956.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 96           |        cudaMalloc retries: 106       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  761761 KB |   14052 MB |  700305 MB |  699561 MB |\n",
            "|       from large pool |  680108 KB |   13966 MB |  699888 MB |  699224 MB |\n",
            "|       from small pool |   81652 KB |      85 MB |     416 MB |     336 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  761761 KB |   14052 MB |  700305 MB |  699561 MB |\n",
            "|       from large pool |  680108 KB |   13966 MB |  699888 MB |  699224 MB |\n",
            "|       from small pool |   81652 KB |      85 MB |     416 MB |     336 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     956 MB |   14292 MB |  379240 MB |  378284 MB |\n",
            "|       from large pool |     876 MB |   14206 MB |  379092 MB |  378216 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     148 MB |      68 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  217183 KB |   12725 MB |  474445 MB |  474233 MB |\n",
            "|       from large pool |  216915 KB |   12723 MB |  473703 MB |  473491 MB |\n",
            "|       from small pool |     267 KB |       2 MB |     742 MB |     742 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    5283    |    4959    |\n",
            "|       from large pool |      53    |     187    |    2785    |    2732    |\n",
            "|       from small pool |     271    |     312    |    2498    |    2227    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    5283    |    4959    |\n",
            "|       from large pool |      53    |     187    |    2785    |    2732    |\n",
            "|       from small pool |     271    |     312    |    2498    |    2227    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     342    |     285    |\n",
            "|       from large pool |      17    |      58    |     268    |     251    |\n",
            "|       from small pool |      40    |      44    |      74    |      34    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    2590    |    2580    |\n",
            "|       from large pool |       9    |      33    |    1692    |    1683    |\n",
            "|       from small pool |       1    |       6    |     898    |     897    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.53 GiB (GPU 0; 14.75 GiB total capacity; 12.79 GiB already allocated; 221.06 MiB free; 13.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 97           |        cudaMalloc retries: 107       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   13091 MB |   14052 MB |  722632 MB |  709540 MB |\n",
            "|       from large pool |   13009 MB |   13966 MB |  722209 MB |  709199 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     422 MB |     340 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   13091 MB |   14052 MB |  722632 MB |  709540 MB |\n",
            "|       from large pool |   13009 MB |   13966 MB |  722209 MB |  709199 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     422 MB |     340 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14084 MB |   14292 MB |  392368 MB |  378284 MB |\n",
            "|       from large pool |   14002 MB |   14206 MB |  392218 MB |  378216 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     150 MB |      68 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     992 MB |   12725 MB |  479012 MB |  478020 MB |\n",
            "|       from large pool |     992 MB |   12723 MB |  478258 MB |  477266 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     753 MB |     753 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     379    |     499    |    5400    |    5021    |\n",
            "|       from large pool |      93    |     187    |    2861    |    2768    |\n",
            "|       from small pool |     286    |     312    |    2539    |    2253    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     379    |     499    |    5400    |    5021    |\n",
            "|       from large pool |      93    |     187    |    2861    |    2768    |\n",
            "|       from small pool |     286    |     312    |    2539    |    2253    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      66    |     101    |     351    |     285    |\n",
            "|       from large pool |      25    |      58    |     276    |     251    |\n",
            "|       from small pool |      41    |      44    |      75    |      34    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      39    |    2638    |    2623    |\n",
            "|       from large pool |      13    |      33    |    1728    |    1715    |\n",
            "|       from small pool |       2    |       6    |     910    |     908    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  66% 97/147 [00:28<00:10,  4.59it/s]2024-10-23 16:02:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 17.26 GiB (GPU 0; 14.75 GiB total capacity; 950.10 MiB already allocated; 10.50 GiB free; 3.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 98           |        cudaMalloc retries: 108       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |     950 MB |   14052 MB |  723517 MB |  722567 MB |\n",
            "|       from large pool |     869 MB |   13966 MB |  723092 MB |  722222 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     425 MB |     345 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |     950 MB |   14052 MB |  723517 MB |  722567 MB |\n",
            "|       from large pool |     869 MB |   13966 MB |  723092 MB |  722222 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     425 MB |     345 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3552 MB |   14292 MB |  392368 MB |  388816 MB |\n",
            "|       from large pool |    3470 MB |   14206 MB |  392218 MB |  388748 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     150 MB |      68 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2601 MB |   12725 MB |  483794 MB |  481192 MB |\n",
            "|       from large pool |    2600 MB |   12723 MB |  483033 MB |  480433 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     760 MB |     759 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    5434    |    5104    |\n",
            "|       from large pool |      57    |     187    |    2876    |    2819    |\n",
            "|       from small pool |     273    |     312    |    2558    |    2285    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    5434    |    5104    |\n",
            "|       from large pool |      57    |     187    |    2876    |    2819    |\n",
            "|       from small pool |     273    |     312    |    2558    |    2285    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |     101    |     351    |     292    |\n",
            "|       from large pool |      18    |      58    |     276    |     258    |\n",
            "|       from small pool |      41    |      44    |      75    |      34    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      11    |      39    |    2674    |    2663    |\n",
            "|       from large pool |       9    |      33    |    1750    |    1741    |\n",
            "|       from small pool |       2    |       6    |     924    |     922    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  67% 98/147 [00:29<00:16,  3.01it/s]2024-10-23 16:02:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 41.42 GiB (GPU 0; 14.75 GiB total capacity; 788.20 MiB already allocated; 13.04 GiB free; 956.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 99           |        cudaMalloc retries: 109       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |     788 MB |   14052 MB |  724193 MB |  723404 MB |\n",
            "|       from large pool |     708 MB |   13966 MB |  723765 MB |  723057 MB |\n",
            "|       from small pool |      79 MB |      85 MB |     427 MB |     347 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |     788 MB |   14052 MB |  724193 MB |  723404 MB |\n",
            "|       from large pool |     708 MB |   13966 MB |  723765 MB |  723057 MB |\n",
            "|       from small pool |      79 MB |      85 MB |     427 MB |     347 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |     956 MB |   14292 MB |  392368 MB |  391412 MB |\n",
            "|       from large pool |     876 MB |   14206 MB |  392218 MB |  391342 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     150 MB |      70 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  171827 KB |   12725 MB |  484236 MB |  484069 MB |\n",
            "|       from large pool |  171590 KB |   12723 MB |  483472 MB |  483304 MB |\n",
            "|       from small pool |     236 KB |       2 MB |     764 MB |     764 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    5461    |    5137    |\n",
            "|       from large pool |      53    |     187    |    2886    |    2833    |\n",
            "|       from small pool |     271    |     312    |    2575    |    2304    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    5461    |    5137    |\n",
            "|       from large pool |      53    |     187    |    2886    |    2833    |\n",
            "|       from small pool |     271    |     312    |    2575    |    2304    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     351    |     294    |\n",
            "|       from large pool |      17    |      58    |     276    |     259    |\n",
            "|       from small pool |      40    |      44    |      75    |      35    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    2690    |    2680    |\n",
            "|       from large pool |       9    |      33    |    1760    |    1751    |\n",
            "|       from small pool |       1    |       6    |     930    |     929    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  67% 99/147 [00:29<00:13,  3.50it/s]2024-10-23 16:02:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 27.23 GiB (GPU 0; 14.75 GiB total capacity; 1.08 GiB already allocated; 12.79 GiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 100          |        cudaMalloc retries: 110       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1109 MB |   14052 MB |  725305 MB |  724196 MB |\n",
            "|       from large pool |    1028 MB |   13966 MB |  724875 MB |  723846 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     430 MB |     349 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1109 MB |   14052 MB |  725305 MB |  724196 MB |\n",
            "|       from large pool |    1028 MB |   13966 MB |  724875 MB |  723846 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     430 MB |     349 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1210 MB |   14292 MB |  392622 MB |  391412 MB |\n",
            "|       from large pool |    1128 MB |   14206 MB |  392470 MB |  391342 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     152 MB |      70 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  103180 KB |   12725 MB |  484501 MB |  484400 MB |\n",
            "|       from large pool |  101451 KB |   12723 MB |  483730 MB |  483631 MB |\n",
            "|       from small pool |    1729 KB |       2 MB |     770 MB |     768 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    5495    |    5165    |\n",
            "|       from large pool |      57    |     187    |    2901    |    2844    |\n",
            "|       from small pool |     273    |     312    |    2594    |    2321    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    5495    |    5165    |\n",
            "|       from large pool |      57    |     187    |    2901    |    2844    |\n",
            "|       from small pool |     273    |     312    |    2594    |    2321    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     101    |     355    |     294    |\n",
            "|       from large pool |      20    |      58    |     279    |     259    |\n",
            "|       from small pool |      41    |      44    |      76    |      35    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      10    |      39    |    2704    |    2694    |\n",
            "|       from large pool |       8    |      33    |    1769    |    1761    |\n",
            "|       from small pool |       2    |       6    |     935    |     933    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.59 GiB (GPU 0; 14.75 GiB total capacity; 10.53 GiB already allocated; 2.43 GiB free; 11.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 101          |        cudaMalloc retries: 111       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10782 MB |   14052 MB |  742881 MB |  732099 MB |\n",
            "|       from large pool |   10700 MB |   13966 MB |  742447 MB |  731746 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     434 MB |     352 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10782 MB |   14052 MB |  742881 MB |  732099 MB |\n",
            "|       from large pool |   10700 MB |   13966 MB |  742447 MB |  731746 MB |\n",
            "|       from small pool |      81 MB |      85 MB |     434 MB |     352 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11818 MB |   14292 MB |  403230 MB |  391412 MB |\n",
            "|       from large pool |   11736 MB |   14206 MB |  403078 MB |  391342 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     152 MB |      70 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1035 MB |   12725 MB |  488606 MB |  487570 MB |\n",
            "|       from large pool |    1035 MB |   12723 MB |  487829 MB |  486794 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     776 MB |     775 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     363    |     499    |    5585    |    5222    |\n",
            "|       from large pool |      82    |     187    |    2960    |    2878    |\n",
            "|       from small pool |     281    |     312    |    2625    |    2344    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     363    |     499    |    5585    |    5222    |\n",
            "|       from large pool |      82    |     187    |    2960    |    2878    |\n",
            "|       from small pool |     281    |     312    |    2625    |    2344    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      65    |     101    |     359    |     294    |\n",
            "|       from large pool |      24    |      58    |     283    |     259    |\n",
            "|       from small pool |      41    |      44    |      76    |      35    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      17    |      39    |    2747    |    2730    |\n",
            "|       from large pool |      15    |      33    |    1805    |    1790    |\n",
            "|       from small pool |       2    |       6    |     942    |     940    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 37.05 GiB (GPU 0; 14.75 GiB total capacity; 1.21 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:28 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 102          |        cudaMalloc retries: 112       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1236 MB |   14052 MB |  744177 MB |  742940 MB |\n",
            "|       from large pool |    1156 MB |   13966 MB |  743739 MB |  742583 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     437 MB |     357 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1236 MB |   14052 MB |  744177 MB |  742940 MB |\n",
            "|       from large pool |    1156 MB |   13966 MB |  743739 MB |  742583 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     437 MB |     357 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  403230 MB |  400090 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  403078 MB |  400020 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     152 MB |      70 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1903 MB |   12725 MB |  493397 MB |  491494 MB |\n",
            "|       from large pool |    1901 MB |   12723 MB |  492613 MB |  490711 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     783 MB |     782 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    5619    |    5289    |\n",
            "|       from large pool |      57    |     187    |    2975    |    2918    |\n",
            "|       from small pool |     273    |     312    |    2644    |    2371    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    5619    |    5289    |\n",
            "|       from large pool |      57    |     187    |    2975    |    2918    |\n",
            "|       from small pool |     273    |     312    |    2644    |    2371    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     359    |     305    |\n",
            "|       from large pool |      13    |      58    |     283    |     270    |\n",
            "|       from small pool |      41    |      44    |      76    |      35    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    2769    |    2762    |\n",
            "|       from large pool |       4    |      33    |    1816    |    1812    |\n",
            "|       from small pool |       3    |       6    |     953    |     950    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  69% 102/147 [00:29<00:12,  3.69it/s]2024-10-23 16:02:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.68 GiB (GPU 0; 14.75 GiB total capacity; 6.70 GiB already allocated; 5.22 GiB free; 8.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:28 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 103          |        cudaMalloc retries: 113       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6857 MB |   14052 MB |  751008 MB |  744151 MB |\n",
            "|       from large pool |    6777 MB |   13966 MB |  750568 MB |  743791 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     440 MB |     360 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6857 MB |   14052 MB |  751008 MB |  744151 MB |\n",
            "|       from large pool |    6777 MB |   13966 MB |  750568 MB |  743791 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     440 MB |     360 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    8958 MB |   14292 MB |  409048 MB |  400090 MB |\n",
            "|       from large pool |    8876 MB |   14206 MB |  408896 MB |  400020 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     152 MB |      70 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2100 MB |   12725 MB |  497091 MB |  494990 MB |\n",
            "|       from large pool |    2098 MB |   12723 MB |  496301 MB |  494202 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     789 MB |     788 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    5654    |    5323    |\n",
            "|       from large pool |      58    |     187    |    2991    |    2933    |\n",
            "|       from small pool |     273    |     312    |    2663    |    2390    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    5654    |    5323    |\n",
            "|       from large pool |      58    |     187    |    2991    |    2933    |\n",
            "|       from small pool |     273    |     312    |    2663    |    2390    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     360    |     305    |\n",
            "|       from large pool |      14    |      58    |     284    |     270    |\n",
            "|       from small pool |      41    |      44    |      76    |      35    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    2781    |    2774    |\n",
            "|       from large pool |       5    |      33    |    1821    |    1816    |\n",
            "|       from small pool |       2    |       6    |     960    |     958    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 9.14 GiB (GPU 0; 14.75 GiB total capacity; 10.23 GiB already allocated; 1.76 GiB free; 12.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:28 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 104          |        cudaMalloc retries: 115       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10474 MB |   14052 MB |  761487 MB |  751013 MB |\n",
            "|       from large pool |   10394 MB |   13966 MB |  761044 MB |  750650 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     443 MB |     362 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10474 MB |   14052 MB |  761487 MB |  751013 MB |\n",
            "|       from large pool |   10394 MB |   13966 MB |  761044 MB |  750650 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     443 MB |     362 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12504 MB |   14292 MB |  418412 MB |  405908 MB |\n",
            "|       from large pool |   12422 MB |   14206 MB |  418260 MB |  405838 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     152 MB |      70 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2029 MB |   12725 MB |  500631 MB |  498602 MB |\n",
            "|       from large pool |    2027 MB |   12723 MB |  499835 MB |  497808 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     795 MB |     794 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    5689    |    5358    |\n",
            "|       from large pool |      58    |     187    |    3007    |    2949    |\n",
            "|       from small pool |     273    |     312    |    2682    |    2409    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    5689    |    5358    |\n",
            "|       from large pool |      58    |     187    |    3007    |    2949    |\n",
            "|       from small pool |     273    |     312    |    2682    |    2409    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     361    |     306    |\n",
            "|       from large pool |      14    |      58    |     285    |     271    |\n",
            "|       from small pool |      41    |      44    |      76    |      35    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2790    |    2784    |\n",
            "|       from large pool |       4    |      33    |    1825    |    1821    |\n",
            "|       from small pool |       2    |       6    |     965    |     963    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  71% 104/147 [00:30<00:10,  4.12it/s]2024-10-23 16:02:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 15.02 GiB (GPU 0; 14.75 GiB total capacity; 1.01 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 105          |        cudaMalloc retries: 116       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1037 MB |   14052 MB |  762498 MB |  761460 MB |\n",
            "|       from large pool |     957 MB |   13966 MB |  762052 MB |  761094 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     445 MB |     365 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1037 MB |   14052 MB |  762498 MB |  761460 MB |\n",
            "|       from large pool |     957 MB |   13966 MB |  762052 MB |  761094 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     445 MB |     365 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  418412 MB |  415272 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  418260 MB |  415202 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     152 MB |      70 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2102 MB |   12725 MB |  504210 MB |  502108 MB |\n",
            "|       from large pool |    2100 MB |   12723 MB |  503408 MB |  501308 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     801 MB |     799 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    5723    |    5393    |\n",
            "|       from large pool |      57    |     187    |    3022    |    2965    |\n",
            "|       from small pool |     273    |     312    |    2701    |    2428    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    5723    |    5393    |\n",
            "|       from large pool |      57    |     187    |    3022    |    2965    |\n",
            "|       from small pool |     273    |     312    |    2701    |    2428    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     361    |     307    |\n",
            "|       from large pool |      13    |      58    |     285    |     272    |\n",
            "|       from small pool |      41    |      44    |      76    |      35    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2801    |    2795    |\n",
            "|       from large pool |       4    |      33    |    1829    |    1825    |\n",
            "|       from small pool |       2    |       6    |     972    |     970    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  71% 105/147 [00:30<00:11,  3.54it/s]2024-10-23 16:02:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 21.38 GiB (GPU 0; 14.75 GiB total capacity; 659.98 MiB already allocated; 10.91 GiB free; 3.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 106          |        cudaMalloc retries: 117       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  675823 KB |   14052 MB |  762983 MB |  762323 MB |\n",
            "|       from large pool |  594233 KB |   13966 MB |  762536 MB |  761955 MB |\n",
            "|       from small pool |   81589 KB |      85 MB |     447 MB |     367 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  675823 KB |   14052 MB |  762983 MB |  762323 MB |\n",
            "|       from large pool |  594233 KB |   13966 MB |  762536 MB |  761955 MB |\n",
            "|       from small pool |   81589 KB |      85 MB |     447 MB |     367 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3138 MB |   14292 MB |  418412 MB |  415274 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  418260 MB |  415202 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     152 MB |      72 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2478 MB |   12725 MB |  507600 MB |  505122 MB |\n",
            "|       from large pool |    2477 MB |   12723 MB |  506793 MB |  504315 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     806 MB |     806 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    5750    |    5426    |\n",
            "|       from large pool |      53    |     187    |    3032    |    2979    |\n",
            "|       from small pool |     271    |     312    |    2718    |    2447    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    5750    |    5426    |\n",
            "|       from large pool |      53    |     187    |    3032    |    2979    |\n",
            "|       from small pool |     271    |     312    |    2718    |    2447    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |     101    |     361    |     308    |\n",
            "|       from large pool |      13    |      58    |     285    |     272    |\n",
            "|       from small pool |      40    |      44    |      76    |      36    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2812    |    2806    |\n",
            "|       from large pool |       5    |      33    |    1834    |    1829    |\n",
            "|       from small pool |       1    |       6    |     978    |     977    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 21.75 GiB (GPU 0; 14.75 GiB total capacity; 1.15 GiB already allocated; 10.91 GiB free; 3.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 107          |        cudaMalloc retries: 118       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1178 MB |   14052 MB |  764105 MB |  762926 MB |\n",
            "|       from large pool |    1098 MB |   13966 MB |  763655 MB |  762557 MB |\n",
            "|       from small pool |      79 MB |      85 MB |     449 MB |     369 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1178 MB |   14052 MB |  764105 MB |  762926 MB |\n",
            "|       from large pool |    1098 MB |   13966 MB |  763655 MB |  762557 MB |\n",
            "|       from small pool |      79 MB |      85 MB |     449 MB |     369 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3138 MB |   14292 MB |  418414 MB |  415276 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  418260 MB |  415202 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     154 MB |      74 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1959 MB |   12725 MB |  510713 MB |  508753 MB |\n",
            "|       from large pool |    1959 MB |   12723 MB |  509902 MB |  507943 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     810 MB |     810 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     327    |     499    |    5779    |    5452    |\n",
            "|       from large pool |      56    |     187    |    3045    |    2989    |\n",
            "|       from small pool |     271    |     312    |    2734    |    2463    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     327    |     499    |    5779    |    5452    |\n",
            "|       from large pool |      56    |     187    |    3045    |    2989    |\n",
            "|       from small pool |     271    |     312    |    2734    |    2463    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |     101    |     362    |     309    |\n",
            "|       from large pool |      13    |      58    |     285    |     272    |\n",
            "|       from small pool |      40    |      44    |      77    |      37    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2822    |    2816    |\n",
            "|       from large pool |       5    |      33    |    1839    |    1834    |\n",
            "|       from small pool |       1    |       6    |     983    |     982    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.75 GiB (GPU 0; 14.75 GiB total capacity; 655.13 MiB already allocated; 10.91 GiB free; 3.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 108          |        cudaMalloc retries: 119       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  670857 KB |   14052 MB |  764583 MB |  763928 MB |\n",
            "|       from large pool |  589271 KB |   13966 MB |  764132 MB |  763556 MB |\n",
            "|       from small pool |   81586 KB |      85 MB |     451 MB |     371 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  670857 KB |   14052 MB |  764583 MB |  763928 MB |\n",
            "|       from large pool |  589271 KB |   13966 MB |  764132 MB |  763556 MB |\n",
            "|       from small pool |   81586 KB |      85 MB |     451 MB |     371 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3138 MB |   14292 MB |  418416 MB |  415278 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  418260 MB |  415202 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     156 MB |      76 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2482 MB |   12725 MB |  514228 MB |  511746 MB |\n",
            "|       from large pool |    2482 MB |   12723 MB |  513411 MB |  510929 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     816 MB |     816 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    5806    |    5482    |\n",
            "|       from large pool |      53    |     187    |    3055    |    3002    |\n",
            "|       from small pool |     271    |     312    |    2751    |    2480    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    5806    |    5482    |\n",
            "|       from large pool |      53    |     187    |    3055    |    3002    |\n",
            "|       from small pool |     271    |     312    |    2751    |    2480    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |     101    |     363    |     310    |\n",
            "|       from large pool |      13    |      58    |     285    |     272    |\n",
            "|       from small pool |      40    |      44    |      78    |      38    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2833    |    2827    |\n",
            "|       from large pool |       5    |      33    |    1844    |    1839    |\n",
            "|       from small pool |       1    |       6    |     989    |     988    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 9.37 GiB (GPU 0; 14.75 GiB total capacity; 10.46 GiB already allocated; 1.53 GiB free; 12.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 109          |        cudaMalloc retries: 120       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10712 MB |   14052 MB |  775304 MB |  764592 MB |\n",
            "|       from large pool |   10632 MB |   13966 MB |  774850 MB |  764218 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     454 MB |     373 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10712 MB |   14052 MB |  775304 MB |  764592 MB |\n",
            "|       from large pool |   10632 MB |   13966 MB |  774850 MB |  764218 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     454 MB |     373 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12734 MB |   14292 MB |  428012 MB |  415278 MB |\n",
            "|       from large pool |   12652 MB |   14206 MB |  427854 MB |  415202 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     158 MB |      76 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2021 MB |   12725 MB |  517412 MB |  515391 MB |\n",
            "|       from large pool |    2019 MB |   12723 MB |  516590 MB |  514570 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     822 MB |     820 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    5841    |    5510    |\n",
            "|       from large pool |      58    |     187    |    3071    |    3013    |\n",
            "|       from small pool |     273    |     312    |    2770    |    2497    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    5841    |    5510    |\n",
            "|       from large pool |      58    |     187    |    3071    |    3013    |\n",
            "|       from small pool |     273    |     312    |    2770    |    2497    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     365    |     310    |\n",
            "|       from large pool |      14    |      58    |     286    |     272    |\n",
            "|       from small pool |      41    |      44    |      79    |      38    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    2846    |    2838    |\n",
            "|       from large pool |       5    |      33    |    1850    |    1845    |\n",
            "|       from small pool |       3    |       6    |     996    |     993    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  74% 109/147 [00:30<00:05,  6.50it/s]2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 16.57 GiB (GPU 0; 14.75 GiB total capacity; 1.16 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 110          |        cudaMalloc retries: 121       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1187 MB |   14052 MB |  776530 MB |  775342 MB |\n",
            "|       from large pool |    1107 MB |   13966 MB |  776073 MB |  774966 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     457 MB |     376 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1187 MB |   14052 MB |  776530 MB |  775342 MB |\n",
            "|       from large pool |    1107 MB |   13966 MB |  776073 MB |  774966 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     457 MB |     376 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  428012 MB |  424872 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  427854 MB |  424796 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     158 MB |      76 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1952 MB |   12725 MB |  521049 MB |  519097 MB |\n",
            "|       from large pool |    1950 MB |   12723 MB |  520220 MB |  518270 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     828 MB |     827 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    5875    |    5545    |\n",
            "|       from large pool |      57    |     187    |    3086    |    3029    |\n",
            "|       from small pool |     273    |     312    |    2789    |    2516    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    5875    |    5545    |\n",
            "|       from large pool |      57    |     187    |    3086    |    3029    |\n",
            "|       from small pool |     273    |     312    |    2789    |    2516    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     365    |     311    |\n",
            "|       from large pool |      13    |      58    |     286    |     273    |\n",
            "|       from small pool |      41    |      44    |      79    |      38    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2858    |    2852    |\n",
            "|       from large pool |       3    |      33    |    1855    |    1852    |\n",
            "|       from small pool |       3    |       6    |    1003    |    1000    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 29.26 GiB (GPU 0; 14.75 GiB total capacity; 1.11 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 111          |        cudaMalloc retries: 122       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1136 MB |   14052 MB |  777682 MB |  776546 MB |\n",
            "|       from large pool |    1055 MB |   13966 MB |  777222 MB |  776166 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     460 MB |     379 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1136 MB |   14052 MB |  777682 MB |  776546 MB |\n",
            "|       from large pool |    1055 MB |   13966 MB |  777222 MB |  776166 MB |\n",
            "|       from small pool |      80 MB |      85 MB |     460 MB |     379 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  428012 MB |  424872 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  427854 MB |  424796 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     158 MB |      76 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2003 MB |   12725 MB |  524729 MB |  522725 MB |\n",
            "|       from large pool |    2002 MB |   12723 MB |  523894 MB |  521891 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     835 MB |     833 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    5909    |    5579    |\n",
            "|       from large pool |      57    |     187    |    3101    |    3044    |\n",
            "|       from small pool |     273    |     312    |    2808    |    2535    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    5909    |    5579    |\n",
            "|       from large pool |      57    |     187    |    3101    |    3044    |\n",
            "|       from small pool |     273    |     312    |    2808    |    2535    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     365    |     311    |\n",
            "|       from large pool |      13    |      58    |     286    |     273    |\n",
            "|       from small pool |      41    |      44    |      79    |      38    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2868    |    2862    |\n",
            "|       from large pool |       4    |      33    |    1860    |    1856    |\n",
            "|       from small pool |       2    |       6    |    1008    |    1006    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  76% 111/147 [00:31<00:06,  5.57it/s]2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 9.73 GiB (GPU 0; 14.75 GiB total capacity; 10.84 GiB already allocated; 1.17 GiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 112          |        cudaMalloc retries: 123       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11097 MB |   14052 MB |     770 GB |  777697 MB |\n",
            "|       from large pool |   11016 MB |   13966 MB |     769 GB |  777314 MB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |     382 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11097 MB |   14052 MB |     770 GB |  777697 MB |\n",
            "|       from large pool |   11016 MB |   13966 MB |     769 GB |  777314 MB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |     382 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13102 MB |   14292 MB |  437974 MB |  424872 MB |\n",
            "|       from large pool |   13020 MB |   14206 MB |  437816 MB |  424796 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     158 MB |      76 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2004 MB |   12725 MB |  528362 MB |  526357 MB |\n",
            "|       from large pool |    2003 MB |   12723 MB |  527521 MB |  525518 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     841 MB |     839 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    5944    |    5613    |\n",
            "|       from large pool |      58    |     187    |    3117    |    3059    |\n",
            "|       from small pool |     273    |     312    |    2827    |    2554    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    5944    |    5613    |\n",
            "|       from large pool |      58    |     187    |    3117    |    3059    |\n",
            "|       from small pool |     273    |     312    |    2827    |    2554    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     366    |     311    |\n",
            "|       from large pool |      14    |      58    |     287    |     273    |\n",
            "|       from small pool |      41    |      44    |      79    |      38    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    2879    |    2872    |\n",
            "|       from large pool |       4    |      33    |    1864    |    1860    |\n",
            "|       from small pool |       3    |       6    |    1015    |    1012    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 15.87 GiB (GPU 0; 14.75 GiB total capacity; 1.03 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 113          |        cudaMalloc retries: 124       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1057 MB |   14052 MB |     771 GB |     770 GB |\n",
            "|       from large pool |     977 MB |   13966 MB |     770 GB |     769 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1057 MB |   14052 MB |     771 GB |     770 GB |\n",
            "|       from large pool |     977 MB |   13966 MB |     770 GB |     769 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  437974 MB |  434834 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  437816 MB |  434758 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     158 MB |      76 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2082 MB |   12725 MB |  531969 MB |  529886 MB |\n",
            "|       from large pool |    2080 MB |   12723 MB |  531122 MB |  529041 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     846 MB |     845 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    5978    |    5648    |\n",
            "|       from large pool |      57    |     187    |    3132    |    3075    |\n",
            "|       from small pool |     273    |     312    |    2846    |    2573    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    5978    |    5648    |\n",
            "|       from large pool |      57    |     187    |    3132    |    3075    |\n",
            "|       from small pool |     273    |     312    |    2846    |    2573    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     366    |     312    |\n",
            "|       from large pool |      13    |      58    |     287    |     274    |\n",
            "|       from small pool |      41    |      44    |      79    |      38    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2890    |    2884    |\n",
            "|       from large pool |       4    |      33    |    1868    |    1864    |\n",
            "|       from small pool |       2    |       6    |    1022    |    1020    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  77% 113/147 [00:31<00:06,  4.96it/s]2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 13.78 GiB (GPU 0; 14.75 GiB total capacity; 1.18 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 114          |        cudaMalloc retries: 125       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1204 MB |   14052 MB |     772 GB |     771 GB |\n",
            "|       from large pool |    1123 MB |   13966 MB |     772 GB |     770 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1204 MB |   14052 MB |     772 GB |     771 GB |\n",
            "|       from large pool |    1123 MB |   13966 MB |     772 GB |     770 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  437974 MB |  434834 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  437816 MB |  434758 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     158 MB |      76 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1935 MB |   12725 MB |  535555 MB |  533619 MB |\n",
            "|       from large pool |    1934 MB |   12723 MB |  534701 MB |  532767 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     853 MB |     851 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6012    |    5682    |\n",
            "|       from large pool |      57    |     187    |    3147    |    3090    |\n",
            "|       from small pool |     273    |     312    |    2865    |    2592    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6012    |    5682    |\n",
            "|       from large pool |      57    |     187    |    3147    |    3090    |\n",
            "|       from small pool |     273    |     312    |    2865    |    2592    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     366    |     312    |\n",
            "|       from large pool |      13    |      58    |     287    |     274    |\n",
            "|       from small pool |      41    |      44    |      79    |      38    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    2902    |    2895    |\n",
            "|       from large pool |       4    |      33    |    1873    |    1869    |\n",
            "|       from small pool |       3    |       6    |    1029    |    1026    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 21.31 GiB (GPU 0; 14.75 GiB total capacity; 659.49 MiB already allocated; 10.91 GiB free; 3.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 115          |        cudaMalloc retries: 126       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  675313 KB |   14052 MB |     773 GB |     772 GB |\n",
            "|       from large pool |  593721 KB |   13966 MB |     772 GB |     771 GB |\n",
            "|       from small pool |   81592 KB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  675313 KB |   14052 MB |     773 GB |     772 GB |\n",
            "|       from large pool |  593721 KB |   13966 MB |     772 GB |     771 GB |\n",
            "|       from small pool |   81592 KB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3138 MB |   14292 MB |  437974 MB |  434836 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  437816 MB |  434758 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     158 MB |      78 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2478 MB |   12725 MB |  539093 MB |  536614 MB |\n",
            "|       from large pool |    2478 MB |   12723 MB |  538234 MB |  535756 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     858 MB |     858 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    6039    |    5715    |\n",
            "|       from large pool |      53    |     187    |    3157    |    3104    |\n",
            "|       from small pool |     271    |     312    |    2882    |    2611    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    6039    |    5715    |\n",
            "|       from large pool |      53    |     187    |    3157    |    3104    |\n",
            "|       from small pool |     271    |     312    |    2882    |    2611    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |     101    |     366    |     313    |\n",
            "|       from large pool |      13    |      58    |     287    |     274    |\n",
            "|       from small pool |      40    |      44    |      79    |      39    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2913    |    2907    |\n",
            "|       from large pool |       5    |      33    |    1878    |    1873    |\n",
            "|       from small pool |       1    |       6    |    1035    |    1034    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 18.48 GiB (GPU 0; 14.75 GiB total capacity; 1.09 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 116          |        cudaMalloc retries: 127       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1114 MB |   14052 MB |     774 GB |     773 GB |\n",
            "|       from large pool |    1034 MB |   13966 MB |     773 GB |     772 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1114 MB |   14052 MB |     774 GB |     773 GB |\n",
            "|       from large pool |    1034 MB |   13966 MB |     773 GB |     772 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  437976 MB |  434836 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  437816 MB |  434758 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     160 MB |      78 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2025 MB |   12725 MB |  542277 MB |  540252 MB |\n",
            "|       from large pool |    2023 MB |   12723 MB |  541413 MB |  539389 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     864 MB |     862 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6073    |    5743    |\n",
            "|       from large pool |      57    |     187    |    3172    |    3115    |\n",
            "|       from small pool |     273    |     312    |    2901    |    2628    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6073    |    5743    |\n",
            "|       from large pool |      57    |     187    |    3172    |    3115    |\n",
            "|       from small pool |     273    |     312    |    2901    |    2628    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     367    |     313    |\n",
            "|       from large pool |      13    |      58    |     287    |     274    |\n",
            "|       from small pool |      41    |      44    |      80    |      39    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2923    |    2917    |\n",
            "|       from large pool |       4    |      33    |    1883    |    1879    |\n",
            "|       from small pool |       2    |       6    |    1040    |    1038    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 25.47 GiB (GPU 0; 14.75 GiB total capacity; 689.90 MiB already allocated; 10.91 GiB free; 3.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 117          |        cudaMalloc retries: 128       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  706457 KB |   14052 MB |     774 GB |     773 GB |\n",
            "|       from large pool |  624846 KB |   13966 MB |     774 GB |     773 GB |\n",
            "|       from small pool |   81611 KB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  706457 KB |   14052 MB |     774 GB |     773 GB |\n",
            "|       from large pool |  624846 KB |   13966 MB |     774 GB |     773 GB |\n",
            "|       from small pool |   81611 KB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3138 MB |   14292 MB |  437976 MB |  434838 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  437816 MB |  434758 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     160 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2448 MB |   12725 MB |  545745 MB |  543297 MB |\n",
            "|       from large pool |    2447 MB |   12723 MB |  544876 MB |  542428 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     869 MB |     868 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    6100    |    5776    |\n",
            "|       from large pool |      53    |     187    |    3182    |    3129    |\n",
            "|       from small pool |     271    |     312    |    2918    |    2647    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    6100    |    5776    |\n",
            "|       from large pool |      53    |     187    |    3182    |    3129    |\n",
            "|       from small pool |     271    |     312    |    2918    |    2647    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |     101    |     367    |     314    |\n",
            "|       from large pool |      13    |      58    |     287    |     274    |\n",
            "|       from small pool |      40    |      44    |      80    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2934    |    2928    |\n",
            "|       from large pool |       5    |      33    |    1888    |    1883    |\n",
            "|       from small pool |       1    |       6    |    1046    |    1045    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  80% 117/147 [00:32<00:03,  8.03it/s]2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.90 GiB (GPU 0; 14.75 GiB total capacity; 1.06 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 118          |        cudaMalloc retries: 129       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1087 MB |   14052 MB |     775 GB |     774 GB |\n",
            "|       from large pool |    1006 MB |   13966 MB |     775 GB |     774 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1087 MB |   14052 MB |     775 GB |     774 GB |\n",
            "|       from large pool |    1006 MB |   13966 MB |     775 GB |     774 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  437978 MB |  434838 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  437816 MB |  434758 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2052 MB |   12725 MB |  548946 MB |  546893 MB |\n",
            "|       from large pool |    2051 MB |   12723 MB |  548071 MB |  546020 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     874 MB |     873 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6134    |    5804    |\n",
            "|       from large pool |      57    |     187    |    3197    |    3140    |\n",
            "|       from small pool |     273    |     312    |    2937    |    2664    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6134    |    5804    |\n",
            "|       from large pool |      57    |     187    |    3197    |    3140    |\n",
            "|       from small pool |     273    |     312    |    2937    |    2664    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     368    |     314    |\n",
            "|       from large pool |      13    |      58    |     287    |     274    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2944    |    2938    |\n",
            "|       from large pool |       4    |      33    |    1893    |    1889    |\n",
            "|       from small pool |       2    |       6    |    1051    |    1049    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 9.23 GiB (GPU 0; 14.75 GiB total capacity; 10.38 GiB already allocated; 1.67 GiB free; 12.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 119          |        cudaMalloc retries: 130       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10626 MB |   14052 MB |     786 GB |     775 GB |\n",
            "|       from large pool |   10546 MB |   13966 MB |     785 GB |     775 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10626 MB |   14052 MB |     786 GB |     775 GB |\n",
            "|       from large pool |   10546 MB |   13966 MB |     785 GB |     775 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12590 MB |   14292 MB |  447428 MB |  434838 MB |\n",
            "|       from large pool |   12508 MB |   14206 MB |  447266 MB |  434758 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1963 MB |   12725 MB |  552549 MB |  550586 MB |\n",
            "|       from large pool |    1961 MB |   12723 MB |  551668 MB |  549707 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     880 MB |     879 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    6169    |    5838    |\n",
            "|       from large pool |      58    |     187    |    3213    |    3155    |\n",
            "|       from small pool |     273    |     312    |    2956    |    2683    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    6169    |    5838    |\n",
            "|       from large pool |      58    |     187    |    3213    |    3155    |\n",
            "|       from small pool |     273    |     312    |    2956    |    2683    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     369    |     314    |\n",
            "|       from large pool |      14    |      58    |     288    |     274    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    2955    |    2948    |\n",
            "|       from large pool |       4    |      33    |    1897    |    1893    |\n",
            "|       from small pool |       3    |       6    |    1058    |    1055    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.04 GiB (GPU 0; 14.75 GiB total capacity; 1.04 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 120          |        cudaMalloc retries: 131       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1061 MB |   14052 MB |     787 GB |     786 GB |\n",
            "|       from large pool |     981 MB |   13966 MB |     786 GB |     785 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1061 MB |   14052 MB |     787 GB |     786 GB |\n",
            "|       from large pool |     981 MB |   13966 MB |     786 GB |     785 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  447428 MB |  444288 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  447266 MB |  444208 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2078 MB |   12725 MB |  556195 MB |  554117 MB |\n",
            "|       from large pool |    2076 MB |   12723 MB |  555308 MB |  553231 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     887 MB |     885 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6203    |    5873    |\n",
            "|       from large pool |      57    |     187    |    3228    |    3171    |\n",
            "|       from small pool |     273    |     312    |    2975    |    2702    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6203    |    5873    |\n",
            "|       from large pool |      57    |     187    |    3228    |    3171    |\n",
            "|       from small pool |     273    |     312    |    2975    |    2702    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     369    |     315    |\n",
            "|       from large pool |      13    |      58    |     288    |     275    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2964    |    2958    |\n",
            "|       from large pool |       4    |      33    |    1901    |    1897    |\n",
            "|       from small pool |       2    |       6    |    1063    |    1061    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  82% 120/147 [00:32<00:03,  7.05it/s]2024-10-23 16:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 21.08 GiB (GPU 0; 14.75 GiB total capacity; 1.14 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 121          |        cudaMalloc retries: 132       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1167 MB |   14052 MB |     788 GB |     787 GB |\n",
            "|       from large pool |    1087 MB |   13966 MB |     787 GB |     786 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1167 MB |   14052 MB |     788 GB |     787 GB |\n",
            "|       from large pool |    1087 MB |   13966 MB |     787 GB |     786 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  447428 MB |  444288 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  447266 MB |  444208 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1972 MB |   12725 MB |  559772 MB |  557800 MB |\n",
            "|       from large pool |    1970 MB |   12723 MB |  558879 MB |  556909 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     893 MB |     891 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6237    |    5907    |\n",
            "|       from large pool |      57    |     187    |    3243    |    3186    |\n",
            "|       from small pool |     273    |     312    |    2994    |    2721    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6237    |    5907    |\n",
            "|       from large pool |      57    |     187    |    3243    |    3186    |\n",
            "|       from small pool |     273    |     312    |    2994    |    2721    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     369    |     315    |\n",
            "|       from large pool |      13    |      58    |     288    |     275    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    2975    |    2968    |\n",
            "|       from large pool |       4    |      33    |    1905    |    1901    |\n",
            "|       from small pool |       3    |       6    |    1070    |    1067    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 12.35 GiB (GPU 0; 14.75 GiB total capacity; 1.05 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 122          |        cudaMalloc retries: 133       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1070 MB |   14052 MB |     789 GB |     788 GB |\n",
            "|       from large pool |     990 MB |   13966 MB |     788 GB |     787 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1070 MB |   14052 MB |     789 GB |     788 GB |\n",
            "|       from large pool |     990 MB |   13966 MB |     788 GB |     787 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  447428 MB |  444288 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  447266 MB |  444208 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2069 MB |   12725 MB |  563413 MB |  561344 MB |\n",
            "|       from large pool |    2067 MB |   12723 MB |  562514 MB |  560446 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     899 MB |     897 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6271    |    5941    |\n",
            "|       from large pool |      57    |     187    |    3258    |    3201    |\n",
            "|       from small pool |     273    |     312    |    3013    |    2740    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6271    |    5941    |\n",
            "|       from large pool |      57    |     187    |    3258    |    3201    |\n",
            "|       from small pool |     273    |     312    |    3013    |    2740    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     369    |     315    |\n",
            "|       from large pool |      13    |      58    |     288    |     275    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    2984    |    2978    |\n",
            "|       from large pool |       4    |      33    |    1909    |    1905    |\n",
            "|       from small pool |       2    |       6    |    1075    |    1073    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 14.50 GiB (GPU 0; 14.75 GiB total capacity; 1.20 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 123          |        cudaMalloc retries: 134       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1226 MB |   14052 MB |     790 GB |     789 GB |\n",
            "|       from large pool |    1146 MB |   13966 MB |     790 GB |     788 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1226 MB |   14052 MB |     790 GB |     789 GB |\n",
            "|       from large pool |    1146 MB |   13966 MB |     790 GB |     788 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  447428 MB |  444288 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  447266 MB |  444208 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1913 MB |   12725 MB |  567018 MB |  565105 MB |\n",
            "|       from large pool |    1911 MB |   12723 MB |  566113 MB |  564201 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     905 MB |     903 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6305    |    5975    |\n",
            "|       from large pool |      57    |     187    |    3273    |    3216    |\n",
            "|       from small pool |     273    |     312    |    3032    |    2759    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6305    |    5975    |\n",
            "|       from large pool |      57    |     187    |    3273    |    3216    |\n",
            "|       from small pool |     273    |     312    |    3032    |    2759    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     369    |     315    |\n",
            "|       from large pool |      13    |      58    |     288    |     275    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    2996    |    2989    |\n",
            "|       from large pool |       4    |      33    |    1914    |    1910    |\n",
            "|       from small pool |       3    |       6    |    1082    |    1079    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 11.77 GiB (GPU 0; 14.75 GiB total capacity; 1.19 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 124          |        cudaMalloc retries: 135       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1215 MB |   14052 MB |     791 GB |     790 GB |\n",
            "|       from large pool |    1134 MB |   13966 MB |     791 GB |     790 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1215 MB |   14052 MB |     791 GB |     790 GB |\n",
            "|       from large pool |    1134 MB |   13966 MB |     791 GB |     790 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  447428 MB |  444288 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  447266 MB |  444208 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1924 MB |   12725 MB |  570758 MB |  568833 MB |\n",
            "|       from large pool |    1923 MB |   12723 MB |  569846 MB |  567923 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     911 MB |     910 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6339    |    6009    |\n",
            "|       from large pool |      57    |     187    |    3288    |    3231    |\n",
            "|       from small pool |     273    |     312    |    3051    |    2778    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6339    |    6009    |\n",
            "|       from large pool |      57    |     187    |    3288    |    3231    |\n",
            "|       from small pool |     273    |     312    |    3051    |    2778    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     369    |     315    |\n",
            "|       from large pool |      13    |      58    |     288    |     275    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    3008    |    3001    |\n",
            "|       from large pool |       4    |      33    |    1919    |    1915    |\n",
            "|       from small pool |       3    |       6    |    1089    |    1086    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  84% 124/147 [00:32<00:02, 10.21it/s]2024-10-23 16:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.28 GiB (GPU 0; 14.75 GiB total capacity; 10.28 GiB already allocated; 2.35 GiB free; 11.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 125          |        cudaMalloc retries: 136       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10527 MB |   14052 MB |     807 GB |     796 GB |\n",
            "|       from large pool |   10446 MB |   13966 MB |     806 GB |     796 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10527 MB |   14052 MB |     807 GB |     796 GB |\n",
            "|       from large pool |   10446 MB |   13966 MB |     806 GB |     796 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11896 MB |   14292 MB |  456184 MB |  444288 MB |\n",
            "|       from large pool |   11814 MB |   14206 MB |  456022 MB |  444208 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1368 MB |   12725 MB |  575134 MB |  573766 MB |\n",
            "|       from large pool |    1367 MB |   12723 MB |  574216 MB |  572849 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     917 MB |     916 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    6402    |    6055    |\n",
            "|       from large pool |      70    |     187    |    3326    |    3256    |\n",
            "|       from small pool |     277    |     312    |    3076    |    2799    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    6402    |    6055    |\n",
            "|       from large pool |      70    |     187    |    3326    |    3256    |\n",
            "|       from small pool |     277    |     312    |    3076    |    2799    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      56    |     101    |     371    |     315    |\n",
            "|       from large pool |      15    |      58    |     290    |     275    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    3032    |    3024    |\n",
            "|       from large pool |       6    |      33    |    1936    |    1930    |\n",
            "|       from small pool |       2    |       6    |    1096    |    1094    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 24.51 GiB (GPU 0; 14.75 GiB total capacity; 1.04 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 126          |        cudaMalloc retries: 137       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1068 MB |   14052 MB |     808 GB |     806 GB |\n",
            "|       from large pool |     987 MB |   13966 MB |     807 GB |     806 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1068 MB |   14052 MB |     808 GB |     806 GB |\n",
            "|       from large pool |     987 MB |   13966 MB |     807 GB |     806 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  456184 MB |  453044 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  456022 MB |  452964 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2071 MB |   12725 MB |  579404 MB |  577332 MB |\n",
            "|       from large pool |    2070 MB |   12723 MB |  578480 MB |  576410 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     924 MB |     922 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6436    |    6106    |\n",
            "|       from large pool |      57    |     187    |    3341    |    3284    |\n",
            "|       from small pool |     273    |     312    |    3095    |    2822    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6436    |    6106    |\n",
            "|       from large pool |      57    |     187    |    3341    |    3284    |\n",
            "|       from small pool |     273    |     312    |    3095    |    2822    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     371    |     317    |\n",
            "|       from large pool |      13    |      58    |     290    |     277    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3051    |    3045    |\n",
            "|       from large pool |       4    |      33    |    1946    |    1942    |\n",
            "|       from small pool |       2    |       6    |    1105    |    1103    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.49 GiB (GPU 0; 14.75 GiB total capacity; 6.49 GiB already allocated; 5.41 GiB free; 8.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 127          |        cudaMalloc retries: 138       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6649 MB |   14052 MB |     814 GB |     808 GB |\n",
            "|       from large pool |    6568 MB |   13966 MB |     814 GB |     807 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6649 MB |   14052 MB |     814 GB |     808 GB |\n",
            "|       from large pool |    6568 MB |   13966 MB |     814 GB |     807 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    8762 MB |   14292 MB |  461806 MB |  453044 MB |\n",
            "|       from large pool |    8680 MB |   14206 MB |  461644 MB |  452964 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2112 MB |   12725 MB |  582944 MB |  580831 MB |\n",
            "|       from large pool |    2111 MB |   12723 MB |  582014 MB |  579903 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     929 MB |     928 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    6471    |    6140    |\n",
            "|       from large pool |      58    |     187    |    3357    |    3299    |\n",
            "|       from small pool |     273    |     312    |    3114    |    2841    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    6471    |    6140    |\n",
            "|       from large pool |      58    |     187    |    3357    |    3299    |\n",
            "|       from small pool |     273    |     312    |    3114    |    2841    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     372    |     317    |\n",
            "|       from large pool |      14    |      58    |     291    |     277    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    3063    |    3056    |\n",
            "|       from large pool |       5    |      33    |    1951    |    1946    |\n",
            "|       from small pool |       2    |       6    |    1112    |    1110    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  86% 127/147 [00:33<00:02,  7.55it/s]2024-10-23 16:02:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 19.42 GiB (GPU 0; 14.75 GiB total capacity; 987.47 MiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 128          |        cudaMalloc retries: 139       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |     987 MB |   14052 MB |     815 GB |     814 GB |\n",
            "|       from large pool |     907 MB |   13966 MB |     814 GB |     814 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |     987 MB |   14052 MB |     815 GB |     814 GB |\n",
            "|       from large pool |     907 MB |   13966 MB |     814 GB |     814 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  461806 MB |  458666 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  461644 MB |  458586 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2152 MB |   12725 MB |  586433 MB |  584281 MB |\n",
            "|       from large pool |    2150 MB |   12723 MB |  585498 MB |  583347 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     935 MB |     933 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6505    |    6175    |\n",
            "|       from large pool |      57    |     187    |    3372    |    3315    |\n",
            "|       from small pool |     273    |     312    |    3133    |    2860    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6505    |    6175    |\n",
            "|       from large pool |      57    |     187    |    3372    |    3315    |\n",
            "|       from small pool |     273    |     312    |    3133    |    2860    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     372    |     318    |\n",
            "|       from large pool |      13    |      58    |     291    |     278    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3075    |    3069    |\n",
            "|       from large pool |       4    |      33    |    1956    |    1952    |\n",
            "|       from small pool |       2    |       6    |    1119    |    1117    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 9.67 GiB (GPU 0; 14.75 GiB total capacity; 10.77 GiB already allocated; 1.24 GiB free; 12.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 129          |        cudaMalloc retries: 140       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11032 MB |   14052 MB |     826 GB |     815 GB |\n",
            "|       from large pool |   10952 MB |   13966 MB |     825 GB |     815 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11032 MB |   14052 MB |     826 GB |     815 GB |\n",
            "|       from large pool |   10952 MB |   13966 MB |     825 GB |     815 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13040 MB |   14292 MB |  471706 MB |  458666 MB |\n",
            "|       from large pool |   12958 MB |   14206 MB |  471544 MB |  458586 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2007 MB |   12725 MB |  589933 MB |  587926 MB |\n",
            "|       from large pool |    2005 MB |   12723 MB |  588992 MB |  586986 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     941 MB |     939 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    6540    |    6209    |\n",
            "|       from large pool |      58    |     187    |    3388    |    3330    |\n",
            "|       from small pool |     273    |     312    |    3152    |    2879    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    6540    |    6209    |\n",
            "|       from large pool |      58    |     187    |    3388    |    3330    |\n",
            "|       from small pool |     273    |     312    |    3152    |    2879    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     373    |     318    |\n",
            "|       from large pool |      14    |      58    |     292    |     278    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    3086    |    3079    |\n",
            "|       from large pool |       4    |      33    |    1960    |    1956    |\n",
            "|       from small pool |       3    |       6    |    1126    |    1123    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  88% 129/147 [00:33<00:02,  7.12it/s]2024-10-23 16:02:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 26.00 GiB (GPU 0; 14.75 GiB total capacity; 1.06 GiB already allocated; 10.90 GiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 130          |        cudaMalloc retries: 141       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1090 MB |   14052 MB |     827 GB |     826 GB |\n",
            "|       from large pool |    1009 MB |   13966 MB |     826 GB |     825 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1090 MB |   14052 MB |     827 GB |     826 GB |\n",
            "|       from large pool |    1009 MB |   13966 MB |     826 GB |     825 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    3140 MB |   14292 MB |  471706 MB |  468566 MB |\n",
            "|       from large pool |    3058 MB |   14206 MB |  471544 MB |  468486 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     162 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2049 MB |   12725 MB |  593549 MB |  591499 MB |\n",
            "|       from large pool |    2048 MB |   12723 MB |  592601 MB |  590553 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     947 MB |     945 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6574    |    6244    |\n",
            "|       from large pool |      57    |     187    |    3403    |    3346    |\n",
            "|       from small pool |     273    |     312    |    3171    |    2898    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6574    |    6244    |\n",
            "|       from large pool |      57    |     187    |    3403    |    3346    |\n",
            "|       from small pool |     273    |     312    |    3171    |    2898    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     373    |     319    |\n",
            "|       from large pool |      13    |      58    |     292    |     279    |\n",
            "|       from small pool |      41    |      44    |      81    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3095    |    3089    |\n",
            "|       from large pool |       4    |      33    |    1964    |    1960    |\n",
            "|       from small pool |       2    |       6    |    1131    |    1129    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB (GPU 0; 14.75 GiB total capacity; 11.76 GiB already allocated; 1.59 GiB free; 12.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 131          |        cudaMalloc retries: 142       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12043 MB |   14052 MB |     847 GB |     835 GB |\n",
            "|       from large pool |   11960 MB |   13966 MB |     846 GB |     835 GB |\n",
            "|       from small pool |      82 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12043 MB |   14052 MB |     847 GB |     835 GB |\n",
            "|       from large pool |   11960 MB |   13966 MB |     846 GB |     835 GB |\n",
            "|       from small pool |      82 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12672 MB |   14292 MB |  481238 MB |  468566 MB |\n",
            "|       from large pool |   12588 MB |   14206 MB |  481074 MB |  468486 MB |\n",
            "|       from small pool |      84 MB |      88 MB |     164 MB |      80 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  643497 KB |   12725 MB |  603320 MB |  602691 MB |\n",
            "|       from large pool |  642064 KB |   12723 MB |  602364 MB |  601737 MB |\n",
            "|       from small pool |    1433 KB |       2 MB |     955 MB |     953 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     379    |     499    |    6691    |    6312    |\n",
            "|       from large pool |      94    |     187    |    3483    |    3389    |\n",
            "|       from small pool |     285    |     312    |    3208    |    2923    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     379    |     499    |    6691    |    6312    |\n",
            "|       from large pool |      94    |     187    |    3483    |    3389    |\n",
            "|       from small pool |     285    |     312    |    3208    |    2923    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      60    |     101    |     379    |     319    |\n",
            "|       from large pool |      18    |      58    |     297    |     279    |\n",
            "|       from small pool |      42    |      44    |      82    |      40    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    3130    |    3122    |\n",
            "|       from large pool |       5    |      33    |    1991    |    1986    |\n",
            "|       from small pool |       3    |       6    |    1139    |    1136    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  89% 131/147 [00:34<00:02,  5.84it/s]2024-10-23 16:02:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 18.69 GiB (GPU 0; 14.75 GiB total capacity; 975.16 MiB already allocated; 11.63 GiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 132          |        cudaMalloc retries: 143       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |     975 MB |   14052 MB |     848 GB |     847 GB |\n",
            "|       from large pool |     895 MB |   13966 MB |     847 GB |     846 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |     975 MB |   14052 MB |     848 GB |     847 GB |\n",
            "|       from large pool |     895 MB |   13966 MB |     847 GB |     846 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2394 MB |   14292 MB |  481238 MB |  478844 MB |\n",
            "|       from large pool |    2312 MB |   14206 MB |  481074 MB |  478762 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     164 MB |      82 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1418 MB |   12725 MB |  609350 MB |  607931 MB |\n",
            "|       from large pool |    1416 MB |   12723 MB |  608387 MB |  606970 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     963 MB |     961 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6725    |    6395    |\n",
            "|       from large pool |      57    |     187    |    3498    |    3441    |\n",
            "|       from small pool |     273    |     312    |    3227    |    2954    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6725    |    6395    |\n",
            "|       from large pool |      57    |     187    |    3498    |    3441    |\n",
            "|       from small pool |     273    |     312    |    3227    |    2954    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     379    |     325    |\n",
            "|       from large pool |      13    |      58    |     297    |     284    |\n",
            "|       from small pool |      41    |      44    |      82    |      41    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3159    |    3153    |\n",
            "|       from large pool |       4    |      33    |    2007    |    2003    |\n",
            "|       from small pool |       2    |       6    |    1152    |    1150    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.09 GiB (GPU 0; 14.75 GiB total capacity; 12.03 GiB already allocated; 1.46 GiB free; 12.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 133          |        cudaMalloc retries: 144       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12318 MB |   14052 MB |     866 GB |     853 GB |\n",
            "|       from large pool |   12237 MB |   13966 MB |     865 GB |     853 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12318 MB |   14052 MB |     866 GB |     853 GB |\n",
            "|       from large pool |   12237 MB |   13966 MB |     865 GB |     853 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12810 MB |   14292 MB |  491654 MB |  478844 MB |\n",
            "|       from large pool |   12728 MB |   14206 MB |  491490 MB |  478762 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     164 MB |      82 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  503741 KB |   12725 MB |  612851 MB |  612359 MB |\n",
            "|       from large pool |  502489 KB |   12723 MB |  611882 MB |  611391 MB |\n",
            "|       from small pool |    1252 KB |       2 MB |     968 MB |     967 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    6788    |    6441    |\n",
            "|       from large pool |      70    |     187    |    3536    |    3466    |\n",
            "|       from small pool |     277    |     312    |    3252    |    2975    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    6788    |    6441    |\n",
            "|       from large pool |      70    |     187    |    3536    |    3466    |\n",
            "|       from small pool |     277    |     312    |    3252    |    2975    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      56    |     101    |     381    |     325    |\n",
            "|       from large pool |      15    |      58    |     299    |     284    |\n",
            "|       from small pool |      41    |      44    |      82    |      41    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    3183    |    3175    |\n",
            "|       from large pool |       6    |      33    |    2024    |    2018    |\n",
            "|       from small pool |       2    |       6    |    1159    |    1157    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  90% 133/147 [00:35<00:03,  4.29it/s]2024-10-23 16:02:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 34.69 GiB (GPU 0; 14.75 GiB total capacity; 1.18 GiB already allocated; 11.63 GiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 134          |        cudaMalloc retries: 145       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1207 MB |   14052 MB |     867 GB |     866 GB |\n",
            "|       from large pool |    1126 MB |   13966 MB |     866 GB |     865 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1207 MB |   14052 MB |     867 GB |     866 GB |\n",
            "|       from large pool |    1126 MB |   13966 MB |     866 GB |     865 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2394 MB |   14292 MB |  491654 MB |  489260 MB |\n",
            "|       from large pool |    2312 MB |   14206 MB |  491490 MB |  489178 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     164 MB |      82 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1186 MB |   12725 MB |  616545 MB |  615358 MB |\n",
            "|       from large pool |    1185 MB |   12723 MB |  615569 MB |  614384 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     975 MB |     974 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    6822    |    6492    |\n",
            "|       from large pool |      57    |     187    |    3551    |    3494    |\n",
            "|       from small pool |     273    |     312    |    3271    |    2998    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    6822    |    6492    |\n",
            "|       from large pool |      57    |     187    |    3551    |    3494    |\n",
            "|       from small pool |     273    |     312    |    3271    |    2998    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     381    |     327    |\n",
            "|       from large pool |      13    |      58    |     299    |     286    |\n",
            "|       from small pool |      41    |      44    |      82    |      41    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    3203    |    3196    |\n",
            "|       from large pool |       4    |      33    |    2035    |    2031    |\n",
            "|       from small pool |       3    |       6    |    1168    |    1165    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  91% 134/147 [00:35<00:03,  3.31it/s]2024-10-23 16:02:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 19.57 GiB (GPU 0; 14.75 GiB total capacity; 645.85 MiB already allocated; 11.63 GiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 135          |        cudaMalloc retries: 146       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  661350 KB |   14052 MB |     867 GB |     867 GB |\n",
            "|       from large pool |  579771 KB |   13966 MB |     867 GB |     866 GB |\n",
            "|       from small pool |   81579 KB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  661350 KB |   14052 MB |     867 GB |     867 GB |\n",
            "|       from large pool |  579771 KB |   13966 MB |     867 GB |     866 GB |\n",
            "|       from small pool |   81579 KB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2392 MB |   14292 MB |  491654 MB |  489262 MB |\n",
            "|       from large pool |    2312 MB |   14206 MB |  491490 MB |  489178 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     164 MB |      84 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1746 MB |   12725 MB |  619337 MB |  617591 MB |\n",
            "|       from large pool |    1745 MB |   12723 MB |  618354 MB |  616609 MB |\n",
            "|       from small pool |       0 MB |       2 MB |     982 MB |     982 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    6849    |    6525    |\n",
            "|       from large pool |      53    |     187    |    3561    |    3508    |\n",
            "|       from small pool |     271    |     312    |    3288    |    3017    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    6849    |    6525    |\n",
            "|       from large pool |      53    |     187    |    3561    |    3508    |\n",
            "|       from small pool |     271    |     312    |    3288    |    3017    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |     101    |     381    |     328    |\n",
            "|       from large pool |      13    |      58    |     299    |     286    |\n",
            "|       from small pool |      40    |      44    |      82    |      42    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3214    |    3208    |\n",
            "|       from large pool |       5    |      33    |    2040    |    2035    |\n",
            "|       from small pool |       1    |       6    |    1174    |    1173    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 14.75 GiB total capacity; 9.60 GiB already allocated; 3.71 GiB free; 10.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 136          |        cudaMalloc retries: 147       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9828 MB |   14052 MB |     881 GB |     872 GB |\n",
            "|       from large pool |    9746 MB |   13966 MB |     881 GB |     871 GB |\n",
            "|       from small pool |      81 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9828 MB |   14052 MB |     881 GB |     872 GB |\n",
            "|       from large pool |    9746 MB |   13966 MB |     881 GB |     871 GB |\n",
            "|       from small pool |      81 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10506 MB |   14292 MB |  499768 MB |  489262 MB |\n",
            "|       from large pool |   10424 MB |   14206 MB |  499602 MB |  489178 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     166 MB |      84 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  694213 KB |   12725 MB |  622410 MB |  621732 MB |\n",
            "|       from large pool |  693817 KB |   12723 MB |  621420 MB |  620743 MB |\n",
            "|       from small pool |     396 KB |       2 MB |     989 MB |     989 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    6912    |    6565    |\n",
            "|       from large pool |      69    |     187    |    3597    |    3528    |\n",
            "|       from small pool |     278    |     312    |    3315    |    3037    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    6912    |    6565    |\n",
            "|       from large pool |      69    |     187    |    3597    |    3528    |\n",
            "|       from small pool |     278    |     312    |    3315    |    3037    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      56    |     101    |     384    |     328    |\n",
            "|       from large pool |      15    |      58    |     301    |     286    |\n",
            "|       from small pool |      41    |      44    |      83    |      42    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       9    |      39    |    3241    |    3232    |\n",
            "|       from large pool |       7    |      33    |    2057    |    2050    |\n",
            "|       from small pool |       2    |       6    |    1184    |    1182    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 11.25 GiB (GPU 0; 14.75 GiB total capacity; 12.34 GiB already allocated; 395.06 MiB free; 13.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 137          |        cudaMalloc retries: 149       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12635 MB |   14052 MB |     894 GB |     881 GB |\n",
            "|       from large pool |   12554 MB |   13966 MB |     893 GB |     881 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12635 MB |   14052 MB |     894 GB |     881 GB |\n",
            "|       from large pool |   12554 MB |   13966 MB |     893 GB |     881 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13910 MB |   14292 MB |  511284 MB |  497374 MB |\n",
            "|       from large pool |   13828 MB |   14206 MB |  511118 MB |  497290 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     166 MB |      84 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1274 MB |   12725 MB |  625900 MB |  624625 MB |\n",
            "|       from large pool |    1273 MB |   12723 MB |  624903 MB |  623630 MB |\n",
            "|       from small pool |       1 MB |       2 MB |     997 MB |     995 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    6947    |    6616    |\n",
            "|       from large pool |      58    |     187    |    3613    |    3555    |\n",
            "|       from small pool |     273    |     312    |    3334    |    3061    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    6947    |    6616    |\n",
            "|       from large pool |      58    |     187    |    3613    |    3555    |\n",
            "|       from small pool |     273    |     312    |    3334    |    3061    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     385    |     330    |\n",
            "|       from large pool |      14    |      58    |     302    |     288    |\n",
            "|       from small pool |      41    |      44    |      83    |      42    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    3261    |    3253    |\n",
            "|       from large pool |       5    |      33    |    2067    |    2062    |\n",
            "|       from small pool |       3    |       6    |    1194    |    1191    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  93% 137/147 [00:36<00:02,  3.78it/s]2024-10-23 16:02:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 9.51 GiB (GPU 0; 14.75 GiB total capacity; 10.68 GiB already allocated; 395.06 MiB free; 13.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 138          |        cudaMalloc retries: 150       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10931 MB |   14052 MB |     904 GB |     894 GB |\n",
            "|       from large pool |   10851 MB |   13966 MB |     904 GB |     893 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10931 MB |   14052 MB |     904 GB |     894 GB |\n",
            "|       from large pool |   10851 MB |   13966 MB |     904 GB |     893 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13910 MB |   14292 MB |  511284 MB |  497374 MB |\n",
            "|       from large pool |   13828 MB |   14206 MB |  511118 MB |  497290 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     166 MB |      84 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2978 MB |   12725 MB |  630565 MB |  627587 MB |\n",
            "|       from large pool |    2976 MB |   12723 MB |  629562 MB |  626585 MB |\n",
            "|       from small pool |       1 MB |       2 MB |    1003 MB |    1001 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    6982    |    6651    |\n",
            "|       from large pool |      58    |     187    |    3629    |    3571    |\n",
            "|       from small pool |     273    |     312    |    3353    |    3080    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    6982    |    6651    |\n",
            "|       from large pool |      58    |     187    |    3629    |    3571    |\n",
            "|       from small pool |     273    |     312    |    3353    |    3080    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     385    |     330    |\n",
            "|       from large pool |      14    |      58    |     302    |     288    |\n",
            "|       from small pool |      41    |      44    |      83    |      42    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    3274    |    3267    |\n",
            "|       from large pool |       4    |      33    |    2073    |    2069    |\n",
            "|       from small pool |       3    |       6    |    1201    |    1198    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  94% 138/147 [00:36<00:02,  3.77it/s]2024-10-23 16:02:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.89 GiB (GPU 0; 14.75 GiB total capacity; 713.12 MiB already allocated; 11.63 GiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 139          |        cudaMalloc retries: 151       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  730235 KB |   14052 MB |     905 GB |     904 GB |\n",
            "|       from large pool |  648608 KB |   13966 MB |     904 GB |     904 GB |\n",
            "|       from small pool |   81626 KB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  730235 KB |   14052 MB |     905 GB |     904 GB |\n",
            "|       from large pool |  648608 KB |   13966 MB |     904 GB |     904 GB |\n",
            "|       from small pool |   81626 KB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2392 MB |   14292 MB |  511284 MB |  508892 MB |\n",
            "|       from large pool |    2312 MB |   14206 MB |  511118 MB |  508806 MB |\n",
            "|       from small pool |      80 MB |      88 MB |     166 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1678 MB |   12725 MB |  633362 MB |  631683 MB |\n",
            "|       from large pool |    1678 MB |   12723 MB |  632353 MB |  630675 MB |\n",
            "|       from small pool |       0 MB |       2 MB |    1008 MB |    1008 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     324    |     499    |    7009    |    6685    |\n",
            "|       from large pool |      53    |     187    |    3639    |    3586    |\n",
            "|       from small pool |     271    |     312    |    3370    |    3099    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     324    |     499    |    7009    |    6685    |\n",
            "|       from large pool |      53    |     187    |    3639    |    3586    |\n",
            "|       from small pool |     271    |     312    |    3370    |    3099    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |     101    |     385    |     332    |\n",
            "|       from large pool |      13    |      58    |     302    |     289    |\n",
            "|       from small pool |      40    |      44    |      83    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3286    |    3280    |\n",
            "|       from large pool |       5    |      33    |    2079    |    2074    |\n",
            "|       from small pool |       1    |       6    |    1207    |    1206    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  95% 139/147 [00:37<00:02,  3.16it/s]2024-10-23 16:02:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 23.48 GiB (GPU 0; 14.75 GiB total capacity; 1.19 GiB already allocated; 11.63 GiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 140          |        cudaMalloc retries: 152       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1213 MB |   14052 MB |     906 GB |     905 GB |\n",
            "|       from large pool |    1133 MB |   13966 MB |     906 GB |     905 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1213 MB |   14052 MB |     906 GB |     905 GB |\n",
            "|       from large pool |    1133 MB |   13966 MB |     906 GB |     905 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2394 MB |   14292 MB |  511286 MB |  508892 MB |\n",
            "|       from large pool |    2312 MB |   14206 MB |  511118 MB |  508806 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1180 MB |   12725 MB |  635878 MB |  634698 MB |\n",
            "|       from large pool |    1178 MB |   12723 MB |  634863 MB |  633685 MB |\n",
            "|       from small pool |       1 MB |       2 MB |    1014 MB |    1012 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    7043    |    6713    |\n",
            "|       from large pool |      57    |     187    |    3654    |    3597    |\n",
            "|       from small pool |     273    |     312    |    3389    |    3116    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    7043    |    6713    |\n",
            "|       from large pool |      57    |     187    |    3654    |    3597    |\n",
            "|       from small pool |     273    |     312    |    3389    |    3116    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     386    |     332    |\n",
            "|       from large pool |      13    |      58    |     302    |     289    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    3299    |    3292    |\n",
            "|       from large pool |       4    |      33    |    2085    |    2081    |\n",
            "|       from small pool |       3    |       6    |    1214    |    1211    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.95 GiB (GPU 0; 14.75 GiB total capacity; 8.24 GiB already allocated; 2.80 GiB free; 11.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 141          |        cudaMalloc retries: 153       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8437 MB |   14052 MB |     918 GB |     910 GB |\n",
            "|       from large pool |    8355 MB |   13966 MB |     918 GB |     910 GB |\n",
            "|       from small pool |      81 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8437 MB |   14052 MB |     918 GB |     910 GB |\n",
            "|       from large pool |    8355 MB |   13966 MB |     918 GB |     910 GB |\n",
            "|       from small pool |      81 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11442 MB |   14292 MB |  520334 MB |  508892 MB |\n",
            "|       from large pool |   11360 MB |   14206 MB |  520166 MB |  508806 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3004 MB |   12725 MB |  642830 MB |  639825 MB |\n",
            "|       from large pool |    3004 MB |   12723 MB |  641808 MB |  638804 MB |\n",
            "|       from small pool |       0 MB |       2 MB |    1021 MB |    1020 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     347    |     499    |    7106    |    6759    |\n",
            "|       from large pool |      70    |     187    |    3692    |    3622    |\n",
            "|       from small pool |     277    |     312    |    3414    |    3137    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     347    |     499    |    7106    |    6759    |\n",
            "|       from large pool |      70    |     187    |    3692    |    3622    |\n",
            "|       from small pool |     277    |     312    |    3414    |    3137    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      57    |     101    |     389    |     332    |\n",
            "|       from large pool |      16    |      58    |     305    |     289    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3323    |    3317    |\n",
            "|       from large pool |       5    |      33    |    2100    |    2095    |\n",
            "|       from small pool |       1    |       6    |    1223    |    1222    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 20.34 GiB (GPU 0; 14.75 GiB total capacity; 1002.84 MiB already allocated; 11.63 GiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 142          |        cudaMalloc retries: 154       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1002 MB |   14052 MB |     919 GB |     918 GB |\n",
            "|       from large pool |     922 MB |   13966 MB |     919 GB |     918 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1002 MB |   14052 MB |     919 GB |     918 GB |\n",
            "|       from large pool |     922 MB |   13966 MB |     919 GB |     918 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2394 MB |   14292 MB |  520334 MB |  517940 MB |\n",
            "|       from large pool |    2312 MB |   14206 MB |  520166 MB |  517854 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1391 MB |   12725 MB |  646839 MB |  645448 MB |\n",
            "|       from large pool |    1389 MB |   12723 MB |  645811 MB |  644422 MB |\n",
            "|       from small pool |       1 MB |       2 MB |    1028 MB |    1026 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    7140    |    6810    |\n",
            "|       from large pool |      57    |     187    |    3707    |    3650    |\n",
            "|       from small pool |     273    |     312    |    3433    |    3160    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    7140    |    6810    |\n",
            "|       from large pool |      57    |     187    |    3707    |    3650    |\n",
            "|       from small pool |     273    |     312    |    3433    |    3160    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     389    |     335    |\n",
            "|       from large pool |      13    |      58    |     305    |     292    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3343    |    3337    |\n",
            "|       from large pool |       4    |      33    |    2110    |    2106    |\n",
            "|       from small pool |       2    |       6    |    1233    |    1231    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  97% 142/147 [00:37<00:01,  3.79it/s]2024-10-23 16:02:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 16.06 GiB (GPU 0; 14.75 GiB total capacity; 1.15 GiB already allocated; 11.63 GiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 143          |        cudaMalloc retries: 155       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1174 MB |   14052 MB |     921 GB |     919 GB |\n",
            "|       from large pool |    1093 MB |   13966 MB |     920 GB |     919 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1174 MB |   14052 MB |     921 GB |     919 GB |\n",
            "|       from large pool |    1093 MB |   13966 MB |     920 GB |     919 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2394 MB |   14292 MB |  520334 MB |  517940 MB |\n",
            "|       from large pool |    2312 MB |   14206 MB |  520166 MB |  517854 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1219 MB |   12725 MB |  649620 MB |  648401 MB |\n",
            "|       from large pool |    1218 MB |   12723 MB |  648586 MB |  647368 MB |\n",
            "|       from small pool |       1 MB |       2 MB |    1034 MB |    1032 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    7174    |    6844    |\n",
            "|       from large pool |      57    |     187    |    3722    |    3665    |\n",
            "|       from small pool |     273    |     312    |    3452    |    3179    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    7174    |    6844    |\n",
            "|       from large pool |      57    |     187    |    3722    |    3665    |\n",
            "|       from small pool |     273    |     312    |    3452    |    3179    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     389    |     335    |\n",
            "|       from large pool |      13    |      58    |     305    |     292    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       7    |      39    |    3354    |    3347    |\n",
            "|       from large pool |       4    |      33    |    2114    |    2110    |\n",
            "|       from small pool |       3    |       6    |    1240    |    1237    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 7.97 GiB (GPU 0; 14.75 GiB total capacity; 9.12 GiB already allocated; 3.66 GiB free; 10.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 144          |        cudaMalloc retries: 156       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9336 MB |   14052 MB |     930 GB |     921 GB |\n",
            "|       from large pool |    9255 MB |   13966 MB |     929 GB |     920 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9336 MB |   14052 MB |     930 GB |     921 GB |\n",
            "|       from large pool |    9255 MB |   13966 MB |     929 GB |     920 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10560 MB |   14292 MB |  528500 MB |  517940 MB |\n",
            "|       from large pool |   10478 MB |   14206 MB |  528332 MB |  517854 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1223 MB |   12725 MB |  652555 MB |  651331 MB |\n",
            "|       from large pool |    1222 MB |   12723 MB |  651514 MB |  650292 MB |\n",
            "|       from small pool |       1 MB |       2 MB |    1040 MB |    1038 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     331    |     499    |    7209    |    6878    |\n",
            "|       from large pool |      58    |     187    |    3738    |    3680    |\n",
            "|       from small pool |     273    |     312    |    3471    |    3198    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     331    |     499    |    7209    |    6878    |\n",
            "|       from large pool |      58    |     187    |    3738    |    3680    |\n",
            "|       from small pool |     273    |     312    |    3471    |    3198    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     390    |     335    |\n",
            "|       from large pool |      14    |      58    |     306    |     292    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    3366    |    3358    |\n",
            "|       from large pool |       5    |      33    |    2119    |    2114    |\n",
            "|       from small pool |       3    |       6    |    1247    |    1244    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.02 GiB (GPU 0; 14.75 GiB total capacity; 11.23 GiB already allocated; 657.06 MiB free; 13.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 145          |        cudaMalloc retries: 157       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11500 MB |   14052 MB |     948 GB |     937 GB |\n",
            "|       from large pool |   11418 MB |   13966 MB |     947 GB |     936 GB |\n",
            "|       from small pool |      81 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11500 MB |   14052 MB |     948 GB |     937 GB |\n",
            "|       from large pool |   11418 MB |   13966 MB |     947 GB |     936 GB |\n",
            "|       from small pool |      81 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13648 MB |   14292 MB |  531588 MB |  517940 MB |\n",
            "|       from large pool |   13566 MB |   14206 MB |  531420 MB |  517854 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2147 MB |   12725 MB |  667805 MB |  665658 MB |\n",
            "|       from large pool |    2147 MB |   12723 MB |  666754 MB |  664607 MB |\n",
            "|       from small pool |       0 MB |       2 MB |    1050 MB |    1050 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     363    |     499    |    7299    |    6936    |\n",
            "|       from large pool |      81    |     187    |    3794    |    3713    |\n",
            "|       from small pool |     282    |     312    |    3505    |    3223    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     363    |     499    |    7299    |    6936    |\n",
            "|       from large pool |      81    |     187    |    3794    |    3713    |\n",
            "|       from small pool |     282    |     312    |    3505    |    3223    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      56    |     101    |     391    |     335    |\n",
            "|       from large pool |      15    |      58    |     307    |     292    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    3400    |    3392    |\n",
            "|       from large pool |       6    |      33    |    2142    |    2136    |\n",
            "|       from small pool |       2    |       6    |    1258    |    1256    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  99% 145/147 [00:37<00:00,  5.26it/s]2024-10-23 16:02:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 13.48 GiB (GPU 0; 14.75 GiB total capacity; 1.08 GiB already allocated; 11.63 GiB free; 2.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:37 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 146          |        cudaMalloc retries: 158       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    1103 MB |   14052 MB |     949 GB |     948 GB |\n",
            "|       from large pool |    1023 MB |   13966 MB |     949 GB |     948 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    1103 MB |   14052 MB |     949 GB |     948 GB |\n",
            "|       from large pool |    1023 MB |   13966 MB |     949 GB |     948 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2394 MB |   14292 MB |  531588 MB |  529194 MB |\n",
            "|       from large pool |    2312 MB |   14206 MB |  531420 MB |  529108 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1290 MB |   12725 MB |  674903 MB |  673612 MB |\n",
            "|       from large pool |    1288 MB |   12723 MB |  673844 MB |  672556 MB |\n",
            "|       from small pool |       1 MB |       2 MB |    1058 MB |    1056 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     330    |     499    |    7333    |    7003    |\n",
            "|       from large pool |      57    |     187    |    3809    |    3752    |\n",
            "|       from small pool |     273    |     312    |    3524    |    3251    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     330    |     499    |    7333    |    7003    |\n",
            "|       from large pool |      57    |     187    |    3809    |    3752    |\n",
            "|       from small pool |     273    |     312    |    3524    |    3251    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |     101    |     391    |     337    |\n",
            "|       from large pool |      13    |      58    |     307    |     294    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      39    |    3423    |    3417    |\n",
            "|       from large pool |       4    |      33    |    2155    |    2151    |\n",
            "|       from small pool |       2    |       6    |    1268    |    1266    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  99% 146/147 [00:38<00:00,  3.60it/s]2024-10-23 16:02:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 14.75 GiB total capacity; 10.14 GiB already allocated; 1.62 GiB free; 12.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:37 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 147          |        cudaMalloc retries: 159       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10387 MB |   14052 MB |     968 GB |     958 GB |\n",
            "|       from large pool |   10305 MB |   13966 MB |     968 GB |     958 GB |\n",
            "|       from small pool |      81 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10387 MB |   14052 MB |     968 GB |     958 GB |\n",
            "|       from large pool |   10305 MB |   13966 MB |     968 GB |     958 GB |\n",
            "|       from small pool |      81 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12642 MB |   14292 MB |  541836 MB |  529194 MB |\n",
            "|       from large pool |   12560 MB |   14206 MB |  541668 MB |  529108 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2254 MB |   12725 MB |  681587 MB |  679332 MB |\n",
            "|       from large pool |    2254 MB |   12723 MB |  680517 MB |  678263 MB |\n",
            "|       from small pool |       0 MB |       2 MB |    1069 MB |    1069 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     378    |     499    |    7449    |    7071    |\n",
            "|       from large pool |      92    |     187    |    3884    |    3792    |\n",
            "|       from small pool |     286    |     312    |    3565    |    3279    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     378    |     499    |    7449    |    7071    |\n",
            "|       from large pool |      92    |     187    |    3884    |    3792    |\n",
            "|       from small pool |     286    |     312    |    3565    |    3279    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      58    |     101    |     395    |     337    |\n",
            "|       from large pool |      17    |      58    |     311    |     294    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       8    |      39    |    3463    |    3455    |\n",
            "|       from large pool |       7    |      33    |    2183    |    2176    |\n",
            "|       from small pool |       1    |       6    |    1280    |    1279    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2024-10-23 16:02:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-23 16:02:37 | WARNING | fairseq.tasks.fairseq_task | 21 samples have invalid sizes and will be skipped, max_positions=(42105, 573), first few sample ids=[193, 218, 272, 231, 121, 114, 267, 225, 10, 4]\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/40 [00:02<01:19,  2.05s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 2/40 [00:03<01:14,  1.96s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 3/40 [00:05<01:03,  1.72s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  10% 4/40 [00:07<01:08,  1.90s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 5/40 [00:08<00:56,  1.62s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 6/40 [00:09<00:51,  1.50s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 7/40 [00:11<00:53,  1.62s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 8/40 [00:14<00:58,  1.82s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 9/40 [00:16<01:03,  2.06s/it]\u001b[A2024-10-23 16:02:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.94 GiB (GPU 0; 14.75 GiB total capacity; 7.76 GiB already allocated; 4.70 GiB free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:54 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 148          |        cudaMalloc retries: 166       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7941 MB |   14052 MB |    1539 GB |    1532 GB |\n",
            "|       from large pool |    7861 MB |   13966 MB |    1539 GB |    1531 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7941 MB |   14052 MB |    1539 GB |    1532 GB |\n",
            "|       from large pool |    7861 MB |   13966 MB |    1539 GB |    1531 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9496 MB |   14292 MB |  598890 MB |  589394 MB |\n",
            "|       from large pool |    9414 MB |   14206 MB |  598722 MB |  589308 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1554 MB |   12725 MB |     961 GB |     959 GB |\n",
            "|       from large pool |    1552 MB |   12723 MB |     960 GB |     958 GB |\n",
            "|       from small pool |       1 MB |       2 MB |       1 GB |       1 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     332    |     499    |   11469    |   11137    |\n",
            "|       from large pool |      54    |     187    |    6541    |    6487    |\n",
            "|       from small pool |     278    |     312    |    4928    |    4650    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     332    |     499    |   11469    |   11137    |\n",
            "|       from large pool |      54    |     187    |    6541    |    6487    |\n",
            "|       from small pool |     278    |     312    |    4928    |    4650    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     408    |     353    |\n",
            "|       from large pool |      14    |      58    |     324    |     310    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       9    |      39    |    4973    |    4964    |\n",
            "|       from large pool |       5    |      33    |    3222    |    3217    |\n",
            "|       from small pool |       4    |       8    |    1751    |    1747    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-23 16:02:54 | WARNING | fairseq.trainer | ran out of memory in validation step, retrying batch\n",
            "2024-10-23 16:02:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.94 GiB (GPU 0; 14.75 GiB total capacity; 8.25 GiB already allocated; 4.70 GiB free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2024-10-23 16:02:54 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 149          |        cudaMalloc retries: 167       |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8448 MB |   14052 MB |    1540 GB |    1532 GB |\n",
            "|       from large pool |    8368 MB |   13966 MB |    1540 GB |    1532 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8448 MB |   14052 MB |    1540 GB |    1532 GB |\n",
            "|       from large pool |    8368 MB |   13966 MB |    1540 GB |    1532 GB |\n",
            "|       from small pool |      80 MB |      85 MB |       0 GB |       0 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    9496 MB |   14292 MB |  598890 MB |  589394 MB |\n",
            "|       from large pool |    9414 MB |   14206 MB |  598722 MB |  589308 MB |\n",
            "|       from small pool |      82 MB |      88 MB |     168 MB |      86 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1047 MB |   12725 MB |     961 GB |     960 GB |\n",
            "|       from large pool |    1045 MB |   12723 MB |     960 GB |     959 GB |\n",
            "|       from small pool |       1 MB |       2 MB |       1 GB |       1 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     343    |     499    |   11496    |   11153    |\n",
            "|       from large pool |      61    |     187    |    6554    |    6493    |\n",
            "|       from small pool |     282    |     312    |    4942    |    4660    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     343    |     499    |   11496    |   11153    |\n",
            "|       from large pool |      61    |     187    |    6554    |    6493    |\n",
            "|       from small pool |     282    |     312    |    4942    |    4660    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      55    |     101    |     408    |     353    |\n",
            "|       from large pool |      14    |      58    |     324    |     310    |\n",
            "|       from small pool |      41    |      44    |      84    |      43    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       9    |      39    |    4981    |    4972    |\n",
            "|       from large pool |       5    |      33    |    3227    |    3222    |\n",
            "|       from small pool |       4    |       8    |    1754    |    1750    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 1121, in valid_step\n",
            "    _loss, sample_size, logging_output = self.task.valid_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/translation.py\", line 383, in valid_step\n",
            "    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 525, in valid_step\n",
            "    loss, sample_size, logging_output = criterion(model, sample)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/criterions/label_smoothed_cross_entropy.py\", line 79, in forward\n",
            "    net_output = model(**sample[\"net_input\"])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_base.py\", line 144, in forward\n",
            "    encoder_out = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 165, in forward\n",
            "    return self.forward_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 294, in forward_scriptable\n",
            "    lr = layer(x, encoder_padding_mask=encoder_padding_mask_out)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 351, in forward\n",
            "    x, _ = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/multihead_attention.py\", line 538, in forward\n",
            "    return F.multi_head_attention_forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 5179, in multi_head_attention_forward\n",
            "    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 4856, in _scaled_dot_product_attention\n",
            "    attn = softmax(attn, dim=-1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1834, in softmax\n",
            "    ret = input.softmax(dim)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 6.94 GiB (GPU 0; 14.75 GiB total capacity; 7.76 GiB already allocated; 4.70 GiB free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 330, in train\n",
            "    valid_losses, should_stop = validate_and_save(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 421, in validate_and_save\n",
            "    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 505, in validate\n",
            "    trainer.valid_step(sample)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 1136, in valid_step\n",
            "    return self.valid_step(sample, raise_oom=True)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 1137, in valid_step\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 1121, in valid_step\n",
            "    _loss, sample_size, logging_output = self.task.valid_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/translation.py\", line 383, in valid_step\n",
            "    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 525, in valid_step\n",
            "    loss, sample_size, logging_output = criterion(model, sample)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/criterions/label_smoothed_cross_entropy.py\", line 79, in forward\n",
            "    net_output = model(**sample[\"net_input\"])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_base.py\", line 144, in forward\n",
            "    encoder_out = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 165, in forward\n",
            "    return self.forward_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 294, in forward_scriptable\n",
            "    lr = layer(x, encoder_padding_mask=encoder_padding_mask_out)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 351, in forward\n",
            "    x, _ = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/multihead_attention.py\", line 538, in forward\n",
            "    return F.multi_head_attention_forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 5179, in multi_head_attention_forward\n",
            "    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 4852, in _scaled_dot_product_attention\n",
            "    attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 6.94 GiB (GPU 0; 14.75 GiB total capacity; 8.25 GiB already allocated; 4.70 GiB free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-wise model training"
      ],
      "metadata": {
        "id": "Hv3LRzXczTX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train data-bin/wordwise \\\n",
        "    --arch transformer --share-decoder-input-output-embed \\\n",
        "    --encoder-layers 6 --decoder-layers 6 \\\n",
        "    --max-tokens 50000 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.1 --dropout 0.1 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 4 --save-dir checkpoints/wordwise_transformer\\\n",
        "    --max-source-positions 42105 --max-target-positions 573 \\\n",
        "    --skip-invalid-size-inputs-valid-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VFxEf3juzWck",
        "outputId": "0190bc70-e8a1-438f-b60c-2556531d7975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-21 16:01:08.200901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-21 16:01:08.233995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-21 16:01:08.253995: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-21 16:01:08.278674: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-21 16:01:09.893336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-10-21 16:01:11 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-10-21 16:01:13 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-10-21 16:01:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 50000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 50000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 4, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/wordwise_transformer', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=50000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=50000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=4, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/wordwise_transformer', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/wordwise', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, encoder_layers=6, decoder_layers=6, dropout=0.1, max_source_positions=42105, max_target_positions=573, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/wordwise', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 42105, 'max_target_positions': 573, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-10-21 16:01:14 | INFO | fairseq.tasks.translation | [src] dictionary: 63256 types\n",
            "2024-10-21 16:01:14 | INFO | fairseq.tasks.translation | [tgt] dictionary: 1544 types\n",
            "2024-10-21 16:01:15 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(63256, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(1544, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=1544, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-10-21 16:01:15 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-10-21 16:01:15 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-10-21 16:01:15 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-10-21 16:01:16 | INFO | fairseq_cli.train | num. shared model params: 96,218,112 (num. trained: 96,218,112)\n",
            "2024-10-21 16:01:16 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-10-21 16:01:16 | INFO | fairseq.data.data_utils | loaded 275 examples from: data-bin/wordwise/valid.src-tgt.src\n",
            "2024-10-21 16:01:16 | INFO | fairseq.data.data_utils | loaded 275 examples from: data-bin/wordwise/valid.src-tgt.tgt\n",
            "2024-10-21 16:01:16 | INFO | fairseq.tasks.translation | data-bin/wordwise valid src-tgt 275 examples\n",
            "2024-10-21 16:01:16 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-10-21 16:01:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-10-21 16:01:16 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-10-21 16:01:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-10-21 16:01:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-10-21 16:01:16 | INFO | fairseq_cli.train | max tokens per device = 50000 and max sentences per device = None\n",
            "2024-10-21 16:01:16 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/wordwise_transformer/checkpoint_last.pt\n",
            "2024-10-21 16:01:16 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/wordwise_transformer/checkpoint_last.pt\n",
            "2024-10-21 16:01:16 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-10-21 16:01:16 | INFO | fairseq.data.data_utils | loaded 1,100 examples from: data-bin/wordwise/train.src-tgt.src\n",
            "2024-10-21 16:01:16 | INFO | fairseq.data.data_utils | loaded 1,100 examples from: data-bin/wordwise/train.src-tgt.tgt\n",
            "2024-10-21 16:01:16 | INFO | fairseq.tasks.translation | data-bin/wordwise train src-tgt 1100 examples\n",
            "2024-10-21 16:01:16 | WARNING | fairseq.tasks.fairseq_task | 2 samples have invalid sizes and will be skipped, max_positions=(42105, 573), first few sample ids=[55, 262]\n",
            "2024-10-21 16:01:16 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2024-10-21 16:01:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 55\n",
            "epoch 001:   0% 0/55 [00:00<?, ?it/s]2024-10-21 16:01:16 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-10-21 16:01:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:   9% 5/55 [00:56<10:32, 12.64s/it]2024-10-21 16:02:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 390.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 283.06 MiB is free. Process 359607 has 14.47 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 1.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:02:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12950 MiB |  12951 MiB | 208966 MiB | 196016 MiB |\n",
            "|       from large pool |  12711 MiB |  12711 MiB | 206929 MiB | 194217 MiB |\n",
            "|       from small pool |    239 MiB |    296 MiB |   2037 MiB |   1798 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12950 MiB |  12951 MiB | 208966 MiB | 196016 MiB |\n",
            "|       from large pool |  12711 MiB |  12711 MiB | 206929 MiB | 194217 MiB |\n",
            "|       from small pool |    239 MiB |    296 MiB |   2037 MiB |   1798 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12939 MiB |  12940 MiB | 208615 MiB | 195675 MiB |\n",
            "|       from large pool |  12700 MiB |  12700 MiB | 206579 MiB | 193878 MiB |\n",
            "|       from small pool |    239 MiB |    296 MiB |   2036 MiB |   1797 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14682 MiB |  14754 MiB |  25918 MiB |  11236 MiB |\n",
            "|       from large pool |  14440 MiB |  14440 MiB |  25604 MiB |  11164 MiB |\n",
            "|       from small pool |    242 MiB |    314 MiB |    314 MiB |     72 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1731 MiB |   3659 MiB | 140553 MiB | 138822 MiB |\n",
            "|       from large pool |   1728 MiB |   3647 MiB | 138555 MiB | 136826 MiB |\n",
            "|       from small pool |      2 MiB |     15 MiB |   1998 MiB |   1995 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1043    |    1078    |   12077    |   11034    |\n",
            "|       from large pool |     247    |     247    |    3598    |    3351    |\n",
            "|       from small pool |     796    |     952    |    8479    |    7683    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1043    |    1078    |   12077    |   11034    |\n",
            "|       from large pool |     247    |     247    |    3598    |    3351    |\n",
            "|       from small pool |     796    |     952    |    8479    |    7683    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     197    |     233    |     295    |      98    |\n",
            "|       from large pool |      76    |      76    |     138    |      62    |\n",
            "|       from small pool |     121    |     157    |     157    |      36    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      93    |      93    |    4865    |    4772    |\n",
            "|       from large pool |      69    |      70    |    1582    |    1513    |\n",
            "|       from small pool |      24    |      38    |    3283    |    3259    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:02:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  13% 7/55 [01:01<05:53,  7.36s/it]2024-10-21 16:02:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 303.06 MiB is free. Process 359607 has 14.45 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:02:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 2            |        cudaMalloc retries: 3         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12502 MiB |  12503 MiB | 267334 MiB | 254831 MiB |\n",
            "|       from large pool |  12272 MiB |  12272 MiB | 264991 MiB | 252719 MiB |\n",
            "|       from small pool |    230 MiB |    296 MiB |   2342 MiB |   2112 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12502 MiB |  12503 MiB | 267334 MiB | 254831 MiB |\n",
            "|       from large pool |  12272 MiB |  12272 MiB | 264991 MiB | 252719 MiB |\n",
            "|       from small pool |    230 MiB |    296 MiB |   2342 MiB |   2112 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12487 MiB |  12488 MiB | 266941 MiB | 254454 MiB |\n",
            "|       from large pool |  12256 MiB |  12256 MiB | 264600 MiB | 252343 MiB |\n",
            "|       from small pool |    230 MiB |    296 MiB |   2340 MiB |   2110 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14662 MiB |  14742 MiB |  27272 MiB |  12610 MiB |\n",
            "|       from large pool |  14430 MiB |  14440 MiB |  26896 MiB |  12466 MiB |\n",
            "|       from small pool |    232 MiB |    302 MiB |    376 MiB |    144 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2159 MiB |   2755 MiB | 190592 MiB | 188432 MiB |\n",
            "|       from large pool |   2157 MiB |   2752 MiB | 188277 MiB | 186119 MiB |\n",
            "|       from small pool |      1 MiB |      8 MiB |   2314 MiB |   2313 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1007    |    1078    |   14626    |   13619    |\n",
            "|       from large pool |     231    |     231    |    4691    |    4460    |\n",
            "|       from small pool |     776    |     952    |    9935    |    9159    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1007    |    1078    |   14626    |   13619    |\n",
            "|       from large pool |     231    |     231    |    4691    |    4460    |\n",
            "|       from small pool |     776    |     952    |    9935    |    9159    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     177    |     227    |     332    |     155    |\n",
            "|       from large pool |      61    |      76    |     144    |      83    |\n",
            "|       from small pool |     116    |     151    |     188    |      72    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      77    |    5963    |    5887    |\n",
            "|       from large pool |      52    |      52    |    2133    |    2081    |\n",
            "|       from small pool |      24    |      29    |    3830    |    3806    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:02:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  20% 11/55 [01:27<04:32,  6.20s/it]2024-10-21 16:02:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 179.06 MiB is free. Process 359607 has 14.57 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 457.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:02:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 3            |        cudaMalloc retries: 6         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13556 MiB |  14328 MiB | 424382 MiB | 410826 MiB |\n",
            "|       from large pool |  13323 MiB |  14096 MiB | 421065 MiB | 407741 MiB |\n",
            "|       from small pool |    232 MiB |    296 MiB |   3316 MiB |   3084 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13556 MiB |  14328 MiB | 424382 MiB | 410826 MiB |\n",
            "|       from large pool |  13323 MiB |  14096 MiB | 421065 MiB | 407741 MiB |\n",
            "|       from small pool |    232 MiB |    296 MiB |   3316 MiB |   3084 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13541 MiB |  14312 MiB | 423869 MiB | 410328 MiB |\n",
            "|       from large pool |  13308 MiB |  14080 MiB | 420555 MiB | 407246 MiB |\n",
            "|       from small pool |    232 MiB |    296 MiB |   3314 MiB |   3082 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14786 MiB |  14786 MiB |  28698 MiB |  13912 MiB |\n",
            "|       from large pool |  14552 MiB |  14552 MiB |  28174 MiB |  13622 MiB |\n",
            "|       from small pool |    234 MiB |    304 MiB |    524 MiB |    290 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 579443 KiB |   4164 MiB | 336475 MiB | 335909 MiB |\n",
            "|       from large pool | 577565 KiB |   4156 MiB | 333162 MiB | 332598 MiB |\n",
            "|       from small pool |   1878 KiB |      9 MiB |   3312 MiB |   3310 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1126    |    1137    |   21875    |   20749    |\n",
            "|       from large pool |     311    |     324    |    7376    |    7065    |\n",
            "|       from small pool |     815    |     952    |   14499    |   13684    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1126    |    1137    |   21875    |   20749    |\n",
            "|       from large pool |     311    |     324    |    7376    |    7065    |\n",
            "|       from small pool |     815    |     952    |   14499    |   13684    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     176    |     211    |     410    |     234    |\n",
            "|       from large pool |      59    |      59    |     148    |      89    |\n",
            "|       from small pool |     117    |     152    |     262    |     145    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      72    |    9187    |    9119    |\n",
            "|       from large pool |      45    |      46    |    3432    |    3387    |\n",
            "|       from small pool |      23    |      37    |    5755    |    5732    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:02:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  45% 25/55 [04:01<04:15,  8.51s/it]2024-10-21 16:05:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 33.06 MiB is free. Process 359607 has 14.71 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 671.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:05:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14260 MiB |  14261 MiB |    913 GiB |    899 GiB |\n",
            "|       from large pool |  14027 MiB |  14027 MiB |    905 GiB |    891 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |      7 GiB |      7 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14260 MiB |  14261 MiB |    913 GiB |    899 GiB |\n",
            "|       from large pool |  14027 MiB |  14027 MiB |    905 GiB |    891 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |      7 GiB |      7 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14244 MiB |  14245 MiB |    912 GiB |    898 GiB |\n",
            "|       from large pool |  14011 MiB |  14011 MiB |    904 GiB |    891 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |      7 GiB |      7 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14932 MiB |  14932 MiB |  32630 MiB |  17698 MiB |\n",
            "|       from large pool |  14696 MiB |  14696 MiB |  31942 MiB |  17246 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |    688 MiB |    452 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 687707 KiB |   4122 MiB | 770775 MiB | 770104 MiB |\n",
            "|       from large pool | 684758 KiB |   4112 MiB | 762555 MiB | 761886 MiB |\n",
            "|       from small pool |   2949 KiB |     10 MiB |   8220 MiB |   8217 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1063    |    1078    |   50124    |   49061    |\n",
            "|       from large pool |     271    |     271    |   16234    |   15963    |\n",
            "|       from small pool |     792    |     952    |   33890    |   33098    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1063    |    1078    |   50124    |   49061    |\n",
            "|       from large pool |     271    |     271    |   16234    |   15963    |\n",
            "|       from small pool |     792    |     952    |   33890    |   33098    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     177    |     215    |     505    |     328    |\n",
            "|       from large pool |      59    |      59    |     161    |     102    |\n",
            "|       from small pool |     118    |     157    |     344    |     226    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      69    |   21565    |   21497    |\n",
            "|       from large pool |      46    |      46    |    7870    |    7824    |\n",
            "|       from small pool |      22    |      33    |   13695    |   13673    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:05:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  64% 35/55 [04:49<02:05,  6.29s/it]2024-10-21 16:06:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 163.06 MiB is free. Process 359607 has 14.59 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 322.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:06:06 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 5            |        cudaMalloc retries: 15        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13865 MiB |  14479 MiB |   1346 GiB |   1333 GiB |\n",
            "|       from large pool |  13632 MiB |  14246 MiB |   1336 GiB |   1323 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13865 MiB |  14479 MiB |   1346 GiB |   1333 GiB |\n",
            "|       from large pool |  13632 MiB |  14246 MiB |   1336 GiB |   1323 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13842 MiB |  14456 MiB |   1345 GiB |   1331 GiB |\n",
            "|       from large pool |  13609 MiB |  14223 MiB |   1335 GiB |   1321 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14802 MiB |  14956 MiB |  60516 MiB |  45714 MiB |\n",
            "|       from large pool |  14568 MiB |  14722 MiB |  59646 MiB |  45078 MiB |\n",
            "|       from small pool |    234 MiB |    308 MiB |    870 MiB |    636 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 446766 KiB |   4144 MiB |   1086 GiB |   1085 GiB |\n",
            "|       from large pool | 445893 KiB |   4132 MiB |   1075 GiB |   1075 GiB |\n",
            "|       from small pool |    873 KiB |     15 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1126    |    1137    |   70901    |   69775    |\n",
            "|       from large pool |     311    |     324    |   24595    |   24284    |\n",
            "|       from small pool |     815    |     952    |   46306    |   45491    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1126    |    1137    |   70901    |   69775    |\n",
            "|       from large pool |     311    |     324    |   24595    |   24284    |\n",
            "|       from small pool |     815    |     952    |   46306    |   45491    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     193    |     215    |     723    |     530    |\n",
            "|       from large pool |      76    |      78    |     288    |     212    |\n",
            "|       from small pool |     117    |     154    |     435    |     318    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      75    |      96    |   30612    |   30537    |\n",
            "|       from large pool |      54    |      59    |   12004    |   11950    |\n",
            "|       from small pool |      21    |      57    |   18608    |   18587    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:06:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  67% 37/55 [04:53<01:17,  4.32s/it]2024-10-21 16:06:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 617.06 MiB is free. Process 359607 has 14.14 GiB memory in use. Of the allocated memory 12.94 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:06:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 6            |        cudaMalloc retries: 17        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13248 MiB |  13695 MiB |   1421 GiB |   1409 GiB |\n",
            "|       from large pool |  13020 MiB |  13467 MiB |   1411 GiB |   1398 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13248 MiB |  13695 MiB |   1421 GiB |   1409 GiB |\n",
            "|       from large pool |  13020 MiB |  13467 MiB |   1411 GiB |   1398 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13234 MiB |  13681 MiB |   1420 GiB |   1407 GiB |\n",
            "|       from large pool |  13005 MiB |  13452 MiB |   1410 GiB |   1397 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14348 MiB |  14780 MiB |  64834 MiB |  50486 MiB |\n",
            "|       from large pool |  14116 MiB |  14476 MiB |  63894 MiB |  49778 MiB |\n",
            "|       from small pool |    232 MiB |    304 MiB |    940 MiB |    708 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1099 MiB |   3984 MiB |   1137 GiB |   1136 GiB |\n",
            "|       from large pool |   1095 MiB |   3976 MiB |   1127 GiB |   1125 GiB |\n",
            "|       from small pool |      3 MiB |     11 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1091    |    1092    |   73627    |   72536    |\n",
            "|       from large pool |     293    |     293    |   25788    |   25495    |\n",
            "|       from small pool |     798    |     952    |   47839    |   47041    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1091    |    1092    |   73627    |   72536    |\n",
            "|       from large pool |     293    |     293    |   25788    |   25495    |\n",
            "|       from small pool |     798    |     952    |   47839    |   47041    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     228    |     765    |     583    |\n",
            "|       from large pool |      66    |      76    |     295    |     229    |\n",
            "|       from small pool |     116    |     152    |     470    |     354    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      85    |   31884    |   31821    |\n",
            "|       from large pool |      45    |      58    |   12601    |   12556    |\n",
            "|       from small pool |      18    |      31    |   19283    |   19265    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:06:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  69% 38/55 [04:56<01:04,  3.79s/it]2024-10-21 16:06:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 359607 has 14.52 GiB memory in use. Of the allocated memory 12.86 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:06:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 7            |        cudaMalloc retries: 19        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13166 MiB |  13695 MiB |   1442 GiB |   1429 GiB |\n",
            "|       from large pool |  12937 MiB |  13467 MiB |   1431 GiB |   1419 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13166 MiB |  13695 MiB |   1442 GiB |   1429 GiB |\n",
            "|       from large pool |  12937 MiB |  13467 MiB |   1431 GiB |   1419 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13148 MiB |  13681 MiB |   1441 GiB |   1428 GiB |\n",
            "|       from large pool |  12918 MiB |  13452 MiB |   1430 GiB |   1417 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14780 MiB |  65610 MiB |  50872 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    304 MiB |    940 MiB |    708 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1571 MiB |   3984 MiB |   1149 GiB |   1148 GiB |\n",
            "|       from large pool |   1568 MiB |   3976 MiB |   1139 GiB |   1137 GiB |\n",
            "|       from small pool |      2 MiB |     11 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1092    |    1092    |   74101    |   73009    |\n",
            "|       from large pool |     294    |     294    |   26072    |   25778    |\n",
            "|       from small pool |     798    |     952    |   48029    |   47231    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1092    |    1092    |   74101    |   73009    |\n",
            "|       from large pool |     294    |     294    |   26072    |   25778    |\n",
            "|       from small pool |     798    |     952    |   48029    |   47231    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     228    |     767    |     584    |\n",
            "|       from large pool |      67    |      76    |     297    |     230    |\n",
            "|       from small pool |     116    |     152    |     470    |     354    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      85    |   32085    |   32028    |\n",
            "|       from large pool |      37    |      58    |   12741    |   12704    |\n",
            "|       from small pool |      20    |      31    |   19344    |   19324    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:06:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  87% 48/55 [06:29<01:04,  9.21s/it]2024-10-21 16:07:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 229.06 MiB is free. Process 359607 has 14.52 GiB memory in use. Of the allocated memory 12.39 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:07:47 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 8            |        cudaMalloc retries: 20        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12688 MiB |  12688 MiB |   1819 GiB |   1806 GiB |\n",
            "|       from large pool |  12459 MiB |  12459 MiB |   1805 GiB |   1793 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     13 GiB |     13 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12688 MiB |  12688 MiB |   1819 GiB |   1806 GiB |\n",
            "|       from large pool |  12459 MiB |  12459 MiB |   1805 GiB |   1793 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     13 GiB |     13 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12675 MiB |  12675 MiB |   1817 GiB |   1805 GiB |\n",
            "|       from large pool |  12446 MiB |  12446 MiB |   1804 GiB |   1791 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     13 GiB |     13 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14736 MiB |  14824 MiB |  65696 MiB |  50960 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    230 MiB |    318 MiB |   1026 MiB |    796 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2047 MiB |   3461 MiB |   1507 GiB |   1505 GiB |\n",
            "|       from large pool |   2046 MiB |   3455 MiB |   1493 GiB |   1491 GiB |\n",
            "|       from small pool |      0 MiB |      8 MiB |     13 GiB |     13 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1120    |    1120    |   94332    |   93212    |\n",
            "|       from large pool |     315    |     315    |   33144    |   32829    |\n",
            "|       from small pool |     805    |     951    |   61188    |   60383    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1120    |    1120    |   94332    |   93212    |\n",
            "|       from large pool |     315    |     315    |   33144    |   32829    |\n",
            "|       from small pool |     805    |     951    |   61188    |   60383    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     226    |     810    |     628    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     115    |     159    |     513    |     398    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      78    |      78    |   41077    |   40999    |\n",
            "|       from large pool |      58    |      58    |   16298    |   16240    |\n",
            "|       from small pool |      20    |      29    |   24779    |   24759    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:07:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  96% 53/55 [07:09<00:18,  9.48s/it]2024-10-21 16:08:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 359607 has 14.52 GiB memory in use. Of the allocated memory 13.38 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:08:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13705 MiB |  13705 MiB |   1992 GiB |   1979 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   1977 GiB |   1964 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     15 GiB |     14 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13705 MiB |  13705 MiB |   1992 GiB |   1979 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   1977 GiB |   1964 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     15 GiB |     14 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13684 MiB |  13684 MiB |   1990 GiB |   1977 GiB |\n",
            "|       from large pool |  13454 MiB |  13454 MiB |   1975 GiB |   1962 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     15 GiB |     14 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14820 MiB |  65780 MiB |  51042 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    314 MiB |   1110 MiB |    878 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1032 MiB |   3875 MiB |   1670 GiB |   1669 GiB |\n",
            "|       from large pool |   1030 MiB |   3860 MiB |   1655 GiB |   1654 GiB |\n",
            "|       from small pool |      1 MiB |     16 MiB |     15 GiB |     15 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1120    |    1120    |  103598    |  102478    |\n",
            "|       from large pool |     315    |     315    |   36251    |   35936    |\n",
            "|       from small pool |     805    |     951    |   67347    |   66542    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1120    |    1120    |  103598    |  102478    |\n",
            "|       from large pool |     315    |     315    |   36251    |   35936    |\n",
            "|       from small pool |     805    |     951    |   67347    |   66542    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     224    |     852    |     669    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     116    |     157    |     555    |     439    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      77    |   45017    |   44952    |\n",
            "|       from large pool |      41    |      47    |   17811    |   17770    |\n",
            "|       from small pool |      24    |      50    |   27206    |   27182    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:08:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  98% 54/55 [07:10<00:07,  7.04s/it]2024-10-21 16:08:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-21 16:08:44 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(42105, 573), first few sample ids=[101]\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 1/19 [00:01<00:19,  1.10s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 2/19 [00:02<00:18,  1.09s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 3/19 [00:03<00:17,  1.08s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 4/19 [00:04<00:16,  1.12s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 5/19 [00:05<00:14,  1.07s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 6/19 [00:06<00:14,  1.13s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 7/19 [00:07<00:11,  1.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 8/19 [00:08<00:10,  1.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 9/19 [00:09<00:11,  1.12s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 10/19 [00:11<00:11,  1.31s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 11/19 [00:13<00:12,  1.58s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 12/19 [00:15<00:11,  1.71s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 13/19 [00:20<00:15,  2.60s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 14/19 [00:23<00:14,  2.91s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 15/19 [00:28<00:13,  3.26s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 16/19 [00:31<00:10,  3.44s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 17/19 [00:36<00:07,  3.94s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 18/19 [00:42<00:04,  4.33s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 19/19 [00:46<00:00,  4.19s/it]\u001b[A\n",
            "                                                                        \u001b[A2024-10-21 16:09:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.405 | nll_loss 6.883 | ppl 117.99 | wps 533.2 | wpb 1758.4 | bsz 14.4 | num_updates 46\n",
            "2024-10-21 16:09:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 46 updates\n",
            "2024-10-21 16:09:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/wordwise_transformer/checkpoint1.pt\n",
            "2024-10-21 16:09:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/wordwise_transformer/checkpoint1.pt\n",
            "2024-10-21 16:09:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/wordwise_transformer/checkpoint1.pt (epoch 1 @ 46 updates, score 7.405) (writing took 19.953772673000458 seconds)\n",
            "2024-10-21 16:09:51 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-10-21 16:09:51 | INFO | train | epoch 001 | loss 9.303 | nll_loss 9.056 | ppl 532.29 | wps 125.8 | ups 0.09 | wpb 1419.7 | bsz 12.3 | num_updates 46 | lr 5.75e-06 | gnorm 14.779 | clip 100 | train_wall 436 | gb_free 1.4 | wall 515\n",
            "2024-10-21 16:09:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 55\n",
            "epoch 002:   0% 0/55 [00:00<?, ?it/s]2024-10-21 16:09:51 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-10-21 16:09:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  20% 11/55 [01:50<07:50, 10.68s/it]2024-10-21 16:11:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 359607 has 14.52 GiB memory in use. Of the allocated memory 12.39 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:11:42 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 10           |        cudaMalloc retries: 22        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12688 MiB |  12688 MiB |   2821 GiB |   2809 GiB |\n",
            "|       from large pool |  12459 MiB |  12459 MiB |   2800 GiB |   2787 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     21 GiB |     21 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12688 MiB |  12688 MiB |   2821 GiB |   2809 GiB |\n",
            "|       from large pool |  12459 MiB |  12459 MiB |   2800 GiB |   2787 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     21 GiB |     21 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12675 MiB |  12675 MiB |   2819 GiB |   2806 GiB |\n",
            "|       from large pool |  12446 MiB |  12446 MiB |   2797 GiB |   2785 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     21 GiB |     21 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14826 MiB |  65868 MiB |  51130 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    320 MiB |   1198 MiB |    966 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2049 MiB |   3870 MiB |   2424 GiB |   2422 GiB |\n",
            "|       from large pool |   2046 MiB |   3860 MiB |   2402 GiB |   2400 GiB |\n",
            "|       from small pool |      2 MiB |     11 MiB |     22 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  139841    |  138717    |\n",
            "|       from large pool |     315    |     315    |   49216    |   48901    |\n",
            "|       from small pool |     809    |     955    |   90625    |   89816    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  139841    |  138717    |\n",
            "|       from large pool |     315    |     315    |   49216    |   48901    |\n",
            "|       from small pool |     809    |     955    |   90625    |   89816    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     227    |     896    |     713    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     116    |     160    |     599    |     483    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      81    |      81    |   60925    |   60844    |\n",
            "|       from large pool |      58    |      58    |   23466    |   23408    |\n",
            "|       from small pool |      23    |      32    |   37459    |   37436    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:11:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  27% 15/55 [02:27<07:45, 11.64s/it]2024-10-21 16:12:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 219.06 MiB is free. Process 359607 has 14.53 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 520.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:12:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 11           |        cudaMalloc retries: 23        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13859 MiB |  14432 MiB |   2967 GiB |   2954 GiB |\n",
            "|       from large pool |  13626 MiB |  14200 MiB |   2945 GiB |   2931 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     22 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13859 MiB |  14432 MiB |   2967 GiB |   2954 GiB |\n",
            "|       from large pool |  13626 MiB |  14200 MiB |   2945 GiB |   2931 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     22 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13843 MiB |  14416 MiB |   2965 GiB |   2951 GiB |\n",
            "|       from large pool |  13610 MiB |  14184 MiB |   2942 GiB |   2929 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     22 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14746 MiB |  14820 MiB |  65950 MiB |  51204 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    240 MiB |    314 MiB |   1280 MiB |   1040 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    886 MiB |   3053 MiB |   2557 GiB |   2556 GiB |\n",
            "|       from large pool |    879 MiB |   3032 MiB |   2534 GiB |   2533 GiB |\n",
            "|       from small pool |      7 MiB |     22 MiB |     23 GiB |     23 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  146896    |  145766    |\n",
            "|       from large pool |     312    |     325    |   51776    |   51464    |\n",
            "|       from small pool |     818    |     955    |   95120    |   94302    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  146896    |  145766    |\n",
            "|       from large pool |     312    |     325    |   51776    |   51464    |\n",
            "|       from small pool |     818    |     955    |   95120    |   94302    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     187    |     224    |     937    |     750    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     120    |     157    |     640    |     520    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      83    |      94    |   63922    |   63839    |\n",
            "|       from large pool |      56    |      58    |   24678    |   24622    |\n",
            "|       from small pool |      27    |      64    |   39244    |   39217    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:12:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  33% 18/55 [02:36<03:44,  6.07s/it]2024-10-21 16:12:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 223.06 MiB is free. Process 359607 has 14.53 GiB memory in use. Of the allocated memory 13.38 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:12:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 12           |        cudaMalloc retries: 24        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13705 MiB |  13705 MiB |   3063 GiB |   3050 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   3040 GiB |   3027 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     23 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13705 MiB |  13705 MiB |   3063 GiB |   3050 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   3040 GiB |   3027 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     23 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13684 MiB |  13684 MiB |   3061 GiB |   3047 GiB |\n",
            "|       from large pool |  13454 MiB |  13454 MiB |   3038 GiB |   3025 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     23 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14742 MiB |  14820 MiB |  66024 MiB |  51282 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |   1354 MiB |   1118 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1036 MiB |   3205 MiB |   2648 GiB |   2647 GiB |\n",
            "|       from large pool |   1030 MiB |   3194 MiB |   2624 GiB |   2623 GiB |\n",
            "|       from small pool |      5 MiB |     12 MiB |     23 GiB |     23 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  151906    |  150782    |\n",
            "|       from large pool |     315    |     315    |   53854    |   53539    |\n",
            "|       from small pool |     809    |     955    |   98052    |   97243    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  151906    |  150782    |\n",
            "|       from large pool |     315    |     315    |   53854    |   53539    |\n",
            "|       from small pool |     809    |     955    |   98052    |   97243    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     185    |     224    |     974    |     789    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     118    |     157    |     677    |     559    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      70    |   66042    |   65977    |\n",
            "|       from large pool |      41    |      47    |   25710    |   25669    |\n",
            "|       from small pool |      24    |      31    |   40332    |   40308    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:12:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  53% 29/55 [04:38<06:14, 14.40s/it]2024-10-21 16:14:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 310.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 359607 has 14.52 GiB memory in use. Of the allocated memory 13.12 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:14:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 13           |        cudaMalloc retries: 25        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13435 MiB |  13436 MiB |   3478 GiB |   3465 GiB |\n",
            "|       from large pool |  13205 MiB |  13205 MiB |   3451 GiB |   3438 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     26 GiB |     26 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13435 MiB |  13436 MiB |   3478 GiB |   3465 GiB |\n",
            "|       from large pool |  13205 MiB |  13205 MiB |   3451 GiB |   3438 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     26 GiB |     26 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13423 MiB |  13424 MiB |   3475 GiB |   3462 GiB |\n",
            "|       from large pool |  13193 MiB |  13193 MiB |   3448 GiB |   3436 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     26 GiB |     26 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14824 MiB |  66106 MiB |  51368 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    318 MiB |   1436 MiB |   1204 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1302 MiB |   2372 MiB |   3048 GiB |   3047 GiB |\n",
            "|       from large pool |   1300 MiB |   2356 MiB |   3021 GiB |   3020 GiB |\n",
            "|       from small pool |      1 MiB |     17 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1019    |    1082    |  173850    |  172831    |\n",
            "|       from large pool |     238    |     238    |   60894    |   60656    |\n",
            "|       from small pool |     781    |     955    |  112956    |  112175    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1019    |    1082    |  173850    |  172831    |\n",
            "|       from large pool |     238    |     238    |   60894    |   60656    |\n",
            "|       from small pool |     781    |     955    |  112956    |  112175    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     226    |    1015    |     832    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     116    |     159    |     718    |     602    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      91    |      92    |   76001    |   75910    |\n",
            "|       from large pool |      64    |      65    |   29355    |   29291    |\n",
            "|       from small pool |      27    |      44    |   46646    |   46619    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:14:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  60% 33/55 [04:57<02:51,  7.78s/it]2024-10-21 16:14:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 225.06 MiB is free. Process 359607 has 14.53 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 482.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:14:49 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 14           |        cudaMalloc retries: 26        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14257 MiB |  14258 MiB |   3625 GiB |   3611 GiB |\n",
            "|       from large pool |  14024 MiB |  14024 MiB |   3598 GiB |   3584 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14257 MiB |  14258 MiB |   3625 GiB |   3611 GiB |\n",
            "|       from large pool |  14024 MiB |  14024 MiB |   3598 GiB |   3584 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14245 MiB |  14246 MiB |   3622 GiB |   3609 GiB |\n",
            "|       from large pool |  14012 MiB |  14012 MiB |   3595 GiB |   3581 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14740 MiB |  14814 MiB |  66182 MiB |  51442 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    234 MiB |    308 MiB |   1512 MiB |   1278 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 494394 KiB |   4761 MiB |   3187 GiB |   3186 GiB |\n",
            "|       from large pool | 492971 KiB |   4749 MiB |   3158 GiB |   3158 GiB |\n",
            "|       from small pool |   1423 KiB |     14 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1067    |    1082    |  180979    |  179912    |\n",
            "|       from large pool |     272    |     272    |   63825    |   63553    |\n",
            "|       from small pool |     795    |     955    |  117154    |  116359    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1067    |    1082    |  180979    |  179912    |\n",
            "|       from large pool |     272    |     272    |   63825    |   63553    |\n",
            "|       from small pool |     795    |     955    |  117154    |  116359    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     184    |     221    |    1053    |     869    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     117    |     154    |     756    |     639    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      94    |      95    |   79354    |   79260    |\n",
            "|       from large pool |      65    |      66    |   30902    |   30837    |\n",
            "|       from small pool |      29    |      53    |   48452    |   48423    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:14:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  64% 35/55 [05:02<01:42,  5.14s/it]2024-10-21 16:14:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 229.06 MiB is free. Process 359607 has 14.52 GiB memory in use. Of the allocated memory 13.69 GiB is allocated by PyTorch, and 719.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:14:56 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 15           |        cudaMalloc retries: 27        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14016 MiB |  14016 MiB |   3683 GiB |   3669 GiB |\n",
            "|       from large pool |  13788 MiB |  13788 MiB |   3655 GiB |   3642 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14016 MiB |  14016 MiB |   3683 GiB |   3669 GiB |\n",
            "|       from large pool |  13788 MiB |  13788 MiB |   3655 GiB |   3642 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14002 MiB |  14002 MiB |   3680 GiB |   3667 GiB |\n",
            "|       from large pool |  13773 MiB |  13773 MiB |   3652 GiB |   3639 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14736 MiB |  14810 MiB |  66252 MiB |  51516 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    230 MiB |    304 MiB |   1582 MiB |   1352 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 736372 KiB |   3232 MiB |   3234 GiB |   3233 GiB |\n",
            "|       from large pool | 734992 KiB |   3224 MiB |   3205 GiB |   3205 GiB |\n",
            "|       from small pool |   1380 KiB |      9 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1096    |    1096    |  183691    |  182595    |\n",
            "|       from large pool |     294    |     294    |   64987    |   64693    |\n",
            "|       from small pool |     802    |     955    |  118704    |  117902    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1096    |    1096    |  183691    |  182595    |\n",
            "|       from large pool |     294    |     294    |   64987    |   64693    |\n",
            "|       from small pool |     802    |     955    |  118704    |  117902    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     219    |    1088    |     906    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     115    |     152    |     791    |     676    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      81    |   80442    |   80372    |\n",
            "|       from large pool |      46    |      57    |   31420    |   31374    |\n",
            "|       from small pool |      24    |      38    |   49022    |   48998    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:14:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  69% 38/55 [05:13<01:11,  4.21s/it]2024-10-21 16:15:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 359607 has 14.52 GiB memory in use. Of the allocated memory 13.56 GiB is allocated by PyTorch, and 851.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:15:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 16           |        cudaMalloc retries: 28        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13118 MiB |  13886 MiB |   3779 GiB |   3767 GiB |\n",
            "|       from large pool |  12887 MiB |  13655 MiB |   3751 GiB |   3738 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13118 MiB |  13886 MiB |   3779 GiB |   3767 GiB |\n",
            "|       from large pool |  12887 MiB |  13655 MiB |   3751 GiB |   3738 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13100 MiB |  13868 MiB |   3776 GiB |   3764 GiB |\n",
            "|       from large pool |  12869 MiB |  13637 MiB |   3748 GiB |   3735 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14818 MiB |  66334 MiB |  51596 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    312 MiB |   1664 MiB |   1432 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1619 MiB |   3459 MiB |   3321 GiB |   3319 GiB |\n",
            "|       from large pool |   1618 MiB |   3443 MiB |   3292 GiB |   3290 GiB |\n",
            "|       from small pool |      0 MiB |     18 MiB |     29 GiB |     29 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  188772    |  187642    |\n",
            "|       from large pool |     312    |     325    |   67107    |   66795    |\n",
            "|       from small pool |     818    |     955    |  121665    |  120847    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  188772    |  187642    |\n",
            "|       from large pool |     312    |     325    |   67107    |   66795    |\n",
            "|       from small pool |     818    |     955    |  121665    |  120847    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     223    |    1129    |     946    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     116    |     156    |     832    |     716    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      81    |   82598    |   82522    |\n",
            "|       from large pool |      58    |      59    |   32461    |   32403    |\n",
            "|       from small pool |      18    |      40    |   50137    |   50119    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:15:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  73% 40/55 [05:19<00:58,  3.93s/it]2024-10-21 16:15:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 229.06 MiB is free. Process 359607 has 14.52 GiB memory in use. Of the allocated memory 12.12 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:15:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 17           |        cudaMalloc retries: 29        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12408 MiB |  12408 MiB |   3843 GiB |   3831 GiB |\n",
            "|       from large pool |  12179 MiB |  12179 MiB |   3814 GiB |   3802 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12408 MiB |  12408 MiB |   3843 GiB |   3831 GiB |\n",
            "|       from large pool |  12179 MiB |  12179 MiB |   3814 GiB |   3802 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12392 MiB |  12392 MiB |   3840 GiB |   3828 GiB |\n",
            "|       from large pool |  12162 MiB |  12162 MiB |   3811 GiB |   3799 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14736 MiB |  14808 MiB |  66404 MiB |  51668 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    230 MiB |    302 MiB |   1734 MiB |   1504 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2327 MiB |   3077 MiB |   3380 GiB |   3378 GiB |\n",
            "|       from large pool |   2326 MiB |   3070 MiB |   3350 GiB |   3348 GiB |\n",
            "|       from small pool |      0 MiB |      8 MiB |     29 GiB |     29 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1068    |    1082    |  191432    |  190364    |\n",
            "|       from large pool |     273    |     273    |   68267    |   67994    |\n",
            "|       from small pool |     795    |     955    |  123165    |  122370    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1068    |    1082    |  191432    |  190364    |\n",
            "|       from large pool |     273    |     273    |   68267    |   67994    |\n",
            "|       from small pool |     795    |     955    |  123165    |  122370    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     218    |    1164    |     982    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     115    |     151    |     867    |     752    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      70    |   83829    |   83769    |\n",
            "|       from large pool |      39    |      47    |   33100    |   33061    |\n",
            "|       from small pool |      21    |      34    |   50729    |   50708    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:15:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  75% 41/55 [05:21<00:44,  3.19s/it]2024-10-21 16:15:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 466.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 151.06 MiB is free. Process 359607 has 14.60 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 2.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:15:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 18           |        cudaMalloc retries: 31        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12661 MiB |  12661 MiB |   3863 GiB |   3851 GiB |\n",
            "|       from large pool |  12432 MiB |  12432 MiB |   3835 GiB |   3822 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12661 MiB |  12661 MiB |   3863 GiB |   3851 GiB |\n",
            "|       from large pool |  12432 MiB |  12432 MiB |   3835 GiB |   3822 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12639 MiB |  12639 MiB |   3860 GiB |   3848 GiB |\n",
            "|       from large pool |  12410 MiB |  12410 MiB |   3831 GiB |   3819 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14814 MiB |  14814 MiB |  66870 MiB |  52056 MiB |\n",
            "|       from large pool |  14584 MiB |  14584 MiB |  65136 MiB |  50552 MiB |\n",
            "|       from small pool |    230 MiB |    302 MiB |   1734 MiB |   1504 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2152 MiB |   3255 MiB |   3395 GiB |   3393 GiB |\n",
            "|       from large pool |   2151 MiB |   3252 MiB |   3366 GiB |   3363 GiB |\n",
            "|       from small pool |      1 MiB |      8 MiB |     29 GiB |     29 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  191958    |  190834    |\n",
            "|       from large pool |     315    |     315    |   68583    |   68268    |\n",
            "|       from small pool |     809    |     955    |  123375    |  122566    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  191958    |  190834    |\n",
            "|       from large pool |     315    |     315    |   68583    |   68268    |\n",
            "|       from small pool |     809    |     955    |  123375    |  122566    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     218    |    1165    |     983    |\n",
            "|       from large pool |      67    |      67    |     298    |     231    |\n",
            "|       from small pool |     115    |     151    |     867    |     752    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      76    |   84032    |   83970    |\n",
            "|       from large pool |      42    |      55    |   33243    |   33201    |\n",
            "|       from small pool |      20    |      34    |   50789    |   50769    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:15:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  85% 47/55 [06:45<01:50, 13.82s/it]2024-10-21 16:16:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 141.06 MiB is free. Process 359607 has 14.61 GiB memory in use. Of the allocated memory 13.67 GiB is allocated by PyTorch, and 828.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:16:38 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 19           |        cudaMalloc retries: 32        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13572 MiB |  14180 MiB |   4064 GiB |   4051 GiB |\n",
            "|       from large pool |  13340 MiB |  13949 MiB |   4033 GiB |   4020 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     31 GiB |     30 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13572 MiB |  14180 MiB |   4064 GiB |   4051 GiB |\n",
            "|       from large pool |  13340 MiB |  13949 MiB |   4033 GiB |   4020 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     31 GiB |     30 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13559 MiB |  14166 MiB |   4061 GiB |   4048 GiB |\n",
            "|       from large pool |  13327 MiB |  13935 MiB |   4030 GiB |   4017 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     31 GiB |     30 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14824 MiB |  14902 MiB |  66958 MiB |  52134 MiB |\n",
            "|       from large pool |  14584 MiB |  14584 MiB |  65136 MiB |  50552 MiB |\n",
            "|       from small pool |    240 MiB |    318 MiB |   1822 MiB |   1582 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1251 MiB |   2255 MiB |   3586 GiB |   3585 GiB |\n",
            "|       from large pool |   1243 MiB |   2236 MiB |   3554 GiB |   3553 GiB |\n",
            "|       from small pool |      8 MiB |     31 MiB |     31 GiB |     31 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1132    |    1141    |  203248    |  202116    |\n",
            "|       from large pool |     314    |     325    |   71501    |   71187    |\n",
            "|       from small pool |     818    |     955    |  131747    |  130929    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1132    |    1141    |  203248    |  202116    |\n",
            "|       from large pool |     314    |     325    |   71501    |   71187    |\n",
            "|       from small pool |     818    |     955    |  131747    |  130929    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     187    |     226    |    1209    |    1022    |\n",
            "|       from large pool |      67    |      67    |     298    |     231    |\n",
            "|       from small pool |     120    |     159    |     911    |     791    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      88    |      90    |   89137    |   89049    |\n",
            "|       from large pool |      65    |      66    |   34745    |   34680    |\n",
            "|       from small pool |      23    |      57    |   54392    |   54369    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:16:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  91% 50/55 [06:55<00:34,  6.98s/it]2024-10-21 16:16:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 474.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 387.06 MiB is free. Process 359607 has 14.37 GiB memory in use. Of the allocated memory 13.61 GiB is allocated by PyTorch, and 641.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:16:47 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 20           |        cudaMalloc retries: 34        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13936 MiB |  13936 MiB |   4170 GiB |   4156 GiB |\n",
            "|       from large pool |  13696 MiB |  13696 MiB |   4138 GiB |   4125 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     31 GiB |     31 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13936 MiB |  13936 MiB |   4170 GiB |   4156 GiB |\n",
            "|       from large pool |  13696 MiB |  13696 MiB |   4138 GiB |   4125 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     31 GiB |     31 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13924 MiB |  13924 MiB |   4167 GiB |   4153 GiB |\n",
            "|       from large pool |  13685 MiB |  13685 MiB |   4135 GiB |   4122 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     31 GiB |     31 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14578 MiB |  14894 MiB |  69182 MiB |  54604 MiB |\n",
            "|       from large pool |  14338 MiB |  14584 MiB |  67288 MiB |  52950 MiB |\n",
            "|       from small pool |    240 MiB |    310 MiB |   1894 MiB |   1654 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 657300 KiB |   3189 MiB |   3682 GiB |   3681 GiB |\n",
            "|       from large pool | 656385 KiB |   3168 MiB |   3649 GiB |   3649 GiB |\n",
            "|       from small pool |    914 KiB |     20 MiB |     32 GiB |     32 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1058    |    1082    |  208135    |  207077    |\n",
            "|       from large pool |     256    |     256    |   73498    |   73242    |\n",
            "|       from small pool |     802    |     955    |  134637    |  133835    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1058    |    1082    |  208135    |  207077    |\n",
            "|       from large pool |     256    |     256    |   73498    |   73242    |\n",
            "|       from small pool |     802    |     955    |  134637    |  133835    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     188    |     222    |    1252    |    1064    |\n",
            "|       from large pool |      68    |      68    |     305    |     237    |\n",
            "|       from small pool |     120    |     155    |     947    |     827    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      84    |      86    |   91181    |   91097    |\n",
            "|       from large pool |      59    |      60    |   35693    |   35634    |\n",
            "|       from small pool |      25    |      52    |   55488    |   55463    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:16:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  98% 54/55 [07:07<00:04,  4.15s/it]2024-10-21 16:17:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 1/19 [00:01<00:19,  1.06s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 2/19 [00:02<00:18,  1.07s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 3/19 [00:03<00:17,  1.07s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 4/19 [00:04<00:16,  1.12s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 5/19 [00:05<00:14,  1.07s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 6/19 [00:06<00:14,  1.13s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 7/19 [00:07<00:11,  1.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 8/19 [00:08<00:10,  1.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 9/19 [00:09<00:11,  1.12s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 10/19 [00:11<00:11,  1.31s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 11/19 [00:13<00:12,  1.59s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 12/19 [00:15<00:12,  1.72s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 13/19 [00:20<00:15,  2.63s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 14/19 [00:24<00:14,  2.94s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 15/19 [00:28<00:13,  3.28s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84% 16/19 [00:32<00:10,  3.46s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 17/19 [00:37<00:07,  3.95s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 18/19 [00:42<00:04,  4.34s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 19/19 [00:46<00:00,  4.20s/it]\u001b[A\n",
            "                                                                        \u001b[A2024-10-21 16:17:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.628 | nll_loss 5.997 | ppl 63.85 | wps 531.1 | wpb 1758.4 | bsz 14.4 | num_updates 90 | best_loss 6.628\n",
            "2024-10-21 16:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 90 updates\n",
            "2024-10-21 16:17:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/wordwise_transformer/checkpoint2.pt\n",
            "2024-10-21 16:18:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/wordwise_transformer/checkpoint2.pt\n",
            "2024-10-21 16:18:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/wordwise_transformer/checkpoint2.pt (epoch 2 @ 90 updates, score 6.628) (writing took 17.96310735499992 seconds)\n",
            "2024-10-21 16:18:16 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-10-21 16:18:16 | INFO | train | epoch 002 | loss 6.88 | nll_loss 6.296 | ppl 78.58 | wps 120.8 | ups 0.09 | wpb 1387.9 | bsz 12.1 | num_updates 90 | lr 1.125e-05 | gnorm 4.126 | clip 100 | train_wall 426 | gb_free 6.4 | wall 1020\n",
            "2024-10-21 16:18:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 55\n",
            "epoch 003:   0% 0/55 [00:00<?, ?it/s]2024-10-21 16:18:16 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-10-21 16:18:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2024-10-21 16:18:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 141.06 MiB is free. Process 359607 has 14.61 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 601.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:18:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 21           |        cudaMalloc retries: 35        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13856 MiB |  14429 MiB |   4685 GiB |   4672 GiB |\n",
            "|       from large pool |  13624 MiB |  14197 MiB |   4650 GiB |   4637 GiB |\n",
            "|       from small pool |    232 MiB |    307 MiB |     35 GiB |     34 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13856 MiB |  14429 MiB |   4685 GiB |   4672 GiB |\n",
            "|       from large pool |  13624 MiB |  14197 MiB |   4650 GiB |   4637 GiB |\n",
            "|       from small pool |    232 MiB |    307 MiB |     35 GiB |     34 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13843 MiB |  14416 MiB |   4682 GiB |   4668 GiB |\n",
            "|       from large pool |  13610 MiB |  14184 MiB |   4646 GiB |   4633 GiB |\n",
            "|       from small pool |    232 MiB |    307 MiB |     35 GiB |     34 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14824 MiB |  14902 MiB |  69506 MiB |  54682 MiB |\n",
            "|       from large pool |  14588 MiB |  14588 MiB |  67538 MiB |  52950 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |   1968 MiB |   1732 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 734401 KiB |   3909 MiB |   4057 GiB |   4057 GiB |\n",
            "|       from large pool | 730956 KiB |   3901 MiB |   4021 GiB |   4021 GiB |\n",
            "|       from small pool |   3444 KiB |     21 MiB |     36 GiB |     36 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1126    |    1137    |  227286    |  226160    |\n",
            "|       from large pool |     312    |     325    |   81378    |   81066    |\n",
            "|       from small pool |     814    |     968    |  145908    |  145094    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1126    |    1137    |  227286    |  226160    |\n",
            "|       from large pool |     312    |     325    |   81378    |   81066    |\n",
            "|       from small pool |     814    |     968    |  145908    |  145094    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     187    |     226    |    1290    |    1103    |\n",
            "|       from large pool |      69    |      69    |     306    |     237    |\n",
            "|       from small pool |     118    |     157    |     984    |     866    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      82    |      87    |  100340    |  100258    |\n",
            "|       from large pool |      59    |      61    |   39700    |   39641    |\n",
            "|       from small pool |      23    |      40    |   60640    |   60617    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:18:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:   2% 1/55 [00:01<01:04,  1.20s/it]2024-10-21 16:18:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 3.06 MiB is free. Process 359607 has 14.74 GiB memory in use. Of the allocated memory 12.12 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:18:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 22           |        cudaMalloc retries: 36        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12408 MiB |  14429 MiB |   4704 GiB |   4692 GiB |\n",
            "|       from large pool |  12179 MiB |  14197 MiB |   4669 GiB |   4657 GiB |\n",
            "|       from small pool |    229 MiB |    307 MiB |     35 GiB |     35 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12408 MiB |  14429 MiB |   4704 GiB |   4692 GiB |\n",
            "|       from large pool |  12179 MiB |  14197 MiB |   4669 GiB |   4657 GiB |\n",
            "|       from small pool |    229 MiB |    307 MiB |     35 GiB |     35 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12392 MiB |  14416 MiB |   4700 GiB |   4688 GiB |\n",
            "|       from large pool |  12162 MiB |  14184 MiB |   4665 GiB |   4653 GiB |\n",
            "|       from small pool |    229 MiB |    307 MiB |     35 GiB |     35 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14962 MiB |  14962 MiB |  69894 MiB |  54932 MiB |\n",
            "|       from large pool |  14726 MiB |  14726 MiB |  67926 MiB |  53200 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |   1968 MiB |   1732 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2553 MiB |   4085 MiB |   4072 GiB |   4070 GiB |\n",
            "|       from large pool |   2546 MiB |   4076 MiB |   4036 GiB |   4033 GiB |\n",
            "|       from small pool |      6 MiB |     21 MiB |     36 GiB |     36 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1064    |    1137    |  227708    |  226644    |\n",
            "|       from large pool |     273    |     325    |   81630    |   81357    |\n",
            "|       from small pool |     791    |     968    |  146078    |  145287    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1064    |    1137    |  227708    |  226644    |\n",
            "|       from large pool |     273    |     325    |   81630    |   81357    |\n",
            "|       from small pool |     791    |     968    |  146078    |  145287    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     187    |     226    |    1291    |    1104    |\n",
            "|       from large pool |      69    |      69    |     307    |     238    |\n",
            "|       from small pool |     118    |     157    |     984    |     866    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      87    |  100547    |  100478    |\n",
            "|       from large pool |      46    |      62    |   39850    |   39804    |\n",
            "|       from small pool |      23    |      40    |   60697    |   60674    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:18:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  24% 13/55 [01:17<03:32,  5.07s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:19:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 311.06 MiB is free. Process 359607 has 14.44 GiB memory in use. Of the allocated memory 13.56 GiB is allocated by PyTorch, and 771.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:19:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 23           |        cudaMalloc retries: 39        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13115 MiB |  13882 MiB |   5178 GiB |   5165 GiB |\n",
            "|       from large pool |  12883 MiB |  13651 MiB |   5139 GiB |   5127 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13115 MiB |  13882 MiB |   5178 GiB |   5165 GiB |\n",
            "|       from large pool |  12883 MiB |  13651 MiB |   5139 GiB |   5127 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13100 MiB |  13868 MiB |   5174 GiB |   5161 GiB |\n",
            "|       from large pool |  12869 MiB |  13637 MiB |   5135 GiB |   5123 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14654 MiB |  14734 MiB |  80984 MiB |  66330 MiB |\n",
            "|       from large pool |  14422 MiB |  14422 MiB |  78854 MiB |  64432 MiB |\n",
            "|       from small pool |    232 MiB |    312 MiB |   2130 MiB |   1898 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1072 MiB |   4480 MiB |   4438 GiB |   4437 GiB |\n",
            "|       from large pool |   1072 MiB |   4474 MiB |   4399 GiB |   4398 GiB |\n",
            "|       from small pool |      0 MiB |      8 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  252589    |  251459    |\n",
            "|       from large pool |     312    |     325    |   90930    |   90618    |\n",
            "|       from small pool |     818    |     954    |  161659    |  160841    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  252589    |  251459    |\n",
            "|       from large pool |     312    |     325    |   90930    |   90618    |\n",
            "|       from small pool |     818    |     954    |  161659    |  160841    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     185    |     225    |    1405    |    1220    |\n",
            "|       from large pool |      69    |      69    |     340    |     271    |\n",
            "|       from small pool |     116    |     156    |    1065    |     949    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |      82    |  111350    |  111273    |\n",
            "|       from large pool |      60    |      61    |   44511    |   44451    |\n",
            "|       from small pool |      17    |      30    |   66839    |   66822    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:19:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  25% 14/55 [01:18<02:40,  3.93s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:19:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 399.06 MiB is free. Process 359607 has 14.36 GiB memory in use. Of the allocated memory 13.69 GiB is allocated by PyTorch, and 549.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:19:38 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 24           |        cudaMalloc retries: 41        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14016 MiB |  14016 MiB |   5202 GiB |   5188 GiB |\n",
            "|       from large pool |  13787 MiB |  13787 MiB |   5163 GiB |   5150 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14016 MiB |  14016 MiB |   5202 GiB |   5188 GiB |\n",
            "|       from large pool |  13787 MiB |  13787 MiB |   5163 GiB |   5150 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14002 MiB |  14002 MiB |   5197 GiB |   5184 GiB |\n",
            "|       from large pool |  13773 MiB |  13773 MiB |   5159 GiB |   5145 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14566 MiB |  14956 MiB |  84824 MiB |  70258 MiB |\n",
            "|       from large pool |  14336 MiB |  14724 MiB |  82694 MiB |  68358 MiB |\n",
            "|       from small pool |    230 MiB |    312 MiB |   2130 MiB |   1900 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 563123 KiB |   4480 MiB |   4450 GiB |   4450 GiB |\n",
            "|       from large pool | 561743 KiB |   4474 MiB |   4411 GiB |   4410 GiB |\n",
            "|       from small pool |   1380 KiB |      8 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1096    |    1141    |  253063    |  251967    |\n",
            "|       from large pool |     294    |     325    |   91214    |   90920    |\n",
            "|       from small pool |     802    |     954    |  161849    |  161047    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1096    |    1141    |  253063    |  251967    |\n",
            "|       from large pool |     294    |     325    |   91214    |   90920    |\n",
            "|       from small pool |     802    |     954    |  161849    |  161047    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     178    |     225    |    1410    |    1232    |\n",
            "|       from large pool |      63    |      69    |     345    |     282    |\n",
            "|       from small pool |     115    |     156    |    1065    |     950    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      66    |      82    |  111530    |  111464    |\n",
            "|       from large pool |      47    |      61    |   44625    |   44578    |\n",
            "|       from small pool |      19    |      30    |   66905    |   66886    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:19:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  36% 20/55 [01:44<02:46,  4.77s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:20:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 77.06 MiB is free. Process 359607 has 14.67 GiB memory in use. Of the allocated memory 13.38 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:20:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 25           |        cudaMalloc retries: 43        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13705 MiB |  13705 MiB |   5416 GiB |   5402 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   5376 GiB |   5363 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13705 MiB |  13705 MiB |   5416 GiB |   5402 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   5376 GiB |   5363 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13684 MiB |  13684 MiB |   5411 GiB |   5398 GiB |\n",
            "|       from large pool |  13454 MiB |  13454 MiB |   5371 GiB |   5358 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14888 MiB |  14888 MiB |  85224 MiB |  70336 MiB |\n",
            "|       from large pool |  14656 MiB |  14656 MiB |  83014 MiB |  68358 MiB |\n",
            "|       from small pool |    232 MiB |    310 MiB |   2210 MiB |   1978 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1182 MiB |   3226 MiB |   4650 GiB |   4649 GiB |\n",
            "|       from large pool |   1180 MiB |   3224 MiB |   4609 GiB |   4608 GiB |\n",
            "|       from small pool |      1 MiB |     14 MiB |     40 GiB |     40 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  264795    |  263671    |\n",
            "|       from large pool |     315    |     315    |   95968    |   95653    |\n",
            "|       from small pool |     809    |     955    |  168827    |  168018    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  264795    |  263671    |\n",
            "|       from large pool |     315    |     315    |   95968    |   95653    |\n",
            "|       from small pool |     809    |     955    |  168827    |  168018    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     180    |     218    |    1451    |    1271    |\n",
            "|       from large pool |      64    |      64    |     346    |     282    |\n",
            "|       from small pool |     116    |     155    |    1105    |     989    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      82    |  116708    |  116647    |\n",
            "|       from large pool |      41    |      47    |   47010    |   46969    |\n",
            "|       from small pool |      20    |      51    |   69698    |   69678    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:20:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  40% 22/55 [01:50<02:18,  4.20s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:20:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 13.06 MiB is free. Process 359607 has 14.73 GiB memory in use. Of the allocated memory 12.39 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:20:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 26           |        cudaMalloc retries: 45        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12689 MiB |  12689 MiB |   5481 GiB |   5468 GiB |\n",
            "|       from large pool |  12460 MiB |  12460 MiB |   5440 GiB |   5428 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     40 GiB |     40 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12689 MiB |  12689 MiB |   5481 GiB |   5468 GiB |\n",
            "|       from large pool |  12460 MiB |  12460 MiB |   5440 GiB |   5428 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     40 GiB |     40 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12675 MiB |  12675 MiB |   5476 GiB |   5464 GiB |\n",
            "|       from large pool |  12446 MiB |  12446 MiB |   5436 GiB |   5424 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     40 GiB |     40 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14952 MiB |  14958 MiB |  85682 MiB |  70730 MiB |\n",
            "|       from large pool |  14722 MiB |  14722 MiB |  83400 MiB |  68678 MiB |\n",
            "|       from small pool |    230 MiB |    302 MiB |   2282 MiB |   2052 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2262 MiB |   3012 MiB |   4711 GiB |   4708 GiB |\n",
            "|       from large pool |   2261 MiB |   3010 MiB |   4669 GiB |   4667 GiB |\n",
            "|       from small pool |      0 MiB |      7 MiB |     41 GiB |     41 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  267559    |  266435    |\n",
            "|       from large pool |     315    |     315    |   97192    |   96877    |\n",
            "|       from small pool |     809    |     955    |  170367    |  169558    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  267559    |  266435    |\n",
            "|       from large pool |     315    |     315    |   97192    |   96877    |\n",
            "|       from small pool |     809    |     955    |  170367    |  169558    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     179    |     215    |    1488    |    1309    |\n",
            "|       from large pool |      64    |      64    |     347    |     283    |\n",
            "|       from small pool |     115    |     151    |    1141    |    1026    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |      80    |  118042    |  117962    |\n",
            "|       from large pool |      56    |      56    |   47700    |   47644    |\n",
            "|       from small pool |      24    |      34    |   70342    |   70318    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:20:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  51% 28/55 [02:41<03:51,  8.56s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:20:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 85.06 MiB is free. Process 359607 has 14.66 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 545.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:20:59 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 27           |        cudaMalloc retries: 49        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13579 MiB |  14334 MiB |   5692 GiB |   5679 GiB |\n",
            "|       from large pool |  13348 MiB |  14103 MiB |   5650 GiB |   5637 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     41 GiB |     41 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13579 MiB |  14334 MiB |   5692 GiB |   5679 GiB |\n",
            "|       from large pool |  13348 MiB |  14103 MiB |   5650 GiB |   5637 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     41 GiB |     41 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13559 MiB |  14313 MiB |   5688 GiB |   5674 GiB |\n",
            "|       from large pool |  13327 MiB |  14081 MiB |   5646 GiB |   5633 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     41 GiB |     41 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14880 MiB |  14880 MiB |  96384 MiB |  81504 MiB |\n",
            "|       from large pool |  14646 MiB |  14646 MiB |  93988 MiB |  79342 MiB |\n",
            "|       from small pool |    234 MiB |    304 MiB |   2396 MiB |   2162 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 651404 KiB |   3479 MiB |   4859 GiB |   4859 GiB |\n",
            "|       from large pool | 649004 KiB |   3472 MiB |   4816 GiB |   4816 GiB |\n",
            "|       from small pool |   2399 KiB |      9 MiB |     42 GiB |     42 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1132    |    1141    |  279015    |  277883    |\n",
            "|       from large pool |     314    |     325    |  101128    |  100814    |\n",
            "|       from small pool |     818    |     955    |  177887    |  177069    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1132    |    1141    |  279015    |  277883    |\n",
            "|       from large pool |     314    |     325    |  101128    |  100814    |\n",
            "|       from small pool |     818    |     955    |  177887    |  177069    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     198    |     233    |    1593    |    1395    |\n",
            "|       from large pool |      81    |      81    |     395    |     314    |\n",
            "|       from small pool |     117    |     152    |    1198    |    1081    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      84    |      91    |  123174    |  123090    |\n",
            "|       from large pool |      64    |      67    |   49757    |   49693    |\n",
            "|       from small pool |      20    |      30    |   73417    |   73397    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:20:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  65% 36/55 [04:24<05:28, 17.31s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:22:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 157.06 MiB is free. Process 359607 has 14.59 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 549.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:22:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 28           |        cudaMalloc retries: 52        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14258 MiB |  14259 MiB |   5978 GiB |   5964 GiB |\n",
            "|       from large pool |  14026 MiB |  14026 MiB |   5933 GiB |   5919 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     44 GiB |     44 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14258 MiB |  14259 MiB |   5978 GiB |   5964 GiB |\n",
            "|       from large pool |  14026 MiB |  14026 MiB |   5933 GiB |   5919 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     44 GiB |     44 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14245 MiB |  14246 MiB |   5973 GiB |   5959 GiB |\n",
            "|       from large pool |  14012 MiB |  14012 MiB |   5928 GiB |   5914 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     44 GiB |     44 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14808 MiB |  14810 MiB | 105938 MiB |  91130 MiB |\n",
            "|       from large pool |  14574 MiB |  14574 MiB | 103450 MiB |  88876 MiB |\n",
            "|       from small pool |    234 MiB |    318 MiB |   2488 MiB |   2254 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 562238 KiB |   2280 MiB |   5062 GiB |   5061 GiB |\n",
            "|       from large pool | 560815 KiB |   2274 MiB |   5016 GiB |   5016 GiB |\n",
            "|       from small pool |   1423 KiB |     18 MiB |     45 GiB |     45 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1067    |    1082    |  294290    |  293223    |\n",
            "|       from large pool |     272    |     272    |  105532    |  105260    |\n",
            "|       from small pool |     795    |     955    |  188758    |  187963    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1067    |    1082    |  294290    |  293223    |\n",
            "|       from large pool |     272    |     272    |  105532    |  105260    |\n",
            "|       from small pool |     795    |     955    |  188758    |  187963    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     205    |     238    |    1680    |    1475    |\n",
            "|       from large pool |      88    |      88    |     436    |     348    |\n",
            "|       from small pool |     117    |     159    |    1244    |    1127    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      99    |     100    |  130369    |  130270    |\n",
            "|       from large pool |      73    |      73    |   52334    |   52261    |\n",
            "|       from small pool |      26    |      49    |   78035    |   78009    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:22:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  73% 40/55 [05:09<03:04, 12.31s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:23:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 11.06 MiB is free. Process 359607 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 536.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:23:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 29           |        cudaMalloc retries: 54        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13943 MiB |  14417 MiB |   6108 GiB |   6095 GiB |\n",
            "|       from large pool |  13704 MiB |  14178 MiB |   6063 GiB |   6049 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     45 GiB |     45 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13943 MiB |  14417 MiB |   6108 GiB |   6095 GiB |\n",
            "|       from large pool |  13704 MiB |  14178 MiB |   6063 GiB |   6049 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     45 GiB |     45 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13924 MiB |  14398 MiB |   6103 GiB |   6090 GiB |\n",
            "|       from large pool |  13685 MiB |  14159 MiB |   6058 GiB |   6044 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     45 GiB |     45 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14954 MiB |  14956 MiB | 113590 MiB |  98636 MiB |\n",
            "|       from large pool |  14714 MiB |  14714 MiB | 111018 MiB |  96304 MiB |\n",
            "|       from small pool |    240 MiB |    314 MiB |   2572 MiB |   2332 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 549731 KiB |   2310 MiB |   5168 GiB |   5167 GiB |\n",
            "|       from large pool | 548816 KiB |   2291 MiB |   5121 GiB |   5120 GiB |\n",
            "|       from small pool |    914 KiB |     21 MiB |     47 GiB |     47 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1058    |    1082    |  301210    |  300152    |\n",
            "|       from large pool |     256    |     257    |  107657    |  107401    |\n",
            "|       from small pool |     802    |     955    |  193553    |  192751    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1058    |    1082    |  301210    |  300152    |\n",
            "|       from large pool |     256    |     257    |  107657    |  107401    |\n",
            "|       from small pool |     802    |     955    |  193553    |  192751    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     204    |     245    |    1748    |    1544    |\n",
            "|       from large pool |      84    |      88    |     462    |     378    |\n",
            "|       from small pool |     120    |     157    |    1286    |    1166    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      91    |      92    |  133617    |  133526    |\n",
            "|       from large pool |      67    |      68    |   53573    |   53506    |\n",
            "|       from small pool |      24    |      59    |   80044    |   80020    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:23:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  95% 52/55 [07:03<00:30, 10.25s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:25:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 285.06 MiB is free. Process 359607 has 14.47 GiB memory in use. Of the allocated memory 12.58 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:25:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 30           |        cudaMalloc retries: 55        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12878 MiB |  12878 MiB |   6580 GiB |   6568 GiB |\n",
            "|       from large pool |  12648 MiB |  12648 MiB |   6531 GiB |   6518 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12878 MiB |  12878 MiB |   6580 GiB |   6568 GiB |\n",
            "|       from large pool |  12648 MiB |  12648 MiB |   6531 GiB |   6518 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12864 MiB |  12864 MiB |   6575 GiB |   6562 GiB |\n",
            "|       from large pool |  12635 MiB |  12635 MiB |   6525 GiB |   6513 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14680 MiB |  14760 MiB | 113870 MiB |  99190 MiB |\n",
            "|       from large pool |  14444 MiB |  14444 MiB | 111222 MiB |  96778 MiB |\n",
            "|       from small pool |    236 MiB |    316 MiB |   2648 MiB |   2412 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1801 MiB |   3820 MiB |   5527 GiB |   5525 GiB |\n",
            "|       from large pool |   1795 MiB |   3801 MiB |   5476 GiB |   5474 GiB |\n",
            "|       from small pool |      6 MiB |     22 MiB |     50 GiB |     50 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1012    |    1082    |  325543    |  324531    |\n",
            "|       from large pool |     233    |     233    |  115716    |  115483    |\n",
            "|       from small pool |     779    |     955    |  209827    |  209048    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1012    |    1082    |  325543    |  324531    |\n",
            "|       from large pool |     233    |     233    |  115716    |  115483    |\n",
            "|       from small pool |     779    |     955    |  209827    |  209048    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     202    |     242    |    1787    |    1585    |\n",
            "|       from large pool |      84    |      84    |     463    |     379    |\n",
            "|       from small pool |     118    |     158    |    1324    |    1206    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |     103    |     103    |  144726    |  144623    |\n",
            "|       from large pool |      76    |      76    |   57966    |   57890    |\n",
            "|       from small pool |      27    |      55    |   86760    |   86733    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:25:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  96% 53/55 [07:04<00:14,  7.42s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:25:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 41.06 MiB is free. Process 359607 has 14.71 GiB memory in use. Of the allocated memory 13.40 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:25:21 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 31           |        cudaMalloc retries: 56        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13121 MiB |  13719 MiB |   6603 GiB |   6590 GiB |\n",
            "|       from large pool |  12889 MiB |  13487 MiB |   6553 GiB |   6540 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13121 MiB |  13719 MiB |   6603 GiB |   6590 GiB |\n",
            "|       from large pool |  12889 MiB |  13487 MiB |   6553 GiB |   6540 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13103 MiB |  13701 MiB |   6597 GiB |   6585 GiB |\n",
            "|       from large pool |  12871 MiB |  13469 MiB |   6548 GiB |   6535 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14924 MiB |  14924 MiB | 114114 MiB |  99190 MiB |\n",
            "|       from large pool |  14688 MiB |  14688 MiB | 111466 MiB |  96778 MiB |\n",
            "|       from small pool |    236 MiB |    316 MiB |   2648 MiB |   2412 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1084 MiB |   3820 MiB |   5543 GiB |   5542 GiB |\n",
            "|       from large pool |   1080 MiB |   3801 MiB |   5492 GiB |   5491 GiB |\n",
            "|       from small pool |      4 MiB |     22 MiB |     51 GiB |     51 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  326146    |  325016    |\n",
            "|       from large pool |     312    |     325    |  116074    |  115762    |\n",
            "|       from small pool |     818    |     955    |  210072    |  209254    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  326146    |  325016    |\n",
            "|       from large pool |     312    |     325    |  116074    |  115762    |\n",
            "|       from small pool |     818    |     955    |  210072    |  209254    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     203    |     242    |    1788    |    1585    |\n",
            "|       from large pool |      85    |      85    |     464    |     379    |\n",
            "|       from small pool |     118    |     158    |    1324    |    1206    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      99    |     104    |  144965    |  144866    |\n",
            "|       from large pool |      74    |      77    |   58135    |   58061    |\n",
            "|       from small pool |      25    |      55    |   86830    |   86805    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:25:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  98% 54/55 [07:05<00:05,  5.52s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=131, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=934, gb_free=6.5, wall=1094]2024-10-21 16:25:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 1/19 [00:01<00:18,  1.05s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 2/19 [00:02<00:18,  1.06s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  16% 3/19 [00:03<00:17,  1.07s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 4/19 [00:04<00:16,  1.11s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 5/19 [00:05<00:14,  1.06s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 6/19 [00:06<00:14,  1.12s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  37% 7/19 [00:07<00:11,  1.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 8/19 [00:08<00:10,  1.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 9/19 [00:09<00:11,  1.12s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 10/19 [00:11<00:11,  1.31s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 11/19 [00:13<00:12,  1.61s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  63% 12/19 [00:15<00:12,  1.73s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 13/19 [00:20<00:15,  2.62s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 14/19 [00:24<00:14,  2.93s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 15/19 [00:28<00:13,  3.27s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  84% 16/19 [00:31<00:10,  3.44s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 17/19 [00:37<00:07,  3.94s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 18/19 [00:42<00:04,  4.33s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 19/19 [00:46<00:00,  4.19s/it]\u001b[A\n",
            "                                                                        \u001b[A2024-10-21 16:26:26 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.077 | nll_loss 5.364 | ppl 41.18 | wps 532.4 | wpb 1758.4 | bsz 14.4 | num_updates 134 | best_loss 6.077\n",
            "2024-10-21 16:26:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 134 updates\n",
            "2024-10-21 16:26:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/wordwise_transformer/checkpoint3.pt\n",
            "2024-10-21 16:26:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/wordwise_transformer/checkpoint3.pt\n",
            "2024-10-21 16:26:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/wordwise_transformer/checkpoint3.pt (epoch 3 @ 134 updates, score 6.077) (writing took 24.281134548999944 seconds)\n",
            "2024-10-21 16:26:50 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-10-21 16:26:50 | INFO | train | epoch 003 | loss 6.347 | nll_loss 5.684 | ppl 51.41 | wps 115 | ups 0.09 | wpb 1342.8 | bsz 11.2 | num_updates 134 | lr 1.675e-05 | gnorm 4.315 | clip 100 | train_wall 429 | gb_free 1.4 | wall 1534\n",
            "2024-10-21 16:26:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 55\n",
            "epoch 004:   0% 0/55 [00:00<?, ?it/s]2024-10-21 16:26:50 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-10-21 16:26:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  18% 10/55 [01:45<09:23, 12.53s/it]2024-10-21 16:28:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 95.06 MiB is free. Process 359607 has 14.65 GiB memory in use. Of the allocated memory 13.83 GiB is allocated by PyTorch, and 706.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:28:37 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 32           |        cudaMalloc retries: 58        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14163 MiB |  14164 MiB |   7347 GiB |   7333 GiB |\n",
            "|       from large pool |  13930 MiB |  13930 MiB |   7291 GiB |   7278 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     55 GiB |     55 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14163 MiB |  14164 MiB |   7347 GiB |   7333 GiB |\n",
            "|       from large pool |  13930 MiB |  13930 MiB |   7291 GiB |   7278 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     55 GiB |     55 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14150 MiB |  14151 MiB |   7341 GiB |   7327 GiB |\n",
            "|       from large pool |  13917 MiB |  13917 MiB |   7285 GiB |   7272 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     55 GiB |     55 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14870 MiB |  14948 MiB | 123316 MiB | 108446 MiB |\n",
            "|       from large pool |  14634 MiB |  14634 MiB | 120588 MiB | 105954 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |   2728 MiB |   2492 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 723568 KiB |   3990 MiB |   6099 GiB |   6099 GiB |\n",
            "|       from large pool | 720097 KiB |   3970 MiB |   6042 GiB |   6041 GiB |\n",
            "|       from small pool |   3471 KiB |     22 MiB |     57 GiB |     57 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1066    |    1082    |  360106    |  359040    |\n",
            "|       from large pool |     271    |     271    |  128340    |  128069    |\n",
            "|       from small pool |     795    |     955    |  231766    |  230971    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1066    |    1082    |  360106    |  359040    |\n",
            "|       from large pool |     271    |     271    |  128340    |  128069    |\n",
            "|       from small pool |     795    |     955    |  231766    |  230971    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     222    |    1860    |    1677    |\n",
            "|       from large pool |      65    |      65    |     496    |     431    |\n",
            "|       from small pool |     118    |     157    |    1364    |    1246    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      79    |      97    |  161253    |  161174    |\n",
            "|       from large pool |      55    |      55    |   64409    |   64354    |\n",
            "|       from small pool |      24    |      59    |   96844    |   96820    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:28:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  25% 14/55 [02:11<06:39,  9.75s/it]2024-10-21 16:29:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 370.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 89.06 MiB is free. Process 359607 has 14.66 GiB memory in use. Of the allocated memory 13.33 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:29:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 33           |        cudaMalloc retries: 60        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12810 MiB |  13650 MiB |   7498 GiB |   7485 GiB |\n",
            "|       from large pool |  12579 MiB |  13419 MiB |   7441 GiB |   7428 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12810 MiB |  13650 MiB |   7498 GiB |   7485 GiB |\n",
            "|       from large pool |  12579 MiB |  13419 MiB |   7441 GiB |   7428 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12794 MiB |  13634 MiB |   7491 GiB |   7479 GiB |\n",
            "|       from large pool |  12564 MiB |  13403 MiB |   7435 GiB |   7422 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14876 MiB |  14948 MiB | 124876 MiB | 110000 MiB |\n",
            "|       from large pool |  14644 MiB |  14644 MiB | 122068 MiB | 107424 MiB |\n",
            "|       from small pool |    232 MiB |    314 MiB |   2808 MiB |   2576 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1325 MiB |   4202 MiB |   6228 GiB |   6226 GiB |\n",
            "|       from large pool |   1324 MiB |   4186 MiB |   6169 GiB |   6168 GiB |\n",
            "|       from small pool |      1 MiB |     19 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  367329    |  366199    |\n",
            "|       from large pool |     312    |     325    |  130987    |  130675    |\n",
            "|       from small pool |     818    |     955    |  236342    |  235524    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  367329    |  366199    |\n",
            "|       from large pool |     312    |     325    |  130987    |  130675    |\n",
            "|       from small pool |     818    |     955    |  236342    |  235524    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     180    |     222    |    1904    |    1724    |\n",
            "|       from large pool |      64    |      65    |     500    |     436    |\n",
            "|       from small pool |     116    |     157    |    1404    |    1288    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      80    |  164308    |  164244    |\n",
            "|       from large pool |      50    |      51    |   65627    |   65577    |\n",
            "|       from small pool |      14    |      46    |   98681    |   98667    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:29:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  27% 15/55 [02:13<04:50,  7.26s/it]2024-10-21 16:29:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 19.06 MiB is free. Process 359607 has 14.73 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 722.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:29:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 34           |        cudaMalloc retries: 62        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13858 MiB |  14430 MiB |   7521 GiB |   7507 GiB |\n",
            "|       from large pool |  13625 MiB |  14199 MiB |   7464 GiB |   7451 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     57 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13858 MiB |  14430 MiB |   7521 GiB |   7507 GiB |\n",
            "|       from large pool |  13625 MiB |  14199 MiB |   7464 GiB |   7451 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     57 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13843 MiB |  14416 MiB |   7515 GiB |   7501 GiB |\n",
            "|       from large pool |  13610 MiB |  14184 MiB |   7458 GiB |   7445 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14946 MiB |  14948 MiB | 125880 MiB | 110934 MiB |\n",
            "|       from large pool |  14712 MiB |  14712 MiB | 123068 MiB | 108356 MiB |\n",
            "|       from small pool |    234 MiB |    314 MiB |   2812 MiB |   2578 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    837 MiB |   4202 MiB |   6246 GiB |   6246 GiB |\n",
            "|       from large pool |    836 MiB |   4186 MiB |   6188 GiB |   6187 GiB |\n",
            "|       from small pool |      1 MiB |     19 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  367925    |  366795    |\n",
            "|       from large pool |     312    |     325    |  131338    |  131026    |\n",
            "|       from small pool |     818    |     955    |  236587    |  235769    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  367925    |  366795    |\n",
            "|       from large pool |     312    |     325    |  131338    |  131026    |\n",
            "|       from small pool |     818    |     955    |  236587    |  235769    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     181    |     222    |    1910    |    1729    |\n",
            "|       from large pool |      64    |      65    |     504    |     440    |\n",
            "|       from small pool |     117    |     157    |    1406    |    1289    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      71    |      80    |  164573    |  164502    |\n",
            "|       from large pool |      50    |      53    |   65785    |   65735    |\n",
            "|       from small pool |      21    |      46    |   98788    |   98767    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:29:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  31% 17/55 [02:21<03:43,  5.88s/it]2024-10-21 16:29:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 474.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 419.06 MiB is free. Process 359607 has 14.34 GiB memory in use. Of the allocated memory 13.15 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:29:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 35           |        cudaMalloc retries: 64        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13462 MiB |  13462 MiB |   7595 GiB |   7582 GiB |\n",
            "|       from large pool |  13223 MiB |  13223 MiB |   7538 GiB |   7525 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     57 GiB |     57 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13462 MiB |  13462 MiB |   7595 GiB |   7582 GiB |\n",
            "|       from large pool |  13223 MiB |  13223 MiB |   7538 GiB |   7525 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     57 GiB |     57 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13452 MiB |  13452 MiB |   7589 GiB |   7576 GiB |\n",
            "|       from large pool |  13213 MiB |  13213 MiB |   7532 GiB |   7519 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     57 GiB |     57 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14546 MiB |  14608 MiB | 128282 MiB | 113736 MiB |\n",
            "|       from large pool |  14306 MiB |  14306 MiB | 125398 MiB | 111092 MiB |\n",
            "|       from small pool |    240 MiB |    302 MiB |   2884 MiB |   2644 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1083 MiB |   4180 MiB |   6301 GiB |   6300 GiB |\n",
            "|       from large pool |   1082 MiB |   4173 MiB |   6242 GiB |   6241 GiB |\n",
            "|       from small pool |      0 MiB |      7 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1057    |    1082    |  370565    |  369508    |\n",
            "|       from large pool |     255    |     255    |  132474    |  132219    |\n",
            "|       from small pool |     802    |     955    |  238091    |  237289    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1057    |    1082    |  370565    |  369508    |\n",
            "|       from large pool |     255    |     255    |  132474    |  132219    |\n",
            "|       from small pool |     802    |     955    |  238091    |  237289    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     180    |     211    |    1951    |    1771    |\n",
            "|       from large pool |      60    |      60    |     509    |     449    |\n",
            "|       from small pool |     120    |     151    |    1442    |    1322    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |      79    |  165779    |  165702    |\n",
            "|       from large pool |      53    |      54    |   66398    |   66345    |\n",
            "|       from small pool |      24    |      36    |   99381    |   99357    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:29:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  42% 23/55 [03:28<06:01, 11.29s/it]2024-10-21 16:30:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 167.06 MiB is free. Process 359607 has 14.58 GiB memory in use. Of the allocated memory 11.93 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:30:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 36           |        cudaMalloc retries: 65        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  11481 MiB |  12214 MiB |   7805 GiB |   7794 GiB |\n",
            "|       from large pool |  11236 MiB |  11968 MiB |   7746 GiB |   7735 GiB |\n",
            "|       from small pool |    245 MiB |    295 MiB |     59 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  11481 MiB |  12214 MiB |   7805 GiB |   7794 GiB |\n",
            "|       from large pool |  11236 MiB |  11968 MiB |   7746 GiB |   7735 GiB |\n",
            "|       from small pool |    245 MiB |    295 MiB |     59 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  11466 MiB |  12198 MiB |   7799 GiB |   7787 GiB |\n",
            "|       from large pool |  11221 MiB |  11953 MiB |   7739 GiB |   7729 GiB |\n",
            "|       from small pool |    245 MiB |    295 MiB |     59 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14798 MiB |  14868 MiB | 128604 MiB | 113806 MiB |\n",
            "|       from large pool |  14550 MiB |  14550 MiB | 125642 MiB | 111092 MiB |\n",
            "|       from small pool |    248 MiB |    318 MiB |   2962 MiB |   2714 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2140 MiB |   2947 MiB |   6497 GiB |   6495 GiB |\n",
            "|       from large pool |   2137 MiB |   2934 MiB |   6436 GiB |   6434 GiB |\n",
            "|       from small pool |      2 MiB |     18 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  381744    |  380614    |\n",
            "|       from large pool |     295    |     306    |  135930    |  135635    |\n",
            "|       from small pool |     835    |     955    |  245814    |  244979    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  381744    |  380614    |\n",
            "|       from large pool |     295    |     306    |  135930    |  135635    |\n",
            "|       from small pool |     835    |     955    |  245814    |  244979    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     185    |     220    |    1991    |    1806    |\n",
            "|       from large pool |      61    |      61    |     510    |     449    |\n",
            "|       from small pool |     124    |     159    |    1481    |    1357    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |      79    |  170806    |  170729    |\n",
            "|       from large pool |      52    |      52    |   68055    |   68003    |\n",
            "|       from small pool |      25    |      47    |  102751    |  102726    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:30:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  45% 25/55 [03:34<03:41,  7.37s/it]2024-10-21 16:30:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 403.06 MiB is free. Process 359607 has 14.35 GiB memory in use. Of the allocated memory 12.63 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:30:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 37           |        cudaMalloc retries: 67        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12932 MiB |  12932 MiB |   7865 GiB |   7852 GiB |\n",
            "|       from large pool |  12703 MiB |  12703 MiB |   7805 GiB |   7793 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12932 MiB |  12932 MiB |   7865 GiB |   7852 GiB |\n",
            "|       from large pool |  12703 MiB |  12703 MiB |   7805 GiB |   7793 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12913 MiB |  12913 MiB |   7858 GiB |   7846 GiB |\n",
            "|       from large pool |  12684 MiB |  12684 MiB |   7799 GiB |   7786 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14562 MiB |  14562 MiB | 132500 MiB | 117938 MiB |\n",
            "|       from large pool |  14330 MiB |  14330 MiB | 129482 MiB | 115152 MiB |\n",
            "|       from small pool |    232 MiB |    302 MiB |   3018 MiB |   2786 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1629 MiB |   2541 MiB |   6546 GiB |   6545 GiB |\n",
            "|       from large pool |   1626 MiB |   2534 MiB |   6485 GiB |   6484 GiB |\n",
            "|       from small pool |      3 MiB |      8 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1068    |    1082    |  384398    |  383330    |\n",
            "|       from large pool |     273    |     273    |  137084    |  136811    |\n",
            "|       from small pool |     795    |     955    |  247314    |  246519    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1068    |    1082    |  384398    |  383330    |\n",
            "|       from large pool |     273    |     273    |  137084    |  136811    |\n",
            "|       from small pool |     795    |     955    |  247314    |  246519    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     174    |     211    |    2024    |    1850    |\n",
            "|       from large pool |      58    |      60    |     515    |     457    |\n",
            "|       from small pool |     116    |     151    |    1509    |    1393    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      70    |  172011    |  171950    |\n",
            "|       from large pool |      36    |      45    |   68647    |   68611    |\n",
            "|       from small pool |      25    |      38    |  103364    |  103339    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:30:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  47% 26/55 [03:37<02:51,  5.92s/it]2024-10-21 16:30:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 69.06 MiB is free. Process 359607 has 14.68 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 568.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:30:28 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 38           |        cudaMalloc retries: 68        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13554 MiB |  14327 MiB |   7889 GiB |   7875 GiB |\n",
            "|       from large pool |  13323 MiB |  14095 MiB |   7829 GiB |   7816 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13554 MiB |  14327 MiB |   7889 GiB |   7875 GiB |\n",
            "|       from large pool |  13323 MiB |  14095 MiB |   7829 GiB |   7816 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13541 MiB |  14313 MiB |   7882 GiB |   7869 GiB |\n",
            "|       from large pool |  13310 MiB |  14081 MiB |   7823 GiB |   7810 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14896 MiB |  14896 MiB | 132834 MiB | 117938 MiB |\n",
            "|       from large pool |  14662 MiB |  14662 MiB | 129814 MiB | 115152 MiB |\n",
            "|       from small pool |    234 MiB |    302 MiB |   3020 MiB |   2786 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1009 MiB |   2563 MiB |   6567 GiB |   6566 GiB |\n",
            "|       from large pool |   1006 MiB |   2558 MiB |   6506 GiB |   6505 GiB |\n",
            "|       from small pool |      2 MiB |      8 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  385001    |  383871    |\n",
            "|       from large pool |     312    |     325    |  137442    |  137130    |\n",
            "|       from small pool |     818    |     955    |  247559    |  246741    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  385001    |  383871    |\n",
            "|       from large pool |     312    |     325    |  137442    |  137130    |\n",
            "|       from small pool |     818    |     955    |  247559    |  246741    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     176    |     211    |    2026    |    1850    |\n",
            "|       from large pool |      59    |      60    |     516    |     457    |\n",
            "|       from small pool |     117    |     151    |    1510    |    1393    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |      81    |  172232    |  172155    |\n",
            "|       from large pool |      51    |      52    |   68791    |   68740    |\n",
            "|       from small pool |      26    |      38    |  103441    |  103415    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:30:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  53% 29/55 [04:06<03:39,  8.43s/it]2024-10-21 16:30:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 17.06 MiB is free. Process 359607 has 14.73 GiB memory in use. Of the allocated memory 12.61 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:30:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 39           |        cudaMalloc retries: 70        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12435 MiB |  13076 MiB |   7987 GiB |   7975 GiB |\n",
            "|       from large pool |  12205 MiB |  12846 MiB |   7927 GiB |   7915 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12435 MiB |  13076 MiB |   7987 GiB |   7975 GiB |\n",
            "|       from large pool |  12205 MiB |  12846 MiB |   7927 GiB |   7915 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12419 MiB |  13060 MiB |   7981 GiB |   7969 GiB |\n",
            "|       from large pool |  12189 MiB |  12831 MiB |   7920 GiB |   7908 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14948 MiB |  14948 MiB | 133308 MiB | 118360 MiB |\n",
            "|       from large pool |  14716 MiB |  14716 MiB | 130200 MiB | 115484 MiB |\n",
            "|       from small pool |    232 MiB |    318 MiB |   3108 MiB |   2876 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2126 MiB |   2691 MiB |   6666 GiB |   6664 GiB |\n",
            "|       from large pool |   2124 MiB |   2677 MiB |   6604 GiB |   6601 GiB |\n",
            "|       from small pool |      1 MiB |     22 MiB |     62 GiB |     62 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  389983    |  388853    |\n",
            "|       from large pool |     312    |     325    |  139185    |  138873    |\n",
            "|       from small pool |     818    |     955    |  250798    |  249980    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  389983    |  388853    |\n",
            "|       from large pool |     312    |     325    |  139185    |  138873    |\n",
            "|       from small pool |     818    |     955    |  250798    |  249980    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     175    |     217    |    2071    |    1896    |\n",
            "|       from large pool |      59    |      59    |     517    |     458    |\n",
            "|       from small pool |     116    |     159    |    1554    |    1438    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      72    |  174401    |  174331    |\n",
            "|       from large pool |      49    |      50    |   69639    |   69590    |\n",
            "|       from small pool |      21    |      43    |  104762    |  104741    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:30:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  58% 32/55 [04:17<02:02,  5.31s/it]2024-10-21 16:31:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 203.06 MiB is free. Process 359607 has 14.55 GiB memory in use. Of the allocated memory 12.12 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:31:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 40           |        cudaMalloc retries: 72        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12408 MiB |  12408 MiB |   8085 GiB |   8072 GiB |\n",
            "|       from large pool |  12179 MiB |  12179 MiB |   8023 GiB |   8012 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12408 MiB |  12408 MiB |   8085 GiB |   8072 GiB |\n",
            "|       from large pool |  12179 MiB |  12179 MiB |   8023 GiB |   8012 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12392 MiB |  12392 MiB |   8078 GiB |   8066 GiB |\n",
            "|       from large pool |  12162 MiB |  12162 MiB |   8017 GiB |   8005 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14762 MiB |  14762 MiB | 134158 MiB | 119396 MiB |\n",
            "|       from large pool |  14530 MiB |  14530 MiB | 130976 MiB | 116446 MiB |\n",
            "|       from small pool |    232 MiB |    304 MiB |   3182 MiB |   2950 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2353 MiB |   4421 MiB |   6756 GiB |   6753 GiB |\n",
            "|       from large pool |   2350 MiB |   4412 MiB |   6693 GiB |   6691 GiB |\n",
            "|       from small pool |      2 MiB |     10 MiB |     62 GiB |     62 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1068    |    1082    |  394881    |  393813    |\n",
            "|       from large pool |     273    |     273    |  141223    |  140950    |\n",
            "|       from small pool |     795    |     955    |  253658    |  252863    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1068    |    1082    |  394881    |  393813    |\n",
            "|       from large pool |     273    |     273    |  141223    |  140950    |\n",
            "|       from small pool |     795    |     955    |  253658    |  252863    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     170    |     210    |    2110    |    1940    |\n",
            "|       from large pool |      54    |      58    |     519    |     465    |\n",
            "|       from small pool |     116    |     152    |    1591    |    1475    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      62    |  176555    |  176498    |\n",
            "|       from large pool |      37    |      42    |   70612    |   70575    |\n",
            "|       from small pool |      20    |      29    |  105943    |  105923    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:31:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  62% 34/55 [04:35<02:50,  8.11s/it]2024-10-21 16:31:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 203.06 MiB is free. Process 359607 has 14.55 GiB memory in use. Of the allocated memory 12.58 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:31:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 41           |        cudaMalloc retries: 73        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12877 MiB |  12877 MiB |   8137 GiB |   8125 GiB |\n",
            "|       from large pool |  12647 MiB |  12647 MiB |   8076 GiB |   8063 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12877 MiB |  12877 MiB |   8137 GiB |   8125 GiB |\n",
            "|       from large pool |  12647 MiB |  12647 MiB |   8076 GiB |   8063 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12864 MiB |  12864 MiB |   8130 GiB |   8118 GiB |\n",
            "|       from large pool |  12635 MiB |  12635 MiB |   8069 GiB |   8057 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14762 MiB |  14842 MiB | 134238 MiB | 119476 MiB |\n",
            "|       from large pool |  14530 MiB |  14530 MiB | 130976 MiB | 116446 MiB |\n",
            "|       from small pool |    232 MiB |    312 MiB |   3262 MiB |   3030 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1884 MiB |   3890 MiB |   6811 GiB |   6809 GiB |\n",
            "|       from large pool |   1882 MiB |   3873 MiB |   6748 GiB |   6746 GiB |\n",
            "|       from small pool |      2 MiB |     18 MiB |     63 GiB |     63 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1012    |    1082    |  397335    |  396323    |\n",
            "|       from large pool |     233    |     233    |  141924    |  141691    |\n",
            "|       from small pool |     779    |     955    |  255411    |  254632    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1012    |    1082    |  397335    |  396323    |\n",
            "|       from large pool |     233    |     233    |  141924    |  141691    |\n",
            "|       from small pool |     779    |     955    |  255411    |  254632    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     170    |     210    |    2150    |    1980    |\n",
            "|       from large pool |      54    |      54    |     519    |     465    |\n",
            "|       from small pool |     116    |     156    |    1631    |    1515    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      74    |      81    |  177744    |  177670    |\n",
            "|       from large pool |      51    |      52    |   71001    |   70950    |\n",
            "|       from small pool |      23    |      45    |  106743    |  106720    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:31:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  73% 40/55 [05:33<03:09, 12.62s/it]2024-10-21 16:32:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 203.06 MiB is free. Process 359607 has 14.55 GiB memory in use. Of the allocated memory 13.50 GiB is allocated by PyTorch, and 934.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:32:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 42           |        cudaMalloc retries: 74        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13398 MiB |  14017 MiB |   8357 GiB |   8344 GiB |\n",
            "|       from large pool |  13167 MiB |  13787 MiB |   8293 GiB |   8280 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     63 GiB |     63 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13398 MiB |  14017 MiB |   8357 GiB |   8344 GiB |\n",
            "|       from large pool |  13167 MiB |  13787 MiB |   8293 GiB |   8280 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     63 GiB |     63 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13384 MiB |  14003 MiB |   8350 GiB |   8337 GiB |\n",
            "|       from large pool |  13153 MiB |  13773 MiB |   8287 GiB |   8274 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     63 GiB |     63 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14762 MiB |  14842 MiB | 134318 MiB | 119556 MiB |\n",
            "|       from large pool |  14530 MiB |  14530 MiB | 130976 MiB | 116446 MiB |\n",
            "|       from small pool |    232 MiB |    312 MiB |   3342 MiB |   3110 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1363 MiB |   3017 MiB |   7043 GiB |   7042 GiB |\n",
            "|       from large pool |   1362 MiB |   3005 MiB |   6978 GiB |   6977 GiB |\n",
            "|       from small pool |      0 MiB |     13 MiB |     64 GiB |     64 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  408833    |  407703    |\n",
            "|       from large pool |     312    |     325    |  145612    |  145300    |\n",
            "|       from small pool |     818    |     955    |  263221    |  262403    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  408833    |  407703    |\n",
            "|       from large pool |     312    |     325    |  145612    |  145300    |\n",
            "|       from small pool |     818    |     955    |  263221    |  262403    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     170    |     210    |    2190    |    2020    |\n",
            "|       from large pool |      54    |      54    |     519    |     465    |\n",
            "|       from small pool |     116    |     156    |    1671    |    1555    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      67    |  182679    |  182616    |\n",
            "|       from large pool |      44    |      45    |   72806    |   72762    |\n",
            "|       from small pool |      19    |      33    |  109873    |  109854    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:32:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  98% 54/55 [07:14<00:08,  8.74s/it]2024-10-21 16:34:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 1/19 [00:01<00:18,  1.05s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 2/19 [00:02<00:18,  1.07s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  16% 3/19 [00:03<00:17,  1.07s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 4/19 [00:04<00:16,  1.11s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 5/19 [00:05<00:14,  1.07s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 6/19 [00:06<00:14,  1.12s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  37% 7/19 [00:07<00:11,  1.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 8/19 [00:08<00:10,  1.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 9/19 [00:09<00:11,  1.12s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 10/19 [00:11<00:11,  1.31s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 11/19 [00:13<00:12,  1.58s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  63% 12/19 [00:15<00:11,  1.71s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 13/19 [00:20<00:15,  2.60s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  74% 14/19 [00:23<00:14,  2.92s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 15/19 [00:28<00:13,  3.27s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  84% 16/19 [00:31<00:10,  3.45s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 17/19 [00:36<00:07,  3.95s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 18/19 [00:42<00:04,  4.34s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 19/19 [00:46<00:00,  4.19s/it]\u001b[A\n",
            "                                                                        \u001b[A2024-10-21 16:34:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.377 | nll_loss 4.528 | ppl 23.07 | wps 532.9 | wpb 1758.4 | bsz 14.4 | num_updates 178 | best_loss 5.377\n",
            "2024-10-21 16:34:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 178 updates\n",
            "2024-10-21 16:34:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/wordwise_transformer/checkpoint4.pt\n",
            "2024-10-21 16:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/wordwise_transformer/checkpoint4.pt\n",
            "2024-10-21 16:35:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/wordwise_transformer/checkpoint4.pt (epoch 4 @ 178 updates, score 5.377) (writing took 21.361756724998486 seconds)\n",
            "2024-10-21 16:35:19 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-10-21 16:35:19 | INFO | train | epoch 004 | loss 5.573 | nll_loss 4.779 | ppl 27.46 | wps 121 | ups 0.09 | wpb 1399.2 | bsz 12.3 | num_updates 178 | lr 2.225e-05 | gnorm 4.754 | clip 100 | train_wall 426 | gb_free 3.8 | wall 2043\n",
            "2024-10-21 16:35:19 | INFO | fairseq_cli.train | done training in 2042.8 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# positional model training"
      ],
      "metadata": {
        "id": "NPeM3uBu-e8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train data-bin/positional \\\n",
        "    --arch transformer --share-decoder-input-output-embed \\\n",
        "    --encoder-layers 6 --decoder-layers 6 \\\n",
        "    --max-tokens 50000 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.1 --dropout 0.1 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 4 --save-dir checkpoints/positional_transformer\\\n",
        "    --max-source-positions 42105 --max-target-positions 573 \\\n",
        "    --skip-invalid-size-inputs-valid-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d0Yk2pOG8OLS",
        "outputId": "82827064-8ceb-4e2e-eadf-12d2b250d527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-21 16:48:59.956321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-21 16:48:59.990349: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-21 16:49:00.000330: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-21 16:49:00.022832: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-21 16:49:01.726510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-10-21 16:49:03 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-10-21 16:49:04 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-10-21 16:49:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 50000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 50000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 4, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/positional_transformer', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=50000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=50000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=4, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/positional_transformer', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/positional', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, encoder_layers=6, decoder_layers=6, dropout=0.1, max_source_positions=42105, max_target_positions=573, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/positional', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 42105, 'max_target_positions': 573, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-10-21 16:49:06 | INFO | fairseq.tasks.translation | [src] dictionary: 63256 types\n",
            "2024-10-21 16:49:06 | INFO | fairseq.tasks.translation | [tgt] dictionary: 1544 types\n",
            "2024-10-21 16:49:07 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(63256, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(1544, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=1544, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-10-21 16:49:07 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-10-21 16:49:07 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-10-21 16:49:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-10-21 16:49:07 | INFO | fairseq_cli.train | num. shared model params: 96,218,112 (num. trained: 96,218,112)\n",
            "2024-10-21 16:49:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-10-21 16:49:07 | INFO | fairseq.data.data_utils | loaded 275 examples from: data-bin/positional/valid.src-tgt.src\n",
            "2024-10-21 16:49:07 | INFO | fairseq.data.data_utils | loaded 275 examples from: data-bin/positional/valid.src-tgt.tgt\n",
            "2024-10-21 16:49:07 | INFO | fairseq.tasks.translation | data-bin/positional valid src-tgt 275 examples\n",
            "2024-10-21 16:49:07 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-10-21 16:49:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-10-21 16:49:07 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-10-21 16:49:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-10-21 16:49:07 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-10-21 16:49:07 | INFO | fairseq_cli.train | max tokens per device = 50000 and max sentences per device = None\n",
            "2024-10-21 16:49:07 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/positional_transformer/checkpoint_last.pt\n",
            "2024-10-21 16:49:07 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/positional_transformer/checkpoint_last.pt\n",
            "2024-10-21 16:49:07 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-10-21 16:49:07 | INFO | fairseq.data.data_utils | loaded 1,100 examples from: data-bin/positional/train.src-tgt.src\n",
            "2024-10-21 16:49:07 | INFO | fairseq.data.data_utils | loaded 1,100 examples from: data-bin/positional/train.src-tgt.tgt\n",
            "2024-10-21 16:49:07 | INFO | fairseq.tasks.translation | data-bin/positional train src-tgt 1100 examples\n",
            "2024-10-21 16:49:07 | WARNING | fairseq.tasks.fairseq_task | 2 samples have invalid sizes and will be skipped, max_positions=(42105, 573), first few sample ids=[55, 262]\n",
            "2024-10-21 16:49:07 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2024-10-21 16:49:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 55\n",
            "epoch 001:   0% 0/55 [00:00<?, ?it/s]2024-10-21 16:49:08 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-10-21 16:49:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:   9% 5/55 [00:55<10:18, 12.37s/it]2024-10-21 16:50:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 390.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 283.06 MiB is free. Process 530225 has 14.47 GiB memory in use. Of the allocated memory 12.65 GiB is allocated by PyTorch, and 1.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:50:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12950 MiB |  12951 MiB | 208966 MiB | 196016 MiB |\n",
            "|       from large pool |  12711 MiB |  12711 MiB | 206929 MiB | 194217 MiB |\n",
            "|       from small pool |    239 MiB |    296 MiB |   2037 MiB |   1798 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12950 MiB |  12951 MiB | 208966 MiB | 196016 MiB |\n",
            "|       from large pool |  12711 MiB |  12711 MiB | 206929 MiB | 194217 MiB |\n",
            "|       from small pool |    239 MiB |    296 MiB |   2037 MiB |   1798 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12939 MiB |  12940 MiB | 208615 MiB | 195675 MiB |\n",
            "|       from large pool |  12700 MiB |  12700 MiB | 206579 MiB | 193878 MiB |\n",
            "|       from small pool |    239 MiB |    296 MiB |   2036 MiB |   1797 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14682 MiB |  14754 MiB |  25918 MiB |  11236 MiB |\n",
            "|       from large pool |  14440 MiB |  14440 MiB |  25604 MiB |  11164 MiB |\n",
            "|       from small pool |    242 MiB |    314 MiB |    314 MiB |     72 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1731 MiB |   3659 MiB | 140553 MiB | 138822 MiB |\n",
            "|       from large pool |   1728 MiB |   3647 MiB | 138555 MiB | 136826 MiB |\n",
            "|       from small pool |      2 MiB |     15 MiB |   1998 MiB |   1995 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1043    |    1078    |   12077    |   11034    |\n",
            "|       from large pool |     247    |     247    |    3598    |    3351    |\n",
            "|       from small pool |     796    |     952    |    8479    |    7683    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1043    |    1078    |   12077    |   11034    |\n",
            "|       from large pool |     247    |     247    |    3598    |    3351    |\n",
            "|       from small pool |     796    |     952    |    8479    |    7683    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     197    |     233    |     295    |      98    |\n",
            "|       from large pool |      76    |      76    |     138    |      62    |\n",
            "|       from small pool |     121    |     157    |     157    |      36    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      93    |      93    |    4865    |    4772    |\n",
            "|       from large pool |      69    |      70    |    1582    |    1513    |\n",
            "|       from small pool |      24    |      38    |    3283    |    3259    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:50:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  13% 7/55 [01:00<05:44,  7.18s/it]2024-10-21 16:50:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 303.06 MiB is free. Process 530225 has 14.45 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:50:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 2            |        cudaMalloc retries: 3         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12502 MiB |  12503 MiB | 267334 MiB | 254831 MiB |\n",
            "|       from large pool |  12272 MiB |  12272 MiB | 264991 MiB | 252719 MiB |\n",
            "|       from small pool |    230 MiB |    296 MiB |   2342 MiB |   2112 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12502 MiB |  12503 MiB | 267334 MiB | 254831 MiB |\n",
            "|       from large pool |  12272 MiB |  12272 MiB | 264991 MiB | 252719 MiB |\n",
            "|       from small pool |    230 MiB |    296 MiB |   2342 MiB |   2112 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12487 MiB |  12488 MiB | 266941 MiB | 254454 MiB |\n",
            "|       from large pool |  12256 MiB |  12256 MiB | 264600 MiB | 252343 MiB |\n",
            "|       from small pool |    230 MiB |    296 MiB |   2340 MiB |   2110 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14662 MiB |  14742 MiB |  27272 MiB |  12610 MiB |\n",
            "|       from large pool |  14430 MiB |  14440 MiB |  26896 MiB |  12466 MiB |\n",
            "|       from small pool |    232 MiB |    302 MiB |    376 MiB |    144 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2159 MiB |   2755 MiB | 190592 MiB | 188432 MiB |\n",
            "|       from large pool |   2157 MiB |   2752 MiB | 188277 MiB | 186119 MiB |\n",
            "|       from small pool |      1 MiB |      8 MiB |   2314 MiB |   2313 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1007    |    1078    |   14626    |   13619    |\n",
            "|       from large pool |     231    |     231    |    4691    |    4460    |\n",
            "|       from small pool |     776    |     952    |    9935    |    9159    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1007    |    1078    |   14626    |   13619    |\n",
            "|       from large pool |     231    |     231    |    4691    |    4460    |\n",
            "|       from small pool |     776    |     952    |    9935    |    9159    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     177    |     227    |     332    |     155    |\n",
            "|       from large pool |      61    |      76    |     144    |      83    |\n",
            "|       from small pool |     116    |     151    |     188    |      72    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      77    |    5963    |    5887    |\n",
            "|       from large pool |      52    |      52    |    2133    |    2081    |\n",
            "|       from small pool |      24    |      29    |    3830    |    3806    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:50:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  20% 11/55 [01:25<04:28,  6.09s/it]2024-10-21 16:50:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 179.06 MiB is free. Process 530225 has 14.57 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 457.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:50:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 3            |        cudaMalloc retries: 6         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13556 MiB |  14328 MiB | 424382 MiB | 410826 MiB |\n",
            "|       from large pool |  13323 MiB |  14096 MiB | 421065 MiB | 407741 MiB |\n",
            "|       from small pool |    232 MiB |    296 MiB |   3316 MiB |   3084 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13556 MiB |  14328 MiB | 424382 MiB | 410826 MiB |\n",
            "|       from large pool |  13323 MiB |  14096 MiB | 421065 MiB | 407741 MiB |\n",
            "|       from small pool |    232 MiB |    296 MiB |   3316 MiB |   3084 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13541 MiB |  14312 MiB | 423869 MiB | 410328 MiB |\n",
            "|       from large pool |  13308 MiB |  14080 MiB | 420555 MiB | 407246 MiB |\n",
            "|       from small pool |    232 MiB |    296 MiB |   3314 MiB |   3082 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14786 MiB |  14786 MiB |  28698 MiB |  13912 MiB |\n",
            "|       from large pool |  14552 MiB |  14552 MiB |  28174 MiB |  13622 MiB |\n",
            "|       from small pool |    234 MiB |    304 MiB |    524 MiB |    290 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 579443 KiB |   4164 MiB | 336475 MiB | 335909 MiB |\n",
            "|       from large pool | 577565 KiB |   4156 MiB | 333162 MiB | 332598 MiB |\n",
            "|       from small pool |   1878 KiB |      9 MiB |   3312 MiB |   3310 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1126    |    1137    |   21875    |   20749    |\n",
            "|       from large pool |     311    |     324    |    7376    |    7065    |\n",
            "|       from small pool |     815    |     952    |   14499    |   13684    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1126    |    1137    |   21875    |   20749    |\n",
            "|       from large pool |     311    |     324    |    7376    |    7065    |\n",
            "|       from small pool |     815    |     952    |   14499    |   13684    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     176    |     211    |     410    |     234    |\n",
            "|       from large pool |      59    |      59    |     148    |      89    |\n",
            "|       from small pool |     117    |     152    |     262    |     145    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      72    |    9187    |    9119    |\n",
            "|       from large pool |      45    |      46    |    3432    |    3387    |\n",
            "|       from small pool |      23    |      37    |    5755    |    5732    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:50:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  45% 25/55 [04:00<04:19,  8.64s/it]2024-10-21 16:53:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 33.06 MiB is free. Process 530225 has 14.71 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 671.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:53:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14260 MiB |  14261 MiB |    913 GiB |    899 GiB |\n",
            "|       from large pool |  14027 MiB |  14027 MiB |    905 GiB |    891 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |      7 GiB |      7 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14260 MiB |  14261 MiB |    913 GiB |    899 GiB |\n",
            "|       from large pool |  14027 MiB |  14027 MiB |    905 GiB |    891 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |      7 GiB |      7 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14244 MiB |  14245 MiB |    912 GiB |    898 GiB |\n",
            "|       from large pool |  14011 MiB |  14011 MiB |    904 GiB |    891 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |      7 GiB |      7 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14932 MiB |  14932 MiB |  32630 MiB |  17698 MiB |\n",
            "|       from large pool |  14696 MiB |  14696 MiB |  31942 MiB |  17246 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |    688 MiB |    452 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 687707 KiB |   4122 MiB | 770775 MiB | 770104 MiB |\n",
            "|       from large pool | 684758 KiB |   4112 MiB | 762555 MiB | 761886 MiB |\n",
            "|       from small pool |   2949 KiB |     10 MiB |   8220 MiB |   8217 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1063    |    1078    |   50124    |   49061    |\n",
            "|       from large pool |     271    |     271    |   16234    |   15963    |\n",
            "|       from small pool |     792    |     952    |   33890    |   33098    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1063    |    1078    |   50124    |   49061    |\n",
            "|       from large pool |     271    |     271    |   16234    |   15963    |\n",
            "|       from small pool |     792    |     952    |   33890    |   33098    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     177    |     215    |     505    |     328    |\n",
            "|       from large pool |      59    |      59    |     161    |     102    |\n",
            "|       from small pool |     118    |     157    |     344    |     226    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      69    |   21565    |   21497    |\n",
            "|       from large pool |      46    |      46    |    7870    |    7824    |\n",
            "|       from small pool |      22    |      33    |   13695    |   13673    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:53:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  64% 35/55 [04:48<02:05,  6.28s/it]2024-10-21 16:53:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 163.06 MiB is free. Process 530225 has 14.59 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 322.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:53:57 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 5            |        cudaMalloc retries: 15        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13865 MiB |  14479 MiB |   1346 GiB |   1333 GiB |\n",
            "|       from large pool |  13632 MiB |  14246 MiB |   1336 GiB |   1323 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13865 MiB |  14479 MiB |   1346 GiB |   1333 GiB |\n",
            "|       from large pool |  13632 MiB |  14246 MiB |   1336 GiB |   1323 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13842 MiB |  14456 MiB |   1345 GiB |   1331 GiB |\n",
            "|       from large pool |  13609 MiB |  14223 MiB |   1335 GiB |   1321 GiB |\n",
            "|       from small pool |    233 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14802 MiB |  14956 MiB |  60516 MiB |  45714 MiB |\n",
            "|       from large pool |  14568 MiB |  14722 MiB |  59646 MiB |  45078 MiB |\n",
            "|       from small pool |    234 MiB |    308 MiB |    870 MiB |    636 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 446766 KiB |   4144 MiB |   1086 GiB |   1085 GiB |\n",
            "|       from large pool | 445893 KiB |   4132 MiB |   1075 GiB |   1075 GiB |\n",
            "|       from small pool |    873 KiB |     15 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1126    |    1137    |   70901    |   69775    |\n",
            "|       from large pool |     311    |     324    |   24595    |   24284    |\n",
            "|       from small pool |     815    |     952    |   46306    |   45491    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1126    |    1137    |   70901    |   69775    |\n",
            "|       from large pool |     311    |     324    |   24595    |   24284    |\n",
            "|       from small pool |     815    |     952    |   46306    |   45491    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     193    |     215    |     723    |     530    |\n",
            "|       from large pool |      76    |      78    |     288    |     212    |\n",
            "|       from small pool |     117    |     154    |     435    |     318    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      75    |      96    |   30612    |   30537    |\n",
            "|       from large pool |      54    |      59    |   12004    |   11950    |\n",
            "|       from small pool |      21    |      57    |   18608    |   18587    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:53:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  67% 37/55 [04:52<01:17,  4.32s/it]2024-10-21 16:54:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 617.06 MiB is free. Process 530225 has 14.14 GiB memory in use. Of the allocated memory 12.94 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:54:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 6            |        cudaMalloc retries: 17        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13248 MiB |  13695 MiB |   1421 GiB |   1409 GiB |\n",
            "|       from large pool |  13020 MiB |  13467 MiB |   1411 GiB |   1398 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13248 MiB |  13695 MiB |   1421 GiB |   1409 GiB |\n",
            "|       from large pool |  13020 MiB |  13467 MiB |   1411 GiB |   1398 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13234 MiB |  13681 MiB |   1420 GiB |   1407 GiB |\n",
            "|       from large pool |  13005 MiB |  13452 MiB |   1410 GiB |   1397 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14348 MiB |  14780 MiB |  64834 MiB |  50486 MiB |\n",
            "|       from large pool |  14116 MiB |  14476 MiB |  63894 MiB |  49778 MiB |\n",
            "|       from small pool |    232 MiB |    304 MiB |    940 MiB |    708 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1099 MiB |   3984 MiB |   1137 GiB |   1136 GiB |\n",
            "|       from large pool |   1095 MiB |   3976 MiB |   1127 GiB |   1125 GiB |\n",
            "|       from small pool |      3 MiB |     11 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1091    |    1092    |   73627    |   72536    |\n",
            "|       from large pool |     293    |     293    |   25788    |   25495    |\n",
            "|       from small pool |     798    |     952    |   47839    |   47041    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1091    |    1092    |   73627    |   72536    |\n",
            "|       from large pool |     293    |     293    |   25788    |   25495    |\n",
            "|       from small pool |     798    |     952    |   47839    |   47041    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     228    |     765    |     583    |\n",
            "|       from large pool |      66    |      76    |     295    |     229    |\n",
            "|       from small pool |     116    |     152    |     470    |     354    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      85    |   31884    |   31821    |\n",
            "|       from large pool |      45    |      58    |   12601    |   12556    |\n",
            "|       from small pool |      18    |      31    |   19283    |   19265    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:54:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  69% 38/55 [04:55<01:04,  3.78s/it]2024-10-21 16:54:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 530225 has 14.52 GiB memory in use. Of the allocated memory 12.86 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:54:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 7            |        cudaMalloc retries: 19        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13166 MiB |  13695 MiB |   1442 GiB |   1429 GiB |\n",
            "|       from large pool |  12937 MiB |  13467 MiB |   1431 GiB |   1419 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13166 MiB |  13695 MiB |   1442 GiB |   1429 GiB |\n",
            "|       from large pool |  12937 MiB |  13467 MiB |   1431 GiB |   1419 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13148 MiB |  13681 MiB |   1441 GiB |   1428 GiB |\n",
            "|       from large pool |  12918 MiB |  13452 MiB |   1430 GiB |   1417 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14780 MiB |  65610 MiB |  50872 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    304 MiB |    940 MiB |    708 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1571 MiB |   3984 MiB |   1149 GiB |   1148 GiB |\n",
            "|       from large pool |   1568 MiB |   3976 MiB |   1139 GiB |   1137 GiB |\n",
            "|       from small pool |      2 MiB |     11 MiB |     10 GiB |     10 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1092    |    1092    |   74101    |   73009    |\n",
            "|       from large pool |     294    |     294    |   26072    |   25778    |\n",
            "|       from small pool |     798    |     952    |   48029    |   47231    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1092    |    1092    |   74101    |   73009    |\n",
            "|       from large pool |     294    |     294    |   26072    |   25778    |\n",
            "|       from small pool |     798    |     952    |   48029    |   47231    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     228    |     767    |     584    |\n",
            "|       from large pool |      67    |      76    |     297    |     230    |\n",
            "|       from small pool |     116    |     152    |     470    |     354    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      85    |   32085    |   32028    |\n",
            "|       from large pool |      37    |      58    |   12741    |   12704    |\n",
            "|       from small pool |      20    |      31    |   19344    |   19324    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:54:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  87% 48/55 [06:28<01:04,  9.18s/it]2024-10-21 16:55:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 229.06 MiB is free. Process 530225 has 14.52 GiB memory in use. Of the allocated memory 12.39 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:55:37 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 8            |        cudaMalloc retries: 20        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12688 MiB |  12688 MiB |   1819 GiB |   1806 GiB |\n",
            "|       from large pool |  12459 MiB |  12459 MiB |   1805 GiB |   1793 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     13 GiB |     13 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12688 MiB |  12688 MiB |   1819 GiB |   1806 GiB |\n",
            "|       from large pool |  12459 MiB |  12459 MiB |   1805 GiB |   1793 GiB |\n",
            "|       from small pool |    229 MiB |    296 MiB |     13 GiB |     13 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12675 MiB |  12675 MiB |   1817 GiB |   1805 GiB |\n",
            "|       from large pool |  12446 MiB |  12446 MiB |   1804 GiB |   1791 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     13 GiB |     13 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14736 MiB |  14824 MiB |  65696 MiB |  50960 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    230 MiB |    318 MiB |   1026 MiB |    796 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2047 MiB |   3461 MiB |   1507 GiB |   1505 GiB |\n",
            "|       from large pool |   2046 MiB |   3455 MiB |   1493 GiB |   1491 GiB |\n",
            "|       from small pool |      0 MiB |      8 MiB |     13 GiB |     13 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1120    |    1120    |   94332    |   93212    |\n",
            "|       from large pool |     315    |     315    |   33144    |   32829    |\n",
            "|       from small pool |     805    |     951    |   61188    |   60383    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1120    |    1120    |   94332    |   93212    |\n",
            "|       from large pool |     315    |     315    |   33144    |   32829    |\n",
            "|       from small pool |     805    |     951    |   61188    |   60383    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     226    |     810    |     628    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     115    |     159    |     513    |     398    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      78    |      78    |   41077    |   40999    |\n",
            "|       from large pool |      58    |      58    |   16298    |   16240    |\n",
            "|       from small pool |      20    |      29    |   24779    |   24759    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:55:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  96% 53/55 [07:08<00:18,  9.47s/it]2024-10-21 16:56:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 530225 has 14.52 GiB memory in use. Of the allocated memory 13.38 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:56:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13705 MiB |  13705 MiB |   1992 GiB |   1979 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   1977 GiB |   1964 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     15 GiB |     14 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13705 MiB |  13705 MiB |   1992 GiB |   1979 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   1977 GiB |   1964 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     15 GiB |     14 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13684 MiB |  13684 MiB |   1990 GiB |   1977 GiB |\n",
            "|       from large pool |  13454 MiB |  13454 MiB |   1975 GiB |   1962 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     15 GiB |     14 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14820 MiB |  65780 MiB |  51042 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    314 MiB |   1110 MiB |    878 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1032 MiB |   3875 MiB |   1670 GiB |   1669 GiB |\n",
            "|       from large pool |   1030 MiB |   3860 MiB |   1655 GiB |   1654 GiB |\n",
            "|       from small pool |      1 MiB |     16 MiB |     15 GiB |     15 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1120    |    1120    |  103598    |  102478    |\n",
            "|       from large pool |     315    |     315    |   36251    |   35936    |\n",
            "|       from small pool |     805    |     951    |   67347    |   66542    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1120    |    1120    |  103598    |  102478    |\n",
            "|       from large pool |     315    |     315    |   36251    |   35936    |\n",
            "|       from small pool |     805    |     951    |   67347    |   66542    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     224    |     852    |     669    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     116    |     157    |     555    |     439    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      77    |   45017    |   44952    |\n",
            "|       from large pool |      41    |      47    |   17811    |   17770    |\n",
            "|       from small pool |      24    |      50    |   27206    |   27182    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:56:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 001:  98% 54/55 [07:09<00:07,  7.02s/it]2024-10-21 16:56:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-21 16:56:35 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(42105, 573), first few sample ids=[101]\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 1/19 [00:01<00:20,  1.12s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 2/19 [00:02<00:18,  1.10s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 3/19 [00:03<00:17,  1.08s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 4/19 [00:04<00:16,  1.12s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 5/19 [00:05<00:14,  1.07s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 6/19 [00:06<00:14,  1.12s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 7/19 [00:07<00:11,  1.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 8/19 [00:08<00:10,  1.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 9/19 [00:09<00:11,  1.12s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 10/19 [00:11<00:11,  1.30s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 11/19 [00:13<00:12,  1.58s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 12/19 [00:15<00:11,  1.70s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 13/19 [00:20<00:15,  2.59s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 14/19 [00:23<00:14,  2.91s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 15/19 [00:28<00:13,  3.26s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 16/19 [00:31<00:10,  3.44s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 17/19 [00:36<00:07,  3.94s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 18/19 [00:42<00:04,  4.33s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 19/19 [00:46<00:00,  4.19s/it]\u001b[A\n",
            "                                                                        \u001b[A2024-10-21 16:57:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.405 | nll_loss 6.883 | ppl 117.99 | wps 534 | wpb 1758.4 | bsz 14.4 | num_updates 46\n",
            "2024-10-21 16:57:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 46 updates\n",
            "2024-10-21 16:57:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/positional_transformer/checkpoint1.pt\n",
            "2024-10-21 16:57:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/positional_transformer/checkpoint1.pt\n",
            "2024-10-21 16:57:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/positional_transformer/checkpoint1.pt (epoch 1 @ 46 updates, score 7.405) (writing took 27.82440035699983 seconds)\n",
            "2024-10-21 16:57:49 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-10-21 16:57:49 | INFO | train | epoch 001 | loss 9.303 | nll_loss 9.056 | ppl 532.29 | wps 124.2 | ups 0.09 | wpb 1419.7 | bsz 12.3 | num_updates 46 | lr 5.75e-06 | gnorm 14.779 | clip 100 | train_wall 435 | gb_free 1.4 | wall 521\n",
            "2024-10-21 16:57:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 55\n",
            "epoch 002:   0% 0/55 [00:00<?, ?it/s]2024-10-21 16:57:49 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-10-21 16:57:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  20% 11/55 [01:49<07:47, 10.64s/it]2024-10-21 16:59:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 530225 has 14.52 GiB memory in use. Of the allocated memory 12.39 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 16:59:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 10           |        cudaMalloc retries: 22        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12688 MiB |  12688 MiB |   2821 GiB |   2809 GiB |\n",
            "|       from large pool |  12459 MiB |  12459 MiB |   2800 GiB |   2787 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     21 GiB |     21 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12688 MiB |  12688 MiB |   2821 GiB |   2809 GiB |\n",
            "|       from large pool |  12459 MiB |  12459 MiB |   2800 GiB |   2787 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     21 GiB |     21 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12675 MiB |  12675 MiB |   2819 GiB |   2806 GiB |\n",
            "|       from large pool |  12446 MiB |  12446 MiB |   2797 GiB |   2785 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     21 GiB |     21 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14826 MiB |  65868 MiB |  51130 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    320 MiB |   1198 MiB |    966 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2049 MiB |   3870 MiB |   2424 GiB |   2422 GiB |\n",
            "|       from large pool |   2046 MiB |   3860 MiB |   2402 GiB |   2400 GiB |\n",
            "|       from small pool |      2 MiB |     11 MiB |     22 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  139841    |  138717    |\n",
            "|       from large pool |     315    |     315    |   49216    |   48901    |\n",
            "|       from small pool |     809    |     955    |   90625    |   89816    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  139841    |  138717    |\n",
            "|       from large pool |     315    |     315    |   49216    |   48901    |\n",
            "|       from small pool |     809    |     955    |   90625    |   89816    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     227    |     896    |     713    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     116    |     160    |     599    |     483    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      81    |      81    |   60925    |   60844    |\n",
            "|       from large pool |      58    |      58    |   23466    |   23408    |\n",
            "|       from small pool |      23    |      32    |   37459    |   37436    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 16:59:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  27% 15/55 [02:27<07:44, 11.61s/it]2024-10-21 17:00:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 219.06 MiB is free. Process 530225 has 14.53 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 520.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:00:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 11           |        cudaMalloc retries: 23        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13880 MiB |  14432 MiB |   2967 GiB |   2954 GiB |\n",
            "|       from large pool |  13648 MiB |  14200 MiB |   2945 GiB |   2931 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     22 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13880 MiB |  14432 MiB |   2967 GiB |   2954 GiB |\n",
            "|       from large pool |  13648 MiB |  14200 MiB |   2945 GiB |   2931 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     22 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13864 MiB |  14416 MiB |   2965 GiB |   2951 GiB |\n",
            "|       from large pool |  13631 MiB |  14184 MiB |   2942 GiB |   2929 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     22 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14746 MiB |  14820 MiB |  65950 MiB |  51204 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    240 MiB |    314 MiB |   1280 MiB |   1040 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    865 MiB |   3053 MiB |   2557 GiB |   2556 GiB |\n",
            "|       from large pool |    857 MiB |   3032 MiB |   2534 GiB |   2533 GiB |\n",
            "|       from small pool |      7 MiB |     22 MiB |     23 GiB |     23 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1132    |    1141    |  146896    |  145764    |\n",
            "|       from large pool |     314    |     325    |   51776    |   51462    |\n",
            "|       from small pool |     818    |     955    |   95120    |   94302    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1132    |    1141    |  146896    |  145764    |\n",
            "|       from large pool |     314    |     325    |   51776    |   51462    |\n",
            "|       from small pool |     818    |     955    |   95120    |   94302    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     187    |     224    |     937    |     750    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     120    |     157    |     640    |     520    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      81    |      94    |   63920    |   63839    |\n",
            "|       from large pool |      54    |      58    |   24676    |   24622    |\n",
            "|       from small pool |      27    |      64    |   39244    |   39217    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:00:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  33% 18/55 [02:35<03:44,  6.07s/it]2024-10-21 17:00:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 223.06 MiB is free. Process 530225 has 14.53 GiB memory in use. Of the allocated memory 13.38 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:00:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 12           |        cudaMalloc retries: 24        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13705 MiB |  13705 MiB |   3063 GiB |   3050 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   3040 GiB |   3027 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     23 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13705 MiB |  13705 MiB |   3063 GiB |   3050 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   3040 GiB |   3027 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     23 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13684 MiB |  13684 MiB |   3061 GiB |   3047 GiB |\n",
            "|       from large pool |  13454 MiB |  13454 MiB |   3038 GiB |   3025 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     23 GiB |     22 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14742 MiB |  14820 MiB |  66024 MiB |  51282 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |   1354 MiB |   1118 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1036 MiB |   3205 MiB |   2648 GiB |   2647 GiB |\n",
            "|       from large pool |   1030 MiB |   3194 MiB |   2624 GiB |   2623 GiB |\n",
            "|       from small pool |      5 MiB |     12 MiB |     23 GiB |     23 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  151906    |  150782    |\n",
            "|       from large pool |     315    |     315    |   53854    |   53539    |\n",
            "|       from small pool |     809    |     955    |   98052    |   97243    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  151906    |  150782    |\n",
            "|       from large pool |     315    |     315    |   53854    |   53539    |\n",
            "|       from small pool |     809    |     955    |   98052    |   97243    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     185    |     224    |     974    |     789    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     118    |     157    |     677    |     559    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      65    |      70    |   66042    |   65977    |\n",
            "|       from large pool |      41    |      47    |   25710    |   25669    |\n",
            "|       from small pool |      24    |      31    |   40332    |   40308    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:00:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  53% 29/55 [04:37<06:15, 14.46s/it]2024-10-21 17:02:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 310.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 530225 has 14.52 GiB memory in use. Of the allocated memory 13.12 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:02:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 13           |        cudaMalloc retries: 25        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13435 MiB |  13436 MiB |   3478 GiB |   3465 GiB |\n",
            "|       from large pool |  13205 MiB |  13205 MiB |   3451 GiB |   3438 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     26 GiB |     26 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13435 MiB |  13436 MiB |   3478 GiB |   3465 GiB |\n",
            "|       from large pool |  13205 MiB |  13205 MiB |   3451 GiB |   3438 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     26 GiB |     26 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13423 MiB |  13424 MiB |   3475 GiB |   3462 GiB |\n",
            "|       from large pool |  13193 MiB |  13193 MiB |   3448 GiB |   3436 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     26 GiB |     26 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14824 MiB |  66106 MiB |  51368 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    318 MiB |   1436 MiB |   1204 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1302 MiB |   2372 MiB |   3048 GiB |   3047 GiB |\n",
            "|       from large pool |   1300 MiB |   2356 MiB |   3021 GiB |   3020 GiB |\n",
            "|       from small pool |      1 MiB |     17 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1019    |    1082    |  173850    |  172831    |\n",
            "|       from large pool |     238    |     238    |   60894    |   60656    |\n",
            "|       from small pool |     781    |     955    |  112956    |  112175    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1019    |    1082    |  173850    |  172831    |\n",
            "|       from large pool |     238    |     238    |   60894    |   60656    |\n",
            "|       from small pool |     781    |     955    |  112956    |  112175    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     226    |    1015    |     832    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     116    |     159    |     718    |     602    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      91    |      92    |   76001    |   75910    |\n",
            "|       from large pool |      64    |      65    |   29355    |   29291    |\n",
            "|       from small pool |      27    |      44    |   46646    |   46619    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:02:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  60% 33/55 [04:56<02:51,  7.80s/it]2024-10-21 17:02:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 225.06 MiB is free. Process 530225 has 14.53 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 482.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:02:47 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 14           |        cudaMalloc retries: 26        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14257 MiB |  14258 MiB |   3625 GiB |   3611 GiB |\n",
            "|       from large pool |  14024 MiB |  14024 MiB |   3598 GiB |   3584 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14257 MiB |  14258 MiB |   3625 GiB |   3611 GiB |\n",
            "|       from large pool |  14024 MiB |  14024 MiB |   3598 GiB |   3584 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14245 MiB |  14246 MiB |   3622 GiB |   3609 GiB |\n",
            "|       from large pool |  14012 MiB |  14012 MiB |   3595 GiB |   3581 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14740 MiB |  14814 MiB |  66182 MiB |  51442 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    234 MiB |    308 MiB |   1512 MiB |   1278 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 494394 KiB |   4761 MiB |   3187 GiB |   3186 GiB |\n",
            "|       from large pool | 492971 KiB |   4749 MiB |   3158 GiB |   3158 GiB |\n",
            "|       from small pool |   1423 KiB |     14 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1067    |    1082    |  180979    |  179912    |\n",
            "|       from large pool |     272    |     272    |   63825    |   63553    |\n",
            "|       from small pool |     795    |     955    |  117154    |  116359    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1067    |    1082    |  180979    |  179912    |\n",
            "|       from large pool |     272    |     272    |   63825    |   63553    |\n",
            "|       from small pool |     795    |     955    |  117154    |  116359    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     184    |     221    |    1053    |     869    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     117    |     154    |     756    |     639    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      94    |      95    |   79354    |   79260    |\n",
            "|       from large pool |      65    |      66    |   30902    |   30837    |\n",
            "|       from small pool |      29    |      53    |   48452    |   48423    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:02:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  64% 35/55 [05:01<01:43,  5.15s/it]2024-10-21 17:02:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 229.06 MiB is free. Process 530225 has 14.52 GiB memory in use. Of the allocated memory 13.69 GiB is allocated by PyTorch, and 719.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:02:53 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 15           |        cudaMalloc retries: 27        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14016 MiB |  14016 MiB |   3683 GiB |   3669 GiB |\n",
            "|       from large pool |  13788 MiB |  13788 MiB |   3655 GiB |   3642 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14016 MiB |  14016 MiB |   3683 GiB |   3669 GiB |\n",
            "|       from large pool |  13788 MiB |  13788 MiB |   3655 GiB |   3642 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14002 MiB |  14002 MiB |   3680 GiB |   3667 GiB |\n",
            "|       from large pool |  13773 MiB |  13773 MiB |   3652 GiB |   3639 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     27 GiB |     27 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14736 MiB |  14810 MiB |  66252 MiB |  51516 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    230 MiB |    304 MiB |   1582 MiB |   1352 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 736372 KiB |   3232 MiB |   3234 GiB |   3233 GiB |\n",
            "|       from large pool | 734992 KiB |   3224 MiB |   3205 GiB |   3205 GiB |\n",
            "|       from small pool |   1380 KiB |      9 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1096    |    1096    |  183691    |  182595    |\n",
            "|       from large pool |     294    |     294    |   64987    |   64693    |\n",
            "|       from small pool |     802    |     955    |  118704    |  117902    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1096    |    1096    |  183691    |  182595    |\n",
            "|       from large pool |     294    |     294    |   64987    |   64693    |\n",
            "|       from small pool |     802    |     955    |  118704    |  117902    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     219    |    1088    |     906    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     115    |     152    |     791    |     676    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      81    |   80442    |   80372    |\n",
            "|       from large pool |      46    |      57    |   31420    |   31374    |\n",
            "|       from small pool |      24    |      38    |   49022    |   48998    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:02:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  69% 38/55 [05:12<01:11,  4.21s/it]2024-10-21 17:03:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 227.06 MiB is free. Process 530225 has 14.52 GiB memory in use. Of the allocated memory 13.56 GiB is allocated by PyTorch, and 851.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:03:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 16           |        cudaMalloc retries: 28        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13118 MiB |  13886 MiB |   3779 GiB |   3767 GiB |\n",
            "|       from large pool |  12887 MiB |  13655 MiB |   3751 GiB |   3738 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13118 MiB |  13886 MiB |   3779 GiB |   3767 GiB |\n",
            "|       from large pool |  12887 MiB |  13655 MiB |   3751 GiB |   3738 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13100 MiB |  13868 MiB |   3776 GiB |   3764 GiB |\n",
            "|       from large pool |  12869 MiB |  13637 MiB |   3748 GiB |   3735 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14738 MiB |  14818 MiB |  66334 MiB |  51596 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    232 MiB |    312 MiB |   1664 MiB |   1432 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1619 MiB |   3459 MiB |   3321 GiB |   3319 GiB |\n",
            "|       from large pool |   1618 MiB |   3443 MiB |   3292 GiB |   3290 GiB |\n",
            "|       from small pool |      0 MiB |     18 MiB |     29 GiB |     29 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  188772    |  187642    |\n",
            "|       from large pool |     312    |     325    |   67107    |   66795    |\n",
            "|       from small pool |     818    |     955    |  121665    |  120847    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  188772    |  187642    |\n",
            "|       from large pool |     312    |     325    |   67107    |   66795    |\n",
            "|       from small pool |     818    |     955    |  121665    |  120847    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     183    |     223    |    1129    |     946    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     116    |     156    |     832    |     716    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      81    |   82598    |   82522    |\n",
            "|       from large pool |      58    |      59    |   32461    |   32403    |\n",
            "|       from small pool |      18    |      40    |   50137    |   50119    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:03:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  73% 40/55 [05:19<00:59,  3.95s/it]2024-10-21 17:03:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 229.06 MiB is free. Process 530225 has 14.52 GiB memory in use. Of the allocated memory 12.12 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:03:09 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 17           |        cudaMalloc retries: 29        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12408 MiB |  12408 MiB |   3843 GiB |   3831 GiB |\n",
            "|       from large pool |  12179 MiB |  12179 MiB |   3814 GiB |   3802 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12408 MiB |  12408 MiB |   3843 GiB |   3831 GiB |\n",
            "|       from large pool |  12179 MiB |  12179 MiB |   3814 GiB |   3802 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12392 MiB |  12392 MiB |   3840 GiB |   3828 GiB |\n",
            "|       from large pool |  12162 MiB |  12162 MiB |   3811 GiB |   3799 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14736 MiB |  14808 MiB |  66404 MiB |  51668 MiB |\n",
            "|       from large pool |  14506 MiB |  14506 MiB |  64670 MiB |  50164 MiB |\n",
            "|       from small pool |    230 MiB |    302 MiB |   1734 MiB |   1504 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2327 MiB |   3077 MiB |   3380 GiB |   3378 GiB |\n",
            "|       from large pool |   2326 MiB |   3070 MiB |   3350 GiB |   3348 GiB |\n",
            "|       from small pool |      0 MiB |      8 MiB |     29 GiB |     29 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1068    |    1082    |  191432    |  190364    |\n",
            "|       from large pool |     273    |     273    |   68267    |   67994    |\n",
            "|       from small pool |     795    |     955    |  123165    |  122370    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1068    |    1082    |  191432    |  190364    |\n",
            "|       from large pool |     273    |     273    |   68267    |   67994    |\n",
            "|       from small pool |     795    |     955    |  123165    |  122370    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     218    |    1164    |     982    |\n",
            "|       from large pool |      67    |      67    |     297    |     230    |\n",
            "|       from small pool |     115    |     151    |     867    |     752    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      70    |   83829    |   83769    |\n",
            "|       from large pool |      39    |      47    |   33100    |   33061    |\n",
            "|       from small pool |      21    |      34    |   50729    |   50708    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:03:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  75% 41/55 [05:20<00:44,  3.21s/it]2024-10-21 17:03:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 466.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 151.06 MiB is free. Process 530225 has 14.60 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 2.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:03:11 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 18           |        cudaMalloc retries: 31        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12661 MiB |  12661 MiB |   3863 GiB |   3851 GiB |\n",
            "|       from large pool |  12432 MiB |  12432 MiB |   3835 GiB |   3822 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12661 MiB |  12661 MiB |   3863 GiB |   3851 GiB |\n",
            "|       from large pool |  12432 MiB |  12432 MiB |   3835 GiB |   3822 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12639 MiB |  12639 MiB |   3860 GiB |   3848 GiB |\n",
            "|       from large pool |  12410 MiB |  12410 MiB |   3831 GiB |   3819 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     28 GiB |     28 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14814 MiB |  14814 MiB |  66870 MiB |  52056 MiB |\n",
            "|       from large pool |  14584 MiB |  14584 MiB |  65136 MiB |  50552 MiB |\n",
            "|       from small pool |    230 MiB |    302 MiB |   1734 MiB |   1504 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2152 MiB |   3255 MiB |   3395 GiB |   3393 GiB |\n",
            "|       from large pool |   2151 MiB |   3252 MiB |   3366 GiB |   3363 GiB |\n",
            "|       from small pool |      1 MiB |      8 MiB |     29 GiB |     29 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  191958    |  190834    |\n",
            "|       from large pool |     315    |     315    |   68583    |   68268    |\n",
            "|       from small pool |     809    |     955    |  123375    |  122566    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  191958    |  190834    |\n",
            "|       from large pool |     315    |     315    |   68583    |   68268    |\n",
            "|       from small pool |     809    |     955    |  123375    |  122566    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     182    |     218    |    1165    |     983    |\n",
            "|       from large pool |      67    |      67    |     298    |     231    |\n",
            "|       from small pool |     115    |     151    |     867    |     752    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      76    |   84032    |   83970    |\n",
            "|       from large pool |      42    |      55    |   33243    |   33201    |\n",
            "|       from small pool |      20    |      34    |   50789    |   50769    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:03:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  85% 47/55 [06:45<01:50, 13.81s/it]2024-10-21 17:04:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 141.06 MiB is free. Process 530225 has 14.61 GiB memory in use. Of the allocated memory 13.67 GiB is allocated by PyTorch, and 828.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:04:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 19           |        cudaMalloc retries: 32        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13554 MiB |  14180 MiB |   4064 GiB |   4051 GiB |\n",
            "|       from large pool |  13323 MiB |  13949 MiB |   4033 GiB |   4020 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     31 GiB |     30 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13554 MiB |  14180 MiB |   4064 GiB |   4051 GiB |\n",
            "|       from large pool |  13323 MiB |  13949 MiB |   4033 GiB |   4020 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     31 GiB |     30 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13541 MiB |  14166 MiB |   4061 GiB |   4048 GiB |\n",
            "|       from large pool |  13310 MiB |  13935 MiB |   4030 GiB |   4017 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     31 GiB |     30 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14824 MiB |  14902 MiB |  66958 MiB |  52134 MiB |\n",
            "|       from large pool |  14584 MiB |  14584 MiB |  65136 MiB |  50552 MiB |\n",
            "|       from small pool |    240 MiB |    318 MiB |   1822 MiB |   1582 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1269 MiB |   2255 MiB |   3586 GiB |   3585 GiB |\n",
            "|       from large pool |   1260 MiB |   2236 MiB |   3554 GiB |   3553 GiB |\n",
            "|       from small pool |      8 MiB |     31 MiB |     31 GiB |     31 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  203248    |  202118    |\n",
            "|       from large pool |     312    |     325    |   71501    |   71189    |\n",
            "|       from small pool |     818    |     955    |  131747    |  130929    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  203248    |  202118    |\n",
            "|       from large pool |     312    |     325    |   71501    |   71189    |\n",
            "|       from small pool |     818    |     955    |  131747    |  130929    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     187    |     226    |    1209    |    1022    |\n",
            "|       from large pool |      67    |      67    |     298    |     231    |\n",
            "|       from small pool |     120    |     159    |     911    |     791    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      90    |      90    |   89140    |   89050    |\n",
            "|       from large pool |      67    |      67    |   34747    |   34680    |\n",
            "|       from small pool |      23    |      57    |   54393    |   54370    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:04:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  91% 50/55 [06:54<00:34,  6.97s/it]2024-10-21 17:04:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 474.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 387.06 MiB is free. Process 530225 has 14.37 GiB memory in use. Of the allocated memory 13.61 GiB is allocated by PyTorch, and 641.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:04:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 20           |        cudaMalloc retries: 34        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13936 MiB |  13936 MiB |   4170 GiB |   4156 GiB |\n",
            "|       from large pool |  13696 MiB |  13696 MiB |   4138 GiB |   4125 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     31 GiB |     31 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13936 MiB |  13936 MiB |   4170 GiB |   4156 GiB |\n",
            "|       from large pool |  13696 MiB |  13696 MiB |   4138 GiB |   4125 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     31 GiB |     31 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13924 MiB |  13924 MiB |   4167 GiB |   4153 GiB |\n",
            "|       from large pool |  13685 MiB |  13685 MiB |   4135 GiB |   4122 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     31 GiB |     31 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14578 MiB |  14894 MiB |  69182 MiB |  54604 MiB |\n",
            "|       from large pool |  14338 MiB |  14584 MiB |  67288 MiB |  52950 MiB |\n",
            "|       from small pool |    240 MiB |    310 MiB |   1894 MiB |   1654 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 657300 KiB |   3189 MiB |   3681 GiB |   3681 GiB |\n",
            "|       from large pool | 656385 KiB |   3168 MiB |   3649 GiB |   3648 GiB |\n",
            "|       from small pool |    914 KiB |     20 MiB |     32 GiB |     32 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1058    |    1082    |  208135    |  207077    |\n",
            "|       from large pool |     256    |     256    |   73498    |   73242    |\n",
            "|       from small pool |     802    |     955    |  134637    |  133835    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1058    |    1082    |  208135    |  207077    |\n",
            "|       from large pool |     256    |     256    |   73498    |   73242    |\n",
            "|       from small pool |     802    |     955    |  134637    |  133835    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     188    |     222    |    1252    |    1064    |\n",
            "|       from large pool |      68    |      68    |     305    |     237    |\n",
            "|       from small pool |     120    |     155    |     947    |     827    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      84    |      86    |   91184    |   91100    |\n",
            "|       from large pool |      59    |      60    |   35695    |   35636    |\n",
            "|       from small pool |      25    |      52    |   55489    |   55464    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:04:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 002:  98% 54/55 [07:06<00:04,  4.16s/it]2024-10-21 17:05:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 1/19 [00:01<00:19,  1.06s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 2/19 [00:02<00:18,  1.07s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 3/19 [00:03<00:17,  1.07s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 4/19 [00:04<00:16,  1.11s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 5/19 [00:05<00:14,  1.06s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 6/19 [00:06<00:14,  1.12s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 7/19 [00:07<00:11,  1.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 8/19 [00:08<00:10,  1.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 9/19 [00:09<00:11,  1.12s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 10/19 [00:11<00:11,  1.31s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 11/19 [00:13<00:12,  1.58s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 12/19 [00:15<00:11,  1.71s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 13/19 [00:20<00:15,  2.60s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 14/19 [00:23<00:14,  2.91s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 15/19 [00:27<00:13,  3.26s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84% 16/19 [00:31<00:10,  3.44s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 17/19 [00:36<00:07,  3.94s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 18/19 [00:42<00:04,  4.33s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 19/19 [00:46<00:00,  4.20s/it]\u001b[A\n",
            "                                                                        \u001b[A2024-10-21 17:05:55 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.628 | nll_loss 5.997 | ppl 63.86 | wps 533.5 | wpb 1758.4 | bsz 14.4 | num_updates 90 | best_loss 6.628\n",
            "2024-10-21 17:05:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 90 updates\n",
            "2024-10-21 17:05:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/positional_transformer/checkpoint2.pt\n",
            "2024-10-21 17:06:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/positional_transformer/checkpoint2.pt\n",
            "2024-10-21 17:06:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/positional_transformer/checkpoint2.pt (epoch 2 @ 90 updates, score 6.628) (writing took 36.657331209000404 seconds)\n",
            "2024-10-21 17:06:32 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-10-21 17:06:32 | INFO | train | epoch 002 | loss 6.88 | nll_loss 6.296 | ppl 78.58 | wps 116.7 | ups 0.08 | wpb 1387.9 | bsz 12.1 | num_updates 90 | lr 1.125e-05 | gnorm 4.126 | clip 100 | train_wall 425 | gb_free 6.4 | wall 1045\n",
            "2024-10-21 17:06:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 55\n",
            "epoch 003:   0% 0/55 [00:00<?, ?it/s]2024-10-21 17:06:32 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-10-21 17:06:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2024-10-21 17:06:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 141.06 MiB is free. Process 530225 has 14.61 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 601.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:06:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 21           |        cudaMalloc retries: 35        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13856 MiB |  14429 MiB |   4685 GiB |   4672 GiB |\n",
            "|       from large pool |  13624 MiB |  14197 MiB |   4650 GiB |   4637 GiB |\n",
            "|       from small pool |    232 MiB |    307 MiB |     35 GiB |     34 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13856 MiB |  14429 MiB |   4685 GiB |   4672 GiB |\n",
            "|       from large pool |  13624 MiB |  14197 MiB |   4650 GiB |   4637 GiB |\n",
            "|       from small pool |    232 MiB |    307 MiB |     35 GiB |     34 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13843 MiB |  14416 MiB |   4682 GiB |   4668 GiB |\n",
            "|       from large pool |  13610 MiB |  14184 MiB |   4646 GiB |   4633 GiB |\n",
            "|       from small pool |    232 MiB |    307 MiB |     35 GiB |     34 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14824 MiB |  14902 MiB |  69506 MiB |  54682 MiB |\n",
            "|       from large pool |  14588 MiB |  14588 MiB |  67538 MiB |  52950 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |   1968 MiB |   1732 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 734401 KiB |   3909 MiB |   4057 GiB |   4056 GiB |\n",
            "|       from large pool | 730956 KiB |   3901 MiB |   4021 GiB |   4020 GiB |\n",
            "|       from small pool |   3444 KiB |     21 MiB |     36 GiB |     36 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1126    |    1137    |  227286    |  226160    |\n",
            "|       from large pool |     312    |     325    |   81378    |   81066    |\n",
            "|       from small pool |     814    |     968    |  145908    |  145094    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1126    |    1137    |  227286    |  226160    |\n",
            "|       from large pool |     312    |     325    |   81378    |   81066    |\n",
            "|       from small pool |     814    |     968    |  145908    |  145094    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     187    |     226    |    1290    |    1103    |\n",
            "|       from large pool |      69    |      69    |     306    |     237    |\n",
            "|       from small pool |     118    |     157    |     984    |     866    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      82    |      87    |  100343    |  100261    |\n",
            "|       from large pool |      59    |      61    |   39702    |   39643    |\n",
            "|       from small pool |      23    |      40    |   60641    |   60618    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:06:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:   2% 1/55 [00:01<01:04,  1.19s/it]2024-10-21 17:06:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 3.06 MiB is free. Process 530225 has 14.74 GiB memory in use. Of the allocated memory 12.12 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:06:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 22           |        cudaMalloc retries: 36        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12408 MiB |  14429 MiB |   4704 GiB |   4692 GiB |\n",
            "|       from large pool |  12179 MiB |  14197 MiB |   4669 GiB |   4657 GiB |\n",
            "|       from small pool |    229 MiB |    307 MiB |     35 GiB |     35 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12408 MiB |  14429 MiB |   4704 GiB |   4692 GiB |\n",
            "|       from large pool |  12179 MiB |  14197 MiB |   4669 GiB |   4657 GiB |\n",
            "|       from small pool |    229 MiB |    307 MiB |     35 GiB |     35 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12392 MiB |  14416 MiB |   4700 GiB |   4688 GiB |\n",
            "|       from large pool |  12162 MiB |  14184 MiB |   4665 GiB |   4653 GiB |\n",
            "|       from small pool |    229 MiB |    307 MiB |     35 GiB |     35 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14962 MiB |  14962 MiB |  69894 MiB |  54932 MiB |\n",
            "|       from large pool |  14726 MiB |  14726 MiB |  67926 MiB |  53200 MiB |\n",
            "|       from small pool |    236 MiB |    314 MiB |   1968 MiB |   1732 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2553 MiB |   4085 MiB |   4072 GiB |   4069 GiB |\n",
            "|       from large pool |   2546 MiB |   4076 MiB |   4036 GiB |   4033 GiB |\n",
            "|       from small pool |      6 MiB |     21 MiB |     36 GiB |     36 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1064    |    1137    |  227708    |  226644    |\n",
            "|       from large pool |     273    |     325    |   81630    |   81357    |\n",
            "|       from small pool |     791    |     968    |  146078    |  145287    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1064    |    1137    |  227708    |  226644    |\n",
            "|       from large pool |     273    |     325    |   81630    |   81357    |\n",
            "|       from small pool |     791    |     968    |  146078    |  145287    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     187    |     226    |    1291    |    1104    |\n",
            "|       from large pool |      69    |      69    |     307    |     238    |\n",
            "|       from small pool |     118    |     157    |     984    |     866    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      87    |  100550    |  100481    |\n",
            "|       from large pool |      46    |      62    |   39852    |   39806    |\n",
            "|       from small pool |      23    |      40    |   60698    |   60675    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:06:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  24% 13/55 [01:17<03:33,  5.09s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:07:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 311.06 MiB is free. Process 530225 has 14.44 GiB memory in use. Of the allocated memory 13.56 GiB is allocated by PyTorch, and 771.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:07:51 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 23           |        cudaMalloc retries: 39        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13115 MiB |  13882 MiB |   5178 GiB |   5165 GiB |\n",
            "|       from large pool |  12883 MiB |  13651 MiB |   5139 GiB |   5127 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13115 MiB |  13882 MiB |   5178 GiB |   5165 GiB |\n",
            "|       from large pool |  12883 MiB |  13651 MiB |   5139 GiB |   5127 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13100 MiB |  13868 MiB |   5174 GiB |   5161 GiB |\n",
            "|       from large pool |  12869 MiB |  13637 MiB |   5135 GiB |   5123 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14654 MiB |  14734 MiB |  80984 MiB |  66330 MiB |\n",
            "|       from large pool |  14422 MiB |  14422 MiB |  78854 MiB |  64432 MiB |\n",
            "|       from small pool |    232 MiB |    312 MiB |   2130 MiB |   1898 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1072 MiB |   4480 MiB |   4438 GiB |   4437 GiB |\n",
            "|       from large pool |   1072 MiB |   4474 MiB |   4398 GiB |   4397 GiB |\n",
            "|       from small pool |      0 MiB |      8 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  252589    |  251459    |\n",
            "|       from large pool |     312    |     325    |   90930    |   90618    |\n",
            "|       from small pool |     818    |     954    |  161659    |  160841    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  252589    |  251459    |\n",
            "|       from large pool |     312    |     325    |   90930    |   90618    |\n",
            "|       from small pool |     818    |     954    |  161659    |  160841    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     185    |     225    |    1405    |    1220    |\n",
            "|       from large pool |      69    |      69    |     340    |     271    |\n",
            "|       from small pool |     116    |     156    |    1065    |     949    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |      82    |  111352    |  111275    |\n",
            "|       from large pool |      60    |      61    |   44512    |   44452    |\n",
            "|       from small pool |      17    |      30    |   66840    |   66823    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:07:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  25% 14/55 [01:19<02:41,  3.94s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:07:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 399.06 MiB is free. Process 530225 has 14.36 GiB memory in use. Of the allocated memory 13.69 GiB is allocated by PyTorch, and 549.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:07:54 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 24           |        cudaMalloc retries: 41        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14016 MiB |  14016 MiB |   5202 GiB |   5188 GiB |\n",
            "|       from large pool |  13787 MiB |  13787 MiB |   5163 GiB |   5150 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14016 MiB |  14016 MiB |   5202 GiB |   5188 GiB |\n",
            "|       from large pool |  13787 MiB |  13787 MiB |   5163 GiB |   5150 GiB |\n",
            "|       from small pool |    228 MiB |    296 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14002 MiB |  14002 MiB |   5197 GiB |   5184 GiB |\n",
            "|       from large pool |  13773 MiB |  13773 MiB |   5159 GiB |   5145 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     38 GiB |     38 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14566 MiB |  14956 MiB |  84824 MiB |  70258 MiB |\n",
            "|       from large pool |  14336 MiB |  14724 MiB |  82694 MiB |  68358 MiB |\n",
            "|       from small pool |    230 MiB |    312 MiB |   2130 MiB |   1900 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 563123 KiB |   4480 MiB |   4450 GiB |   4449 GiB |\n",
            "|       from large pool | 561743 KiB |   4474 MiB |   4410 GiB |   4410 GiB |\n",
            "|       from small pool |   1380 KiB |      8 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1096    |    1141    |  253063    |  251967    |\n",
            "|       from large pool |     294    |     325    |   91214    |   90920    |\n",
            "|       from small pool |     802    |     954    |  161849    |  161047    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1096    |    1141    |  253063    |  251967    |\n",
            "|       from large pool |     294    |     325    |   91214    |   90920    |\n",
            "|       from small pool |     802    |     954    |  161849    |  161047    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     178    |     225    |    1410    |    1232    |\n",
            "|       from large pool |      63    |      69    |     345    |     282    |\n",
            "|       from small pool |     115    |     156    |    1065    |     950    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      66    |      82    |  111532    |  111466    |\n",
            "|       from large pool |      47    |      61    |   44626    |   44579    |\n",
            "|       from small pool |      19    |      30    |   66906    |   66887    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:07:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  36% 20/55 [01:44<02:47,  4.78s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:08:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 77.06 MiB is free. Process 530225 has 14.67 GiB memory in use. Of the allocated memory 13.38 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:08:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 25           |        cudaMalloc retries: 43        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13705 MiB |  13705 MiB |   5416 GiB |   5402 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   5376 GiB |   5363 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13705 MiB |  13705 MiB |   5416 GiB |   5402 GiB |\n",
            "|       from large pool |  13475 MiB |  13475 MiB |   5376 GiB |   5363 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13684 MiB |  13684 MiB |   5411 GiB |   5398 GiB |\n",
            "|       from large pool |  13454 MiB |  13454 MiB |   5371 GiB |   5358 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     39 GiB |     39 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14888 MiB |  14888 MiB |  85224 MiB |  70336 MiB |\n",
            "|       from large pool |  14656 MiB |  14656 MiB |  83014 MiB |  68358 MiB |\n",
            "|       from small pool |    232 MiB |    310 MiB |   2210 MiB |   1978 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1182 MiB |   3226 MiB |   4650 GiB |   4648 GiB |\n",
            "|       from large pool |   1180 MiB |   3224 MiB |   4609 GiB |   4608 GiB |\n",
            "|       from small pool |      1 MiB |     14 MiB |     40 GiB |     40 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  264795    |  263671    |\n",
            "|       from large pool |     315    |     315    |   95968    |   95653    |\n",
            "|       from small pool |     809    |     955    |  168827    |  168018    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  264795    |  263671    |\n",
            "|       from large pool |     315    |     315    |   95968    |   95653    |\n",
            "|       from small pool |     809    |     955    |  168827    |  168018    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     180    |     218    |    1451    |    1271    |\n",
            "|       from large pool |      64    |      64    |     346    |     282    |\n",
            "|       from small pool |     116    |     155    |    1105    |     989    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      82    |  116709    |  116648    |\n",
            "|       from large pool |      41    |      47    |   47010    |   46969    |\n",
            "|       from small pool |      20    |      51    |   69699    |   69679    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:08:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  40% 22/55 [01:50<02:19,  4.22s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:08:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 13.06 MiB is free. Process 530225 has 14.73 GiB memory in use. Of the allocated memory 12.39 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:08:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 26           |        cudaMalloc retries: 45        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12689 MiB |  12689 MiB |   5481 GiB |   5468 GiB |\n",
            "|       from large pool |  12460 MiB |  12460 MiB |   5440 GiB |   5428 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     40 GiB |     40 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12689 MiB |  12689 MiB |   5481 GiB |   5468 GiB |\n",
            "|       from large pool |  12460 MiB |  12460 MiB |   5440 GiB |   5428 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     40 GiB |     40 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12675 MiB |  12675 MiB |   5476 GiB |   5464 GiB |\n",
            "|       from large pool |  12446 MiB |  12446 MiB |   5436 GiB |   5424 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     40 GiB |     40 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14952 MiB |  14958 MiB |  85682 MiB |  70730 MiB |\n",
            "|       from large pool |  14722 MiB |  14722 MiB |  83400 MiB |  68678 MiB |\n",
            "|       from small pool |    230 MiB |    302 MiB |   2282 MiB |   2052 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2262 MiB |   3012 MiB |   4710 GiB |   4708 GiB |\n",
            "|       from large pool |   2261 MiB |   3010 MiB |   4669 GiB |   4667 GiB |\n",
            "|       from small pool |      0 MiB |      7 MiB |     41 GiB |     41 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1124    |    1124    |  267559    |  266435    |\n",
            "|       from large pool |     315    |     315    |   97192    |   96877    |\n",
            "|       from small pool |     809    |     955    |  170367    |  169558    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1124    |    1124    |  267559    |  266435    |\n",
            "|       from large pool |     315    |     315    |   97192    |   96877    |\n",
            "|       from small pool |     809    |     955    |  170367    |  169558    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     179    |     215    |    1488    |    1309    |\n",
            "|       from large pool |      64    |      64    |     347    |     283    |\n",
            "|       from small pool |     115    |     151    |    1141    |    1026    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |      80    |  118043    |  117963    |\n",
            "|       from large pool |      56    |      56    |   47700    |   47644    |\n",
            "|       from small pool |      24    |      34    |   70343    |   70319    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:08:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  51% 28/55 [02:42<03:52,  8.60s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:09:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 85.06 MiB is free. Process 530225 has 14.66 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 545.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:09:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 27           |        cudaMalloc retries: 49        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13562 MiB |  14334 MiB |   5692 GiB |   5679 GiB |\n",
            "|       from large pool |  13330 MiB |  14103 MiB |   5650 GiB |   5637 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     41 GiB |     41 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13562 MiB |  14334 MiB |   5692 GiB |   5679 GiB |\n",
            "|       from large pool |  13330 MiB |  14103 MiB |   5650 GiB |   5637 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     41 GiB |     41 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13541 MiB |  14313 MiB |   5688 GiB |   5674 GiB |\n",
            "|       from large pool |  13310 MiB |  14081 MiB |   5646 GiB |   5633 GiB |\n",
            "|       from small pool |    231 MiB |    296 MiB |     41 GiB |     41 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14880 MiB |  14880 MiB |  96384 MiB |  81504 MiB |\n",
            "|       from large pool |  14646 MiB |  14646 MiB |  93988 MiB |  79342 MiB |\n",
            "|       from small pool |    234 MiB |    304 MiB |   2396 MiB |   2162 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 669619 KiB |   3479 MiB |   4858 GiB |   4858 GiB |\n",
            "|       from large pool | 667219 KiB |   3472 MiB |   4815 GiB |   4815 GiB |\n",
            "|       from small pool |   2399 KiB |      9 MiB |     42 GiB |     42 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  279015    |  277885    |\n",
            "|       from large pool |     312    |     325    |  101128    |  100816    |\n",
            "|       from small pool |     818    |     955    |  177887    |  177069    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  279015    |  277885    |\n",
            "|       from large pool |     312    |     325    |  101128    |  100816    |\n",
            "|       from small pool |     818    |     955    |  177887    |  177069    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     198    |     233    |    1593    |    1395    |\n",
            "|       from large pool |      81    |      81    |     395    |     314    |\n",
            "|       from small pool |     117    |     152    |    1198    |    1081    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      85    |      91    |  123173    |  123088    |\n",
            "|       from large pool |      65    |      67    |   49755    |   49690    |\n",
            "|       from small pool |      20    |      30    |   73418    |   73398    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:09:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  65% 36/55 [04:24<05:29, 17.33s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:10:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 157.06 MiB is free. Process 530225 has 14.59 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 549.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:10:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 28           |        cudaMalloc retries: 52        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14258 MiB |  14259 MiB |   5978 GiB |   5964 GiB |\n",
            "|       from large pool |  14026 MiB |  14026 MiB |   5933 GiB |   5919 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     44 GiB |     44 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14258 MiB |  14259 MiB |   5978 GiB |   5964 GiB |\n",
            "|       from large pool |  14026 MiB |  14026 MiB |   5933 GiB |   5919 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     44 GiB |     44 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14245 MiB |  14246 MiB |   5973 GiB |   5959 GiB |\n",
            "|       from large pool |  14012 MiB |  14012 MiB |   5928 GiB |   5914 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     44 GiB |     44 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14808 MiB |  14810 MiB | 105938 MiB |  91130 MiB |\n",
            "|       from large pool |  14574 MiB |  14574 MiB | 103450 MiB |  88876 MiB |\n",
            "|       from small pool |    234 MiB |    318 MiB |   2488 MiB |   2254 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 562238 KiB |   2280 MiB |   5061 GiB |   5060 GiB |\n",
            "|       from large pool | 560815 KiB |   2274 MiB |   5015 GiB |   5015 GiB |\n",
            "|       from small pool |   1423 KiB |     18 MiB |     45 GiB |     45 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1067    |    1082    |  294290    |  293223    |\n",
            "|       from large pool |     272    |     272    |  105532    |  105260    |\n",
            "|       from small pool |     795    |     955    |  188758    |  187963    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1067    |    1082    |  294290    |  293223    |\n",
            "|       from large pool |     272    |     272    |  105532    |  105260    |\n",
            "|       from small pool |     795    |     955    |  188758    |  187963    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     205    |     238    |    1680    |    1475    |\n",
            "|       from large pool |      88    |      88    |     436    |     348    |\n",
            "|       from small pool |     117    |     159    |    1244    |    1127    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      99    |     100    |  130362    |  130263    |\n",
            "|       from large pool |      73    |      73    |   52327    |   52254    |\n",
            "|       from small pool |      26    |      49    |   78035    |   78009    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:10:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  73% 40/55 [05:10<03:05, 12.36s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:11:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 11.06 MiB is free. Process 530225 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 536.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:11:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 29           |        cudaMalloc retries: 54        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13943 MiB |  14417 MiB |   6108 GiB |   6095 GiB |\n",
            "|       from large pool |  13704 MiB |  14178 MiB |   6063 GiB |   6049 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     45 GiB |     45 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13943 MiB |  14417 MiB |   6108 GiB |   6095 GiB |\n",
            "|       from large pool |  13704 MiB |  14178 MiB |   6063 GiB |   6049 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     45 GiB |     45 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13924 MiB |  14398 MiB |   6103 GiB |   6090 GiB |\n",
            "|       from large pool |  13685 MiB |  14159 MiB |   6058 GiB |   6044 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     45 GiB |     45 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14954 MiB |  14956 MiB | 113590 MiB |  98636 MiB |\n",
            "|       from large pool |  14714 MiB |  14714 MiB | 111018 MiB |  96304 MiB |\n",
            "|       from small pool |    240 MiB |    314 MiB |   2572 MiB |   2332 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 549731 KiB |   2310 MiB |   5167 GiB |   5166 GiB |\n",
            "|       from large pool | 548816 KiB |   2291 MiB |   5120 GiB |   5119 GiB |\n",
            "|       from small pool |    914 KiB |     20 MiB |     47 GiB |     47 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1058    |    1082    |  301210    |  300152    |\n",
            "|       from large pool |     256    |     257    |  107657    |  107401    |\n",
            "|       from small pool |     802    |     955    |  193553    |  192751    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1058    |    1082    |  301210    |  300152    |\n",
            "|       from large pool |     256    |     257    |  107657    |  107401    |\n",
            "|       from small pool |     802    |     955    |  193553    |  192751    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     204    |     245    |    1748    |    1544    |\n",
            "|       from large pool |      84    |      88    |     462    |     378    |\n",
            "|       from small pool |     120    |     157    |    1286    |    1166    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      93    |      94    |  133617    |  133524    |\n",
            "|       from large pool |      67    |      68    |   53566    |   53499    |\n",
            "|       from small pool |      26    |      58    |   80051    |   80025    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:11:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  95% 52/55 [07:04<00:30, 10.21s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:13:37 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 283.06 MiB is free. Process 530225 has 14.47 GiB memory in use. Of the allocated memory 12.58 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:13:37 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 30           |        cudaMalloc retries: 55        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12878 MiB |  12878 MiB |   6580 GiB |   6568 GiB |\n",
            "|       from large pool |  12648 MiB |  12648 MiB |   6531 GiB |   6518 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12878 MiB |  12878 MiB |   6580 GiB |   6568 GiB |\n",
            "|       from large pool |  12648 MiB |  12648 MiB |   6531 GiB |   6518 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12864 MiB |  12864 MiB |   6575 GiB |   6562 GiB |\n",
            "|       from large pool |  12635 MiB |  12635 MiB |   6525 GiB |   6513 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14682 MiB |  14760 MiB | 113870 MiB |  99188 MiB |\n",
            "|       from large pool |  14444 MiB |  14444 MiB | 111222 MiB |  96778 MiB |\n",
            "|       from small pool |    238 MiB |    316 MiB |   2648 MiB |   2410 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1803 MiB |   3821 MiB |   5526 GiB |   5524 GiB |\n",
            "|       from large pool |   1795 MiB |   3801 MiB |   5475 GiB |   5473 GiB |\n",
            "|       from small pool |      8 MiB |     24 MiB |     50 GiB |     50 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1012    |    1082    |  325543    |  324531    |\n",
            "|       from large pool |     233    |     233    |  115716    |  115483    |\n",
            "|       from small pool |     779    |     955    |  209827    |  209048    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1012    |    1082    |  325543    |  324531    |\n",
            "|       from large pool |     233    |     233    |  115716    |  115483    |\n",
            "|       from small pool |     779    |     955    |  209827    |  209048    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     203    |     242    |    1787    |    1584    |\n",
            "|       from large pool |      84    |      84    |     463    |     379    |\n",
            "|       from small pool |     119    |     158    |    1324    |    1205    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |     102    |     102    |  144806    |  144704    |\n",
            "|       from large pool |      76    |      76    |   57973    |   57897    |\n",
            "|       from small pool |      26    |      54    |   86833    |   86807    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:13:37 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  96% 53/55 [07:05<00:14,  7.39s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:13:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 39.06 MiB is free. Process 530225 has 14.71 GiB memory in use. Of the allocated memory 13.40 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:13:38 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 31           |        cudaMalloc retries: 56        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13121 MiB |  13719 MiB |   6603 GiB |   6590 GiB |\n",
            "|       from large pool |  12889 MiB |  13487 MiB |   6553 GiB |   6540 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13121 MiB |  13719 MiB |   6603 GiB |   6590 GiB |\n",
            "|       from large pool |  12889 MiB |  13487 MiB |   6553 GiB |   6540 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13103 MiB |  13701 MiB |   6597 GiB |   6585 GiB |\n",
            "|       from large pool |  12871 MiB |  13469 MiB |   6548 GiB |   6535 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     49 GiB |     49 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14926 MiB |  14926 MiB | 114114 MiB |  99188 MiB |\n",
            "|       from large pool |  14688 MiB |  14688 MiB | 111466 MiB |  96778 MiB |\n",
            "|       from small pool |    238 MiB |    316 MiB |   2648 MiB |   2410 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1086 MiB |   3821 MiB |   5542 GiB |   5540 GiB |\n",
            "|       from large pool |   1080 MiB |   3801 MiB |   5490 GiB |   5489 GiB |\n",
            "|       from small pool |      6 MiB |     24 MiB |     51 GiB |     51 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  326146    |  325016    |\n",
            "|       from large pool |     312    |     325    |  116074    |  115762    |\n",
            "|       from small pool |     818    |     955    |  210072    |  209254    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  326146    |  325016    |\n",
            "|       from large pool |     312    |     325    |  116074    |  115762    |\n",
            "|       from small pool |     818    |     955    |  210072    |  209254    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     204    |     242    |    1788    |    1584    |\n",
            "|       from large pool |      85    |      85    |     464    |     379    |\n",
            "|       from small pool |     119    |     158    |    1324    |    1205    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      97    |     103    |  145047    |  144950    |\n",
            "|       from large pool |      74    |      77    |   58142    |   58068    |\n",
            "|       from small pool |      23    |      54    |   86905    |   86882    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:13:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 003:  98% 54/55 [07:06<00:05,  5.51s/it, loss=7.935, nll_loss=7.498, ppl=180.74, wps=128.1, ups=0.09, wpb=1438.6, bsz=12.4, num_updates=100, lr=1.25e-05, gnorm=8.962, clip=100, train_wall=932, gb_free=6.5, wall=1119]2024-10-21 17:13:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 1/19 [00:01<00:19,  1.06s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 2/19 [00:02<00:18,  1.07s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  16% 3/19 [00:03<00:17,  1.07s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 4/19 [00:04<00:16,  1.11s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 5/19 [00:05<00:14,  1.06s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 6/19 [00:06<00:14,  1.12s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  37% 7/19 [00:07<00:11,  1.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 8/19 [00:08<00:10,  1.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 9/19 [00:09<00:11,  1.12s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 10/19 [00:11<00:11,  1.31s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 11/19 [00:13<00:12,  1.61s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  63% 12/19 [00:15<00:12,  1.73s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 13/19 [00:20<00:15,  2.62s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 14/19 [00:24<00:14,  2.93s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 15/19 [00:28<00:13,  3.27s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  84% 16/19 [00:31<00:10,  3.44s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 17/19 [00:37<00:07,  3.94s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 18/19 [00:42<00:04,  4.32s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 19/19 [00:46<00:00,  4.19s/it]\u001b[A\n",
            "                                                                        \u001b[A2024-10-21 17:14:42 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.077 | nll_loss 5.364 | ppl 41.18 | wps 532.5 | wpb 1758.4 | bsz 14.4 | num_updates 134 | best_loss 6.077\n",
            "2024-10-21 17:14:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 134 updates\n",
            "2024-10-21 17:14:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/positional_transformer/checkpoint3.pt\n",
            "2024-10-21 17:14:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/positional_transformer/checkpoint3.pt\n",
            "2024-10-21 17:15:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/positional_transformer/checkpoint3.pt (epoch 3 @ 134 updates, score 6.077) (writing took 74.68904265799938 seconds)\n",
            "2024-10-21 17:15:57 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-10-21 17:15:57 | INFO | train | epoch 003 | loss 6.347 | nll_loss 5.684 | ppl 51.41 | wps 104.6 | ups 0.08 | wpb 1342.8 | bsz 11.2 | num_updates 134 | lr 1.675e-05 | gnorm 4.316 | clip 100 | train_wall 430 | gb_free 1.4 | wall 1609\n",
            "2024-10-21 17:15:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 55\n",
            "epoch 004:   0% 0/55 [00:00<?, ?it/s]2024-10-21 17:15:57 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-10-21 17:15:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  18% 10/55 [01:45<09:23, 12.52s/it]2024-10-21 17:17:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 93.06 MiB is free. Process 530225 has 14.65 GiB memory in use. Of the allocated memory 13.83 GiB is allocated by PyTorch, and 708.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:17:44 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 32           |        cudaMalloc retries: 58        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14163 MiB |  14164 MiB |   7347 GiB |   7333 GiB |\n",
            "|       from large pool |  13930 MiB |  13930 MiB |   7291 GiB |   7278 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     55 GiB |     55 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14163 MiB |  14164 MiB |   7347 GiB |   7333 GiB |\n",
            "|       from large pool |  13930 MiB |  13930 MiB |   7291 GiB |   7278 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     55 GiB |     55 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14150 MiB |  14151 MiB |   7341 GiB |   7327 GiB |\n",
            "|       from large pool |  13917 MiB |  13917 MiB |   7285 GiB |   7272 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     55 GiB |     55 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14872 MiB |  14950 MiB | 123316 MiB | 108444 MiB |\n",
            "|       from large pool |  14634 MiB |  14634 MiB | 120588 MiB | 105954 MiB |\n",
            "|       from small pool |    238 MiB |    316 MiB |   2728 MiB |   2490 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 725616 KiB |   3992 MiB |   6098 GiB |   6097 GiB |\n",
            "|       from large pool | 720097 KiB |   3970 MiB |   6041 GiB |   6040 GiB |\n",
            "|       from small pool |   5519 KiB |     24 MiB |     57 GiB |     57 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1066    |    1082    |  360106    |  359040    |\n",
            "|       from large pool |     271    |     271    |  128340    |  128069    |\n",
            "|       from small pool |     795    |     955    |  231766    |  230971    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1066    |    1082    |  360106    |  359040    |\n",
            "|       from large pool |     271    |     271    |  128340    |  128069    |\n",
            "|       from small pool |     795    |     955    |  231766    |  230971    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     184    |     223    |    1860    |    1676    |\n",
            "|       from large pool |      65    |      65    |     496    |     431    |\n",
            "|       from small pool |     119    |     158    |    1364    |    1245    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      83    |     103    |  161135    |  161052    |\n",
            "|       from large pool |      55    |      55    |   64426    |   64371    |\n",
            "|       from small pool |      28    |      65    |   96709    |   96681    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:17:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  25% 14/55 [02:11<06:40,  9.77s/it]2024-10-21 17:18:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 370.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 89.06 MiB is free. Process 530225 has 14.66 GiB memory in use. Of the allocated memory 13.33 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:18:10 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 33           |        cudaMalloc retries: 60        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12810 MiB |  13650 MiB |   7498 GiB |   7485 GiB |\n",
            "|       from large pool |  12579 MiB |  13419 MiB |   7441 GiB |   7428 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12810 MiB |  13650 MiB |   7498 GiB |   7485 GiB |\n",
            "|       from large pool |  12579 MiB |  13419 MiB |   7441 GiB |   7428 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12794 MiB |  13634 MiB |   7491 GiB |   7479 GiB |\n",
            "|       from large pool |  12564 MiB |  13403 MiB |   7435 GiB |   7422 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14876 MiB |  14948 MiB | 124874 MiB | 109998 MiB |\n",
            "|       from large pool |  14644 MiB |  14644 MiB | 122068 MiB | 107424 MiB |\n",
            "|       from small pool |    232 MiB |    314 MiB |   2806 MiB |   2574 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1325 MiB |   4203 MiB |   6226 GiB |   6225 GiB |\n",
            "|       from large pool |   1324 MiB |   4186 MiB |   6168 GiB |   6166 GiB |\n",
            "|       from small pool |      1 MiB |     19 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  367329    |  366199    |\n",
            "|       from large pool |     312    |     325    |  130987    |  130675    |\n",
            "|       from small pool |     818    |     955    |  236342    |  235524    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  367329    |  366199    |\n",
            "|       from large pool |     312    |     325    |  130987    |  130675    |\n",
            "|       from small pool |     818    |     955    |  236342    |  235524    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     180    |     222    |    1903    |    1723    |\n",
            "|       from large pool |      64    |      65    |     500    |     436    |\n",
            "|       from small pool |     116    |     157    |    1403    |    1287    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      79    |  164258    |  164188    |\n",
            "|       from large pool |      50    |      51    |   65653    |   65603    |\n",
            "|       from small pool |      20    |      45    |   98605    |   98585    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:18:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  27% 15/55 [02:13<04:51,  7.28s/it]2024-10-21 17:18:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 19.06 MiB is free. Process 530225 has 14.73 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 722.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:18:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 34           |        cudaMalloc retries: 62        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13858 MiB |  14430 MiB |   7521 GiB |   7507 GiB |\n",
            "|       from large pool |  13625 MiB |  14199 MiB |   7464 GiB |   7451 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     57 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13858 MiB |  14430 MiB |   7521 GiB |   7507 GiB |\n",
            "|       from large pool |  13625 MiB |  14199 MiB |   7464 GiB |   7451 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     57 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13843 MiB |  14416 MiB |   7515 GiB |   7501 GiB |\n",
            "|       from large pool |  13610 MiB |  14184 MiB |   7458 GiB |   7445 GiB |\n",
            "|       from small pool |    232 MiB |    295 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14946 MiB |  14948 MiB | 125878 MiB | 110932 MiB |\n",
            "|       from large pool |  14712 MiB |  14712 MiB | 123068 MiB | 108356 MiB |\n",
            "|       from small pool |    234 MiB |    314 MiB |   2810 MiB |   2576 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    837 MiB |   4203 MiB |   6245 GiB |   6244 GiB |\n",
            "|       from large pool |    836 MiB |   4186 MiB |   6186 GiB |   6186 GiB |\n",
            "|       from small pool |      1 MiB |     19 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  367925    |  366795    |\n",
            "|       from large pool |     312    |     325    |  131338    |  131026    |\n",
            "|       from small pool |     818    |     955    |  236587    |  235769    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  367925    |  366795    |\n",
            "|       from large pool |     312    |     325    |  131338    |  131026    |\n",
            "|       from small pool |     818    |     955    |  236587    |  235769    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     181    |     222    |    1909    |    1728    |\n",
            "|       from large pool |      64    |      65    |     504    |     440    |\n",
            "|       from small pool |     117    |     157    |    1405    |    1288    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      75    |      80    |  164530    |  164455    |\n",
            "|       from large pool |      50    |      53    |   65811    |   65761    |\n",
            "|       from small pool |      25    |      45    |   98719    |   98694    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:18:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  31% 17/55 [02:21<03:42,  5.85s/it]2024-10-21 17:18:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 474.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 419.06 MiB is free. Process 530225 has 14.34 GiB memory in use. Of the allocated memory 13.15 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:18:19 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 35           |        cudaMalloc retries: 64        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13462 MiB |  13462 MiB |   7595 GiB |   7582 GiB |\n",
            "|       from large pool |  13223 MiB |  13223 MiB |   7538 GiB |   7525 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     57 GiB |     57 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13462 MiB |  13462 MiB |   7595 GiB |   7582 GiB |\n",
            "|       from large pool |  13223 MiB |  13223 MiB |   7538 GiB |   7525 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     57 GiB |     57 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13452 MiB |  13452 MiB |   7589 GiB |   7576 GiB |\n",
            "|       from large pool |  13213 MiB |  13213 MiB |   7532 GiB |   7519 GiB |\n",
            "|       from small pool |    239 MiB |    295 MiB |     57 GiB |     57 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14546 MiB |  14608 MiB | 128280 MiB | 113734 MiB |\n",
            "|       from large pool |  14306 MiB |  14306 MiB | 125398 MiB | 111092 MiB |\n",
            "|       from small pool |    240 MiB |    302 MiB |   2882 MiB |   2642 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1083 MiB |   4180 MiB |   6300 GiB |   6299 GiB |\n",
            "|       from large pool |   1082 MiB |   4173 MiB |   6241 GiB |   6240 GiB |\n",
            "|       from small pool |      0 MiB |      7 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1057    |    1082    |  370565    |  369508    |\n",
            "|       from large pool |     255    |     255    |  132474    |  132219    |\n",
            "|       from small pool |     802    |     955    |  238091    |  237289    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1057    |    1082    |  370565    |  369508    |\n",
            "|       from large pool |     255    |     255    |  132474    |  132219    |\n",
            "|       from small pool |     802    |     955    |  238091    |  237289    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     180    |     211    |    1950    |    1770    |\n",
            "|       from large pool |      60    |      60    |     509    |     449    |\n",
            "|       from small pool |     120    |     151    |    1441    |    1321    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |      82    |  165744    |  165664    |\n",
            "|       from large pool |      53    |      54    |   66424    |   66371    |\n",
            "|       from small pool |      27    |      40    |   99320    |   99293    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:18:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  42% 23/55 [03:27<05:53, 11.06s/it]2024-10-21 17:19:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 322.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 167.06 MiB is free. Process 530225 has 14.58 GiB memory in use. Of the allocated memory 11.93 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:19:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 36           |        cudaMalloc retries: 65        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  11481 MiB |  12214 MiB |   7805 GiB |   7794 GiB |\n",
            "|       from large pool |  11236 MiB |  11968 MiB |   7746 GiB |   7735 GiB |\n",
            "|       from small pool |    245 MiB |    295 MiB |     59 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  11481 MiB |  12214 MiB |   7805 GiB |   7794 GiB |\n",
            "|       from large pool |  11236 MiB |  11968 MiB |   7746 GiB |   7735 GiB |\n",
            "|       from small pool |    245 MiB |    295 MiB |     59 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  11466 MiB |  12198 MiB |   7799 GiB |   7787 GiB |\n",
            "|       from large pool |  11221 MiB |  11953 MiB |   7739 GiB |   7729 GiB |\n",
            "|       from small pool |    245 MiB |    295 MiB |     59 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14798 MiB |  14866 MiB | 128600 MiB | 113802 MiB |\n",
            "|       from large pool |  14550 MiB |  14550 MiB | 125642 MiB | 111092 MiB |\n",
            "|       from small pool |    248 MiB |    316 MiB |   2958 MiB |   2710 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2140 MiB |   2947 MiB |   6495 GiB |   6493 GiB |\n",
            "|       from large pool |   2137 MiB |   2934 MiB |   6435 GiB |   6433 GiB |\n",
            "|       from small pool |      2 MiB |     17 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  381744    |  380614    |\n",
            "|       from large pool |     295    |     306    |  135930    |  135635    |\n",
            "|       from small pool |     835    |     955    |  245814    |  244979    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  381744    |  380614    |\n",
            "|       from large pool |     295    |     306    |  135930    |  135635    |\n",
            "|       from small pool |     835    |     955    |  245814    |  244979    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     185    |     219    |    1989    |    1804    |\n",
            "|       from large pool |      61    |      61    |     510    |     449    |\n",
            "|       from small pool |     124    |     158    |    1479    |    1355    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      79    |  170689    |  170613    |\n",
            "|       from large pool |      52    |      52    |   68081    |   68029    |\n",
            "|       from small pool |      24    |      49    |  102608    |  102584    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:19:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  45% 25/55 [03:34<03:37,  7.26s/it]2024-10-21 17:19:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 401.06 MiB is free. Process 530225 has 14.35 GiB memory in use. Of the allocated memory 12.63 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:19:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 37           |        cudaMalloc retries: 67        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12932 MiB |  12932 MiB |   7865 GiB |   7852 GiB |\n",
            "|       from large pool |  12703 MiB |  12703 MiB |   7805 GiB |   7793 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12932 MiB |  12932 MiB |   7865 GiB |   7852 GiB |\n",
            "|       from large pool |  12703 MiB |  12703 MiB |   7805 GiB |   7793 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12913 MiB |  12913 MiB |   7858 GiB |   7846 GiB |\n",
            "|       from large pool |  12684 MiB |  12684 MiB |   7799 GiB |   7786 GiB |\n",
            "|       from small pool |    228 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14564 MiB |  14564 MiB | 132498 MiB | 117934 MiB |\n",
            "|       from large pool |  14330 MiB |  14330 MiB | 129482 MiB | 115152 MiB |\n",
            "|       from small pool |    234 MiB |    304 MiB |   3016 MiB |   2782 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1631 MiB |   2541 MiB |   6545 GiB |   6543 GiB |\n",
            "|       from large pool |   1626 MiB |   2534 MiB |   6484 GiB |   6482 GiB |\n",
            "|       from small pool |      5 MiB |     10 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1068    |    1082    |  384398    |  383330    |\n",
            "|       from large pool |     273    |     273    |  137084    |  136811    |\n",
            "|       from small pool |     795    |     955    |  247314    |  246519    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1068    |    1082    |  384398    |  383330    |\n",
            "|       from large pool |     273    |     273    |  137084    |  136811    |\n",
            "|       from small pool |     795    |     955    |  247314    |  246519    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     175    |     212    |    2023    |    1848    |\n",
            "|       from large pool |      58    |      60    |     515    |     457    |\n",
            "|       from small pool |     117    |     152    |    1508    |    1391    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      73    |  171914    |  171852    |\n",
            "|       from large pool |      36    |      45    |   68674    |   68638    |\n",
            "|       from small pool |      26    |      35    |  103240    |  103214    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:19:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  47% 26/55 [03:36<02:49,  5.84s/it]2024-10-21 17:19:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 332.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 69.06 MiB is free. Process 530225 has 14.68 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 568.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:19:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 38           |        cudaMalloc retries: 68        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13554 MiB |  14327 MiB |   7889 GiB |   7875 GiB |\n",
            "|       from large pool |  13323 MiB |  14095 MiB |   7829 GiB |   7816 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13554 MiB |  14327 MiB |   7889 GiB |   7875 GiB |\n",
            "|       from large pool |  13323 MiB |  14095 MiB |   7829 GiB |   7816 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13541 MiB |  14313 MiB |   7882 GiB |   7869 GiB |\n",
            "|       from large pool |  13310 MiB |  14081 MiB |   7823 GiB |   7810 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14896 MiB |  14896 MiB | 132830 MiB | 117934 MiB |\n",
            "|       from large pool |  14662 MiB |  14662 MiB | 129814 MiB | 115152 MiB |\n",
            "|       from small pool |    234 MiB |    304 MiB |   3016 MiB |   2782 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1009 MiB |   2565 MiB |   6566 GiB |   6565 GiB |\n",
            "|       from large pool |   1006 MiB |   2558 MiB |   6504 GiB |   6503 GiB |\n",
            "|       from small pool |      2 MiB |     10 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  385001    |  383871    |\n",
            "|       from large pool |     312    |     325    |  137442    |  137130    |\n",
            "|       from small pool |     818    |     955    |  247559    |  246741    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  385001    |  383871    |\n",
            "|       from large pool |     312    |     325    |  137442    |  137130    |\n",
            "|       from small pool |     818    |     955    |  247559    |  246741    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     176    |     212    |    2024    |    1848    |\n",
            "|       from large pool |      59    |      60    |     516    |     457    |\n",
            "|       from small pool |     117    |     152    |    1508    |    1391    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |      81    |  172131    |  172054    |\n",
            "|       from large pool |      51    |      52    |   68818    |   68767    |\n",
            "|       from small pool |      26    |      35    |  103313    |  103287    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:19:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  53% 29/55 [04:05<03:39,  8.43s/it]2024-10-21 17:20:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 386.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 17.06 MiB is free. Process 530225 has 14.73 GiB memory in use. Of the allocated memory 12.61 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:20:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 39           |        cudaMalloc retries: 70        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12435 MiB |  13076 MiB |   7987 GiB |   7975 GiB |\n",
            "|       from large pool |  12205 MiB |  12846 MiB |   7927 GiB |   7915 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12435 MiB |  13076 MiB |   7987 GiB |   7975 GiB |\n",
            "|       from large pool |  12205 MiB |  12846 MiB |   7927 GiB |   7915 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12419 MiB |  13060 MiB |   7981 GiB |   7969 GiB |\n",
            "|       from large pool |  12189 MiB |  12831 MiB |   7920 GiB |   7908 GiB |\n",
            "|       from small pool |    230 MiB |    295 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14948 MiB |  14948 MiB | 133302 MiB | 118354 MiB |\n",
            "|       from large pool |  14716 MiB |  14716 MiB | 130200 MiB | 115484 MiB |\n",
            "|       from small pool |    232 MiB |    318 MiB |   3102 MiB |   2870 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2126 MiB |   2691 MiB |   6664 GiB |   6662 GiB |\n",
            "|       from large pool |   2124 MiB |   2677 MiB |   6602 GiB |   6600 GiB |\n",
            "|       from small pool |      1 MiB |     22 MiB |     62 GiB |     62 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  389983    |  388853    |\n",
            "|       from large pool |     312    |     325    |  139185    |  138873    |\n",
            "|       from small pool |     818    |     955    |  250798    |  249980    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  389983    |  388853    |\n",
            "|       from large pool |     312    |     325    |  139185    |  138873    |\n",
            "|       from small pool |     818    |     955    |  250798    |  249980    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     175    |     217    |    2068    |    1893    |\n",
            "|       from large pool |      59    |      59    |     517    |     458    |\n",
            "|       from small pool |     116    |     159    |    1551    |    1435    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      74    |  174379    |  174310    |\n",
            "|       from large pool |      49    |      50    |   69666    |   69617    |\n",
            "|       from small pool |      20    |      42    |  104713    |  104693    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:20:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  58% 32/55 [04:16<02:02,  5.32s/it]2024-10-21 17:20:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 388.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 205.06 MiB is free. Process 530225 has 14.54 GiB memory in use. Of the allocated memory 12.12 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:20:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 40           |        cudaMalloc retries: 72        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12408 MiB |  12408 MiB |   8085 GiB |   8072 GiB |\n",
            "|       from large pool |  12179 MiB |  12179 MiB |   8023 GiB |   8012 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12408 MiB |  12408 MiB |   8085 GiB |   8072 GiB |\n",
            "|       from large pool |  12179 MiB |  12179 MiB |   8023 GiB |   8012 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12392 MiB |  12392 MiB |   8078 GiB |   8066 GiB |\n",
            "|       from large pool |  12162 MiB |  12162 MiB |   8017 GiB |   8005 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14760 MiB |  14762 MiB | 134152 MiB | 119392 MiB |\n",
            "|       from large pool |  14530 MiB |  14530 MiB | 130976 MiB | 116446 MiB |\n",
            "|       from small pool |    230 MiB |    302 MiB |   3176 MiB |   2946 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   2351 MiB |   4419 MiB |   6754 GiB |   6752 GiB |\n",
            "|       from large pool |   2350 MiB |   4412 MiB |   6692 GiB |   6689 GiB |\n",
            "|       from small pool |      0 MiB |      8 MiB |     62 GiB |     62 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1068    |    1082    |  394881    |  393813    |\n",
            "|       from large pool |     273    |     273    |  141223    |  140950    |\n",
            "|       from small pool |     795    |     955    |  253658    |  252863    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1068    |    1082    |  394881    |  393813    |\n",
            "|       from large pool |     273    |     273    |  141223    |  140950    |\n",
            "|       from small pool |     795    |     955    |  253658    |  252863    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     169    |     209    |    2107    |    1938    |\n",
            "|       from large pool |      54    |      58    |     519    |     465    |\n",
            "|       from small pool |     115    |     151    |    1588    |    1473    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      65    |  176550    |  176489    |\n",
            "|       from large pool |      37    |      42    |   70639    |   70602    |\n",
            "|       from small pool |      24    |      27    |  105911    |  105887    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:20:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  62% 34/55 [04:35<02:50,  8.11s/it]2024-10-21 17:20:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 378.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 203.06 MiB is free. Process 530225 has 14.55 GiB memory in use. Of the allocated memory 12.58 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:20:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 41           |        cudaMalloc retries: 73        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  12877 MiB |  12877 MiB |   8137 GiB |   8125 GiB |\n",
            "|       from large pool |  12647 MiB |  12647 MiB |   8076 GiB |   8063 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  12877 MiB |  12877 MiB |   8137 GiB |   8125 GiB |\n",
            "|       from large pool |  12647 MiB |  12647 MiB |   8076 GiB |   8063 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  12864 MiB |  12864 MiB |   8130 GiB |   8118 GiB |\n",
            "|       from large pool |  12635 MiB |  12635 MiB |   8069 GiB |   8057 GiB |\n",
            "|       from small pool |    229 MiB |    295 MiB |     61 GiB |     61 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14762 MiB |  14838 MiB | 134230 MiB | 119468 MiB |\n",
            "|       from large pool |  14530 MiB |  14530 MiB | 130976 MiB | 116446 MiB |\n",
            "|       from small pool |    232 MiB |    308 MiB |   3254 MiB |   3022 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1884 MiB |   3887 MiB |   6809 GiB |   6808 GiB |\n",
            "|       from large pool |   1882 MiB |   3873 MiB |   6746 GiB |   6744 GiB |\n",
            "|       from small pool |      2 MiB |     15 MiB |     63 GiB |     63 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1012    |    1082    |  397335    |  396323    |\n",
            "|       from large pool |     233    |     233    |  141924    |  141691    |\n",
            "|       from small pool |     779    |     955    |  255411    |  254632    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1012    |    1082    |  397335    |  396323    |\n",
            "|       from large pool |     233    |     233    |  141924    |  141691    |\n",
            "|       from small pool |     779    |     955    |  255411    |  254632    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     170    |     208    |    2146    |    1976    |\n",
            "|       from large pool |      54    |      54    |     519    |     465    |\n",
            "|       from small pool |     116    |     154    |    1627    |    1511    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      73    |      78    |  177791    |  177718    |\n",
            "|       from large pool |      51    |      52    |   71028    |   70977    |\n",
            "|       from small pool |      22    |      42    |  106763    |  106741    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:20:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  73% 40/55 [05:32<03:08, 12.55s/it]2024-10-21 17:21:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 203.06 MiB is free. Process 530225 has 14.55 GiB memory in use. Of the allocated memory 13.50 GiB is allocated by PyTorch, and 934.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "2024-10-21 17:21:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 42           |        cudaMalloc retries: 74        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13398 MiB |  14017 MiB |   8357 GiB |   8344 GiB |\n",
            "|       from large pool |  13167 MiB |  13787 MiB |   8293 GiB |   8280 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     63 GiB |     63 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13398 MiB |  14017 MiB |   8357 GiB |   8344 GiB |\n",
            "|       from large pool |  13167 MiB |  13787 MiB |   8293 GiB |   8280 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     63 GiB |     63 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13384 MiB |  14003 MiB |   8350 GiB |   8337 GiB |\n",
            "|       from large pool |  13153 MiB |  13773 MiB |   8287 GiB |   8274 GiB |\n",
            "|       from small pool |    231 MiB |    295 MiB |     63 GiB |     63 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14762 MiB |  14842 MiB | 134310 MiB | 119548 MiB |\n",
            "|       from large pool |  14530 MiB |  14530 MiB | 130976 MiB | 116446 MiB |\n",
            "|       from small pool |    232 MiB |    312 MiB |   3334 MiB |   3102 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   1363 MiB |   3017 MiB |   7042 GiB |   7040 GiB |\n",
            "|       from large pool |   1362 MiB |   3005 MiB |   6977 GiB |   6975 GiB |\n",
            "|       from small pool |      0 MiB |     15 MiB |     64 GiB |     64 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    1130    |    1141    |  408833    |  407703    |\n",
            "|       from large pool |     312    |     325    |  145612    |  145300    |\n",
            "|       from small pool |     818    |     955    |  263221    |  262403    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    1130    |    1141    |  408833    |  407703    |\n",
            "|       from large pool |     312    |     325    |  145612    |  145300    |\n",
            "|       from small pool |     818    |     955    |  263221    |  262403    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     170    |     210    |    2186    |    2016    |\n",
            "|       from large pool |      54    |      54    |     519    |     465    |\n",
            "|       from small pool |     116    |     156    |    1667    |    1551    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      64    |      66    |  182828    |  182764    |\n",
            "|       from large pool |      44    |      45    |   72833    |   72789    |\n",
            "|       from small pool |      20    |      39    |  109995    |  109975    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2024-10-21 17:21:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 004:  98% 54/55 [07:12<00:08,  8.61s/it]2024-10-21 17:23:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 1/19 [00:01<00:18,  1.04s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 2/19 [00:02<00:17,  1.06s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  16% 3/19 [00:03<00:16,  1.06s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 4/19 [00:04<00:16,  1.11s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 5/19 [00:05<00:14,  1.06s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 6/19 [00:06<00:14,  1.12s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  37% 7/19 [00:07<00:11,  1.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 8/19 [00:08<00:10,  1.03it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 9/19 [00:09<00:11,  1.11s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 10/19 [00:11<00:11,  1.30s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 11/19 [00:13<00:12,  1.57s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  63% 12/19 [00:15<00:11,  1.70s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 13/19 [00:20<00:15,  2.59s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  74% 14/19 [00:23<00:14,  2.91s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 15/19 [00:27<00:13,  3.27s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  84% 16/19 [00:31<00:10,  3.44s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 17/19 [00:36<00:07,  3.94s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 18/19 [00:42<00:04,  4.34s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 19/19 [00:45<00:00,  4.19s/it]\u001b[A\n",
            "                                                                        \u001b[A2024-10-21 17:24:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.377 | nll_loss 4.528 | ppl 23.08 | wps 533.7 | wpb 1758.4 | bsz 14.4 | num_updates 178 | best_loss 5.377\n",
            "2024-10-21 17:24:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 178 updates\n",
            "2024-10-21 17:24:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/positional_transformer/checkpoint4.pt\n",
            "2024-10-21 17:24:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/positional_transformer/checkpoint4.pt\n",
            "2024-10-21 17:24:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/positional_transformer/checkpoint4.pt (epoch 4 @ 178 updates, score 5.377) (writing took 21.378963434999605 seconds)\n",
            "2024-10-21 17:24:24 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-10-21 17:24:24 | INFO | train | epoch 004 | loss 5.573 | nll_loss 4.779 | ppl 27.46 | wps 121.5 | ups 0.09 | wpb 1399.2 | bsz 12.3 | num_updates 178 | lr 2.225e-05 | gnorm 4.76 | clip 100 | train_wall 424 | gb_free 3.8 | wall 2116\n",
            "2024-10-21 17:24:24 | INFO | fairseq_cli.train | done training in 2116.0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Models"
      ],
      "metadata": {
        "id": "Xjjky4PlFog4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BPE model evaluation\n",
        "!fairseq-generate data-bin/bpe --path checkpoints/bpe_transformer/checkpoint_best.pt \\\n",
        "    --beam 5 --remove-bpe --max-source-positions 42105 --max-target-positions 500 \\\n",
        "    --skip-invalid-size-inputs-valid-test > bpe_output.txt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t50I9KK5FncJ",
        "outputId": "8592ccf2-8f80-4663-ea28-a04d3db275d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-21 18:21:18.006477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-21 18:21:18.037730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-21 18:21:18.044325: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-21 18:21:18.059462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-21 18:21:19.181380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-10-21 18:21:21 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-10-21 18:21:23 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/bpe_transformer/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/bpe', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 42105, 'max_target_positions': 500, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-10-21 18:21:23 | INFO | fairseq.tasks.translation | [src] dictionary: 520 types\n",
            "2024-10-21 18:21:23 | INFO | fairseq.tasks.translation | [tgt] dictionary: 488 types\n",
            "2024-10-21 18:21:23 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/bpe_transformer/checkpoint_best.pt\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "2024-10-21 18:21:25 | INFO | fairseq.data.data_utils | loaded 5 examples from: data-bin/bpe/test.src-tgt.src\n",
            "2024-10-21 18:21:25 | INFO | fairseq.data.data_utils | loaded 5 examples from: data-bin/bpe/test.src-tgt.tgt\n",
            "2024-10-21 18:21:25 | INFO | fairseq.tasks.translation | data-bin/bpe test src-tgt 5 examples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-generate\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/generate.py\", line 413, in cli_main\n",
            "    main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/generate.py\", line 50, in main\n",
            "    return _main(cfg, sys.stdout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/generate.py\", line 201, in _main\n",
            "    hypos = task.inference_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 540, in inference_step\n",
            "    return generator.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 204, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 274, in _generate\n",
            "    encoder_outs = self.model.forward_encoder(net_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 801, in forward_encoder\n",
            "    return [model.encoder.forward_torchscript(net_input) for model in self.models]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/sequence_generator.py\", line 801, in <listcomp>\n",
            "    return [model.encoder.forward_torchscript(net_input) for model in self.models]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/fairseq_encoder.py\", line 55, in forward_torchscript\n",
            "    return self.forward_non_torchscript(net_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/fairseq_encoder.py\", line 62, in forward_non_torchscript\n",
            "    return self.forward(**encoder_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 165, in forward\n",
            "    return self.forward_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 294, in forward_scriptable\n",
            "    lr = layer(x, encoder_padding_mask=encoder_padding_mask_out)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 319, in forward\n",
            "    output = torch._transformer_encoder_layer_fwd(\n",
            "RuntimeError: Mask Type should be defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-wise model evaluation\n",
        "!fairseq-generate data-bin/wordwise --path checkpoints/wordwise_transformer/checkpoint_best.pt --beam 5 > wordwise_output.txt\n",
        "\n",
        "# Positional encoding (is included by default in Transformer models)\n",
        "!fairseq-generate data-bin/positional --path checkpoints/positional_transformer/checkpoint_best.pt --beam 5 > positional_output.txt\n"
      ],
      "metadata": {
        "id": "E2DHM5keK_91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}